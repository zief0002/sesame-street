[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Instructor/TA",
    "section": "",
    "text": "Instructor: Andrew Zieffler  Email: zief0002@umn.edu  Office: Education Sciences Building 178  Office Hours: Tuesday 9:00 AM–10:00 AM; and by appointment  Virtual Office: If you want to meet virtually, send me a Google calendar invite and include a Zoom link in the invite.\n\nTA: Kyle Stagnaro  Email: stagn011@umn.edu  Office: Zoom  Office Hours: Monday 12:00 PM–1:00 PM; and by appointment"
  },
  {
    "objectID": "assignments.html",
    "href": "assignments.html",
    "title": "Assignment Due Dates",
    "section": "",
    "text": "Below are the due dates for the assignments, as well as links to each assignment. The due dates may change at the instructor’s discretion. Any revised due dates will be announced in class and posted to the website."
  },
  {
    "objectID": "assignments/assignment-01-introduction-to-quarto.html",
    "href": "assignments/assignment-01-introduction-to-quarto.html",
    "title": "Assignment 01",
    "section": "",
    "text": "The goal of this assignment is to give you experience using Quarto to integrate analysis and documentation. In this assignment, you will use the data from the file fertility.csv to explain variation in infant mortality rates."
  },
  {
    "objectID": "assignments/assignment-01-introduction-to-quarto.html#instructions",
    "href": "assignments/assignment-01-introduction-to-quarto.html#instructions",
    "title": "Assignment 01",
    "section": "Instructions",
    "text": "Instructions\nCreate a QMD document to respond to each of the questions below. Please adhere to the following guidelines for further formatting your assignment:\n\nAll graphics should be resized so that they do not take up more room than necessary and should have an appropriate caption. Learn how to do this in a code chunk using knitr syntax.\nAny typed mathematics (equations, matrices, vectors, etc.) should be appropriately typeset within the document using Markdown’s equation typesetting. See here for some examples of how mathematics can be typeset in R Markdown.\nAll syntax should be hidden (i.e., not displayed) unless specifically asked for.\nAny messages or warnings produced (e.g., from loading packages) should also be hidden.\n\nFor each question, specify the question number (e.g., Question 2) using a level-2 (or smaller) header. This assignment is worth 15 points."
  },
  {
    "objectID": "assignments/assignment-01-introduction-to-quarto.html#questions",
    "href": "assignments/assignment-01-introduction-to-quarto.html#questions",
    "title": "Assignment 01",
    "section": "Questions",
    "text": "Questions\n\nImport the data and display the first several rows of data (not all of it). Use one of the paged table options in your YAML to ensure that this is printed nicely. All syntax for these commands should be hidden.\nRecode the gni_class variable by creating a dummy variable called high_gni. To create this dummy variable use the syntax below. Explain in words (not code) how the high_gni dummy variable is being coded.\n\n\nfertility = fertility |&gt;\n  mutate(\n    high_gni = if_else(gni_class == \"Upper/Middle\" | gni_class == \"Upper\", 1, 0)\n  )\n\n\nCreate a well-formatted table that includes the mean infant mortality rate and the standard deviation of infant mortality rates for each of the two High GNI levels represented in the data. Display these in a summary table. All numerical output should be rounded to two decimal places. Also add an appropriate caption (the caption does not have to follow APA formatting). Do not display any syntax.\nUse the lm() function to fit a main effects linear model (unstandardized) regressing infant mortality on your dummy variable and female education level. Use the tidy() function from the broom package to display the model’s coefficient-level output. (Reminder: Do not display any syntax, only the model’s coefficient-level output.)\nUse a bulleted list to provide an interpretation of each estimated regression coefficient (including the intercept) from the regression you fitted in Question 3; one interpretation per list item.\nCreate a well-formatted table of the model’s coefficient tidy() output (e.g., using the gt() function from the {gt} package). In the final outputted table, the five column names should be “Predictor”, “B”, “SE”, “t”, and “p”, respectively. (The last four column names should be italicized since they are statistics.) All numerical output should be rounded to two decimal places, except the p-values, which should be rounded to three decimal places. Also add a caption. (2pts.)\nCreate a publication quality plot that displays the results from the fitted model. For this plot, put the female education level predictor on the x-axis. Display separate lines to show the effects for each level of the dummy-coded GNI variable. The two lines should be displayed using different linetypes or colors (or both) so that they can be easily differentiated in the plot. Be sure that the figure includes a caption using the fig-cap option in the code chunk. The plot should be centered on the page. Adjust the aspect ratio of the plot using fig-width and fig-height in the code chunk so that it looks good. Lastly, change the values of the output width/height (out-width, out-height) to change the size of the plot from the default values. (2pts.)\nUse a display equation to write the equation for the underlying regression model (including error and assumptions) using Greek letters, subscripts, and variable names. Also write the equation for the fitted least squares regression equation based on the output from lm(). Type these two equations in the same display equation, each on a separate line of text in your document, and align the equals signs. (2pts.)\nWrite the following sentence: “The estimated partial regression coefficient (\\(\\hat\\beta_\\mathrm{Female~Education}\\)) is \\(x\\).” In this sentence, use an inline code chunk to replace \\(x\\) with the value for the fitted coefficient from the fitted equation. In this code chunk, do not just write in the value for the coefficient, but use syntax to extract the value from the tidy() output. (Hint: Google “R extract element from dataframe”.) (2pts.)\nCompute the sum of squared residuals for the fitted regression. Although you can use the anova() function to check your work, compute this value by actually using R syntax to carry out the computation, \\(\\sum(y_i - \\hat{y}_i)^2\\). Show the syntax you used in your document.\nWrite a sentence that includes two references in an inline citation. This should also generate the actual references when you knit your document. One of the two references should be the Fox textbook. The other should be a journal article of your choice. You can choose the topic of the sentence and how the two references are used in the citation. (Note the references do not actually have to pertain to what is written in the sentence. This is just an exercise in using the bibliography tools in Markdown.) Specify an appropriate CSL file so that the references and citations are in APA format. (If you want to use a citation style that is different from APA, say for a specific journal, use the appropriate CSL file, and indicate that on the assignment.) Both the BIB and CSL files should be included in your project’s assets directory. (2pts.)"
  },
  {
    "objectID": "assignments/assignment-01-introduction-to-quarto.html#what-to-submit",
    "href": "assignments/assignment-01-introduction-to-quarto.html#what-to-submit",
    "title": "Assignment 01",
    "section": "What to Submit",
    "text": "What to Submit\nYou need to submit a zipped version of your entire assignment-01 project directory. When the TA unzips this and opens your R project file they will render your QMD document. This should produce the HTML file that will be graded. (You can include your HTML file as an extra attachment if you want, but the QMD document will need to render. If it doesn’t render the TA will return it to you to try again.)"
  },
  {
    "objectID": "assignments/assignment-02-polynomial-effects.html",
    "href": "assignments/assignment-02-polynomial-effects.html",
    "title": "Assignment 02",
    "section": "",
    "text": "The goal of this assignment is to give you experience fitting, interpreting, and evaluating models with polynomial effects. In this assignment, you will use the data from the file fertility.csv to explain variation in in infant mortality rates."
  },
  {
    "objectID": "assignments/assignment-02-polynomial-effects.html#instructions",
    "href": "assignments/assignment-02-polynomial-effects.html#instructions",
    "title": "Assignment 02",
    "section": "Instructions",
    "text": "Instructions\nThis assignment needs to be completed using Quarto. Submit a zipped version of your entire assignment-02 project directory. Your QMD document should include a response to each of the questions below. Please adhere to the following guidelines for further formatting your assignment:\n\nAll graphics should be resized so that they do not take up more room than necessary and should have an appropriate caption.\nAny typed mathematics (equations, matrices, vectors, etc.) should be appropriately typeset within the document using Markdown’s equation typesetting.\nAll syntax should be hidden (i.e., not displayed) unless specifically asked for.\nAny messages or warnings produced (e.g., from loading packages) should also be hidden.\n\nFor each question, specify the question number (e.g., Question 2) using a level-2 (or smaller) header. This assignment is worth 15 points."
  },
  {
    "objectID": "assignments/assignment-02-polynomial-effects.html#model-1-linear-effect-of-female-education-level",
    "href": "assignments/assignment-02-polynomial-effects.html#model-1-linear-effect-of-female-education-level",
    "title": "Assignment 02",
    "section": "Model 1: Linear Effect of Female Education Level",
    "text": "Model 1: Linear Effect of Female Education Level\n\nCreate a scatterplot showing the relationship between female education level and infant mortality rates.\nDescribe the relationship between female education level and infant mortality rates. Be sure to comment on the structural form, direction and strength of the relationship. Also comment on any potential observations that deviate from following this relationship (unusual observations or clusters of observations).\nCompute and report the Pearson correlation coefficient between female education level and infant mortality rate. Based on your response to Question 2, explain whether the Pearson correlation coefficient is an appropriate summary measure of the relationship.\nRegress infant mortality rates on female education level. For this model, posit a linear effect of female education level on infant mortality rate (Model 1). Create the scatterplot of the standardized residuals versus the fitted values from Model 1.\nDoes this plot suggest problems about meeting the assumption that the average residual is zero at each fitted value? Explain."
  },
  {
    "objectID": "assignments/assignment-02-polynomial-effects.html#model-2-quadratic-effect-of-female-education-level",
    "href": "assignments/assignment-02-polynomial-effects.html#model-2-quadratic-effect-of-female-education-level",
    "title": "Assignment 02",
    "section": "Model 2: Quadratic Effect of Female Education Level",
    "text": "Model 2: Quadratic Effect of Female Education Level\n\nRegress infant mortality rates on female education level. For this model, posit a quadratic effect of female education level on infant mortality rate (Model 2). Write the fitted equation using Equation Editor (or some other program that correctly types mathematical expressions).\nCompute, report, and interpret the likelihood ratio between Model 2 and Model 1.\nCarry out a likelihood ratio test to compare Model 1 and Model 2. Report the results from this test in a nicely formatted table."
  },
  {
    "objectID": "assignments/assignment-02-polynomial-effects.html#model-3-control-for-differences-in-gross-national-income-gni",
    "href": "assignments/assignment-02-polynomial-effects.html#model-3-control-for-differences-in-gross-national-income-gni",
    "title": "Assignment 02",
    "section": "Model 3: Control for Differences in Gross National Income (GNI)",
    "text": "Model 3: Control for Differences in Gross National Income (GNI)\n\nRegress infant mortality rates on female education level. For this model, posit a quadratic effect of female education level on infant mortality rate, and also control for differences in Gross National Income (Model 3). (Use all four levels of GNI.) Write the fitted equation using Equation Editor (or some other program that correctly types mathematical expressions).\nCarry out a likelihood ratio test to compare Model 2 and Model 3. Add the results from this test to the table you created in Question #8."
  },
  {
    "objectID": "assignments/assignment-02-polynomial-effects.html#adopting-a-model",
    "href": "assignments/assignment-02-polynomial-effects.html#adopting-a-model",
    "title": "Assignment 02",
    "section": "Adopting a Model",
    "text": "Adopting a Model\n\nBased on the results of the two likelihood ratio tests, which model will you adopt? Explain.\nCreate the density plot of the marginal distribution of the standardized residuals for your adopted model, as well as the scatterplot of the standardized residuals versus the fitted values. Place these plots side-by-side in your printed document and, for the purposes of captioning, etc. treat them as two subfigures within a single figure.\nBased on the plots you created in Question 12, evaluate and comment on the tenability of each of the model assumptions."
  },
  {
    "objectID": "assignments/assignment-02-polynomial-effects.html#presenting-the-results",
    "href": "assignments/assignment-02-polynomial-effects.html#presenting-the-results",
    "title": "Assignment 02",
    "section": "Presenting the Results",
    "text": "Presenting the Results\n\nMimic the format and structure of either of the first two tables in the Presenting Results from Many Fitted Regression Models section of the document Creating Tables to Present Statistical Results to create a table to present the numerical information from the three models you fitted in this assignment. Make sure the table you create also has an appropriate caption. If the table is too wide, change the page orientation in your word processing program to “Landscape”, rather than changing the size of the font. (Note: Only this table should be presented in landscape orientation…not your entire assignment!)\nCreate a publication quality plot that displays the fitted curves from Model 3. Display four separate lines to show the effect of Gross National Income. The four lines should be displayed using different linetypes or colors (or both) so that they can be easily differentiated in the plot."
  },
  {
    "objectID": "assignments/assignment-03-evidence-and-model-selection.html",
    "href": "assignments/assignment-03-evidence-and-model-selection.html",
    "title": "Assignment 03",
    "section": "",
    "text": "The goal of this assignment is to build your understanding of using information criteria for model selection. In this assignment, you will use the data from the file wine.csv to examine several different predictors of wine rating (a measure of the wine’s quality). The literature has suggested that price of wine is quite predictive of a wine’s quality."
  },
  {
    "objectID": "assignments/assignment-03-evidence-and-model-selection.html#instructions",
    "href": "assignments/assignment-03-evidence-and-model-selection.html#instructions",
    "title": "Assignment 03",
    "section": "Instructions",
    "text": "Instructions\nSubmit either your QMD and HTML file or, if you are not using Quarto, a PDF file of your responses to the following questions. Please adhere to the following guidelines for further formatting your assignment:\n\nAll graphics should be resized so that they do not take up more room than necessary and should have an appropriate caption.\nAny typed mathematics (equations, matrices, vectors, etc.) should be appropriately typeset within the document using Markdown’s equation typesetting.\nAll syntax should be hidden (i.e., not displayed) unless specifically asked for.\nAny messages or warnings produced (e.g., from loading packages) should also be hidden.\n\nThis assignment is worth 15 points."
  },
  {
    "objectID": "assignments/assignment-03-evidence-and-model-selection.html#preparation",
    "href": "assignments/assignment-03-evidence-and-model-selection.html#preparation",
    "title": "Assignment 03",
    "section": "Preparation",
    "text": "Preparation\nRead the article: Snipes, M., & Taylor, D. C. (2014). Model selection and Akaike Information Criteria: An example from wine ratings and prices. Wine Economics and Policy, 3(1), 3–9. https://doi.org/10.1016/j.wep.2014.03.001\nYou will be carrying out a form of a robustness study to evaluate the working hypotheses proposed in Snipes and Taylor (2014). In this study, we will fit the same candidate models that Snipes and Taylor fitted in their analysis, however we will be using a different data set (e.g., our data includes more regions than Snipes and Taylor’s data). We will also make one change in the analysis, and that is we will treat vintage as a continuous variable; we won’t categorize it like Snipes and Taylor did. By using a different set of data and making slightly different analytic decisions we can more vigorously evaluate the underlying working hypotheses.\nFit the same nine candidate models that Snipes and Taylor fitted in their analysis, using the wine.csv data. In these models use wine rating (rating) as the outcome. Remember to treat vintage as a continuous variable in the models that include it. These models will be named Model 0, Model 1, …, Model 8 to be consistent with the naming in the Snipes and Taylor paper. Use these fitted models to answer the questions in the assignment."
  },
  {
    "objectID": "assignments/assignment-03-evidence-and-model-selection.html#model-selection-likelihood-framework-of-evidence",
    "href": "assignments/assignment-03-evidence-and-model-selection.html#model-selection-likelihood-framework-of-evidence",
    "title": "Assignment 03",
    "section": "Model Selection: Likelihood Framework of Evidence",
    "text": "Model Selection: Likelihood Framework of Evidence\n\nCompute and report the likelihood for Model 0 given the residuals and set of model assumptions. Use dnorm() for this computation, and show your syntax for full credit.\nCreate a table of the log-likelihoods for the nine candidate models. (Use the logLik() function to compute these values.)\nCompute and interpret the likelihood ratio for comparing the empirical support between Model 2 and Model 3.\nCan we carry out a likelihood ratio test to evaluate whether the amount of empirical support when comparing Model 2 and Model 3 is more than we expect because of sampling error? If so, compute and report the results from the \\(\\chi^2\\)-test. If not, explain why not.\nCompute and interpret the likelihood ratio for comparing the empirical support between Model 2 and Model 5.\nCan we carry out a likelihood ratio test to evaluate whether the amount of empirical support when comparing Model 2 and Model 5 is more than we expect because of sampling error? If so, compute and report the results from the \\(\\chi^2\\)-test. If not, explain why not."
  },
  {
    "objectID": "assignments/assignment-03-evidence-and-model-selection.html#model-selection-information-criteria",
    "href": "assignments/assignment-03-evidence-and-model-selection.html#model-selection-information-criteria",
    "title": "Assignment 03",
    "section": "Model Selection: Information Criteria",
    "text": "Model Selection: Information Criteria\n\nCreate a table of model evidence that includes the following information for each of the nine candidate models. (2pts.)\n\n\nModel\nLog-likelihood\nK\nAICc\n\\(\\Delta\\) AICc\nModel Probability (AICc Weight)\n\nUse this table of model evidence to answer Questions 8–14.\n\nUse the AICc values to select the working hypothesis with the most empirical evidence.\nInterpret the model probability (i.e., AICc weight) for the working hypothesis with the most empirical evidence.\nCompute and interpret the evidence ratio that compares the two working hypotheses with the most empirical evidence.\nBased on previous literature, Snipes and Taylor hypothesized that price was an important predictor of wine quality. Based on your analyses, is price an important predictor of wine quality? Justify your response by referring to the model evidence. (Hint: Pay attention to which models include price and which do not.)\nDoes the empirical evidence support adopting more than one working hypothesis? Justify your response by referring to the model evidence.\nDoes the empirical evidence from the Snipes and Taylor analyses support adopting more than one candidate model? Justify your response by by referring to the model evidence.\nBased on your responses to the last two questions, which set of analyses (yours or Snipes and Taylor) has more model selection uncertainty? Explain."
  },
  {
    "objectID": "assignments/assignment-03-evidence-and-model-selection.html#what-to-submit",
    "href": "assignments/assignment-03-evidence-and-model-selection.html#what-to-submit",
    "title": "Assignment 03",
    "section": "What to Submit",
    "text": "What to Submit\nYou need to submit a zipped version of your entire assignment-01 project directory. When the TA unzips this and opens your R project file they will render your QMD document. This should produce the HTML file that will be graded. (You can include your HTML file as an extra attachment if you want, but the QMD document will need to render. If it doesn’t render the TA will return it to you to try again.)"
  },
  {
    "objectID": "assignments/assignment-04-logarithmic-transformations.html",
    "href": "assignments/assignment-04-logarithmic-transformations.html",
    "title": "Assignment 04",
    "section": "",
    "text": "The goal of this assignment is to give you experience fitting, interpreting, and evaluating models with logarithmically transformed variables. In this assignment, you will use the data from the file wine.csv to examine several different predictors of wine rating (a measure of the wine’s quality). The literature has suggested that price of wine is quite predictive of a wine’s quality. You will be carrying out a replication study (using a different data set) of a study published by Snipes and Taylor (2014)."
  },
  {
    "objectID": "assignments/assignment-04-logarithmic-transformations.html#instructions",
    "href": "assignments/assignment-04-logarithmic-transformations.html#instructions",
    "title": "Assignment 04",
    "section": "Instructions",
    "text": "Instructions\nSubmit either your QMD and HTML file or, if you are not using Quarto, a PDF file of your responses to the following questions. Please adhere to the following guidelines for further formatting your assignment:\n\nAll graphics should be resized so that they do not take up more room than necessary and should have an appropriate caption.\nAny typed mathematics (equations, matrices, vectors, etc.) should be appropriately typeset within the document using Markdown’s equation typesetting.\nAll syntax should be hidden (i.e., not displayed) unless specifically asked for.\nAny messages or warnings produced (e.g., from loading packages) should also be hidden.\n\nThis assignment is worth 15 points."
  },
  {
    "objectID": "assignments/assignment-04-logarithmic-transformations.html#model-1-effect-of-wine-rating-on-price",
    "href": "assignments/assignment-04-logarithmic-transformations.html#model-1-effect-of-wine-rating-on-price",
    "title": "Assignment 04",
    "section": "Model 1: Effect of Wine Rating on Price",
    "text": "Model 1: Effect of Wine Rating on Price\n\nCreate and examine the scatterplot of the relationship between wine rating (predictor) and price. Include the loess smoother in this plot. Does this plot suggest any nonlinearity in the relationship between wine rating and price that we need to address?\nRegress the log-transformed price variable (using the natural logarithm) on wine rating (Model 1). Report and interpret the slope coefficient (using the log-metric) from the fitted model.\nReport and interpret the back-transformed slope coefficient from Model 1."
  },
  {
    "objectID": "assignments/assignment-04-logarithmic-transformations.html#effect-of-wine-rating-and-region-on-log-transformed-price",
    "href": "assignments/assignment-04-logarithmic-transformations.html#effect-of-wine-rating-and-region-on-log-transformed-price",
    "title": "Assignment 04",
    "section": "Effect of Wine Rating and Region on Log-Transformed Price",
    "text": "Effect of Wine Rating and Region on Log-Transformed Price\nFit two additional models:\n\nA model that includes the effects of whether or not the wine is from California (i.e., california) to predict variation in the log-transformed price (Model 2).\nA model that includes the effects of wine rating and whether or not the wine is from California (i.e., california) to predict variation in the log-transformed price (Model 3).\n\n\nInterpret the effect associated with california predictor (using the log-metric) from Model 3.\nReport and interpret the back-transformed coefficient associated with california predictor from Model 3.\n\nFit a model that includes both the the wine rating and california main effects, as well as, the interaction effect between those predictors to predict variation in the log-transformed price (Model 4).\n\nCreate a table to present the numerical information from the three models you fitted in this assignment along with the AICc values. (Mimic the Presenting Results from Many Fitted Regression Models section of the document Creating Tables to Present Statistical Results to create this table. Include the AICc value below the RMSE value in the table.) Make sure the table you create also has an appropriate caption. If the table is too wide, change the page orientation in your word processing program to “Landscape”, rather than changing the size of the font. (Note: Only this table should be presented in landscape orientation…not your entire assignment!) (3pts.)"
  },
  {
    "objectID": "assignments/assignment-04-logarithmic-transformations.html#adopting-a-final-candidate-model",
    "href": "assignments/assignment-04-logarithmic-transformations.html#adopting-a-final-candidate-model",
    "title": "Assignment 04",
    "section": "Adopting a “Final” Candidate Model",
    "text": "Adopting a “Final” Candidate Model\n\nBased on the model evidence, which of the candidate models will you adopt as your “final” model? Explain.\nWrite the fitted equation for the adopted candidate model.\nCreate and report a set of residual plots that allow you to evaluate the adopted model’s assumptions. Are the assumptions for the model satisfied? Explain. (2pts.)"
  },
  {
    "objectID": "assignments/assignment-04-logarithmic-transformations.html#presenting-the-results",
    "href": "assignments/assignment-04-logarithmic-transformations.html#presenting-the-results",
    "title": "Assignment 04",
    "section": "Presenting the Results",
    "text": "Presenting the Results\n\nCreate a publication quality plot that displays the fitted curve(s) from your adopted candidate model. If you show more than one curve, each line should be easily differentiated in the plot. (Note: Make sure that you back-transform any log-transformed variables when you create this plot.) (2pts.)\nUse the plot to help describe/interpret the effect of wine rating on price."
  },
  {
    "objectID": "assignments/assignment-05-logistic-regression.html",
    "href": "assignments/assignment-05-logistic-regression.html",
    "title": "Assignment 05",
    "section": "",
    "text": "This goal of this assignment is to give you experience working with working with logistic regression models to analyze dichotomous outcome data. In this assignment, you will use the data from the file same-sex-marriage.csv to examine the effects of two aspects of religion (denomination, and frequency of attendance of religious services) on the support of same-sex marriage."
  },
  {
    "objectID": "assignments/assignment-05-logistic-regression.html#instructions",
    "href": "assignments/assignment-05-logistic-regression.html#instructions",
    "title": "Assignment 05",
    "section": "Instructions",
    "text": "Instructions\nSubmit either your QMD and HTML file or, if you are not using Quarto, a PDF file of your responses to the following questions. Please adhere to the following guidelines for further formatting your assignment:\n\nAll graphics should be resized so that they do not take up more room than necessary and should have an appropriate caption.\nAny typed mathematics (equations, matrices, vectors, etc.) should be appropriately typeset within the document using Markdown’s equation typesetting.\nAll syntax should be hidden (i.e., not displayed) unless specifically asked for.\nAny messages or warnings produced (e.g., from loading packages) should also be hidden.\n\nThis assignment is worth 15 points."
  },
  {
    "objectID": "assignments/assignment-05-logistic-regression.html#effect-of-religious-service-attendance",
    "href": "assignments/assignment-05-logistic-regression.html#effect-of-religious-service-attendance",
    "title": "Assignment 05",
    "section": "Effect of Religious Service Attendance",
    "text": "Effect of Religious Service Attendance\nYou will begin the analysis by examining the effect of religious service attendance on support of same-sex marriage. Because the data for this predictor come from a Likert scale (ordinal in nature), we need to examine whether we can treat it as a continuous predictor in the model, or whether we should treat it as categorical.\n\nBegin by computing the empirical proportion of people that support same-sex marriage for each of the attendance categories. Create a line plot that shows the relationship between proportion of support and attendance.\n\nBased on the plot you just created, the relationship between proportion of support and attendance seems linear. Because of this, we can treat the Likert data as continuous; using a line (or polynomial) to fit the relationship. The only caution being that when interpreting a slope, we say something like, “a one-unit difference in \\(X\\) is associated with a \\(\\hat\\beta_1\\)-unit difference in \\(Y\\)”. For ordinal (Likert) data a one-unit difference in \\(X\\) really indicates a shift from one category to the next highest category.\n\nFit two logistic models to the data using religious service attendance. In the first model, only include the linear effect of attendance. In the second model, include both a linear and quadratic effect to predict variation in support for same-sex marriage. Which model should be adopted (linear or quadratic)? Justify your response by providing any statistical evidence you used in reaching your decision. (Note: The model you adopt here will be henceforth referred to as Model 1.)\n\n\n\n\nWrite the fitted equation for Model 1. (Don’t forget to include all appropriate subscripts. Also define any terms in the model that are ambiguous.)\nUse the fitted equation for Model 1 to predict the (a) log-odds, (b) odds, and (c) probability of someone supporting same-sex marriage if that person attends religious services almost every week."
  },
  {
    "objectID": "assignments/assignment-05-logistic-regression.html#effect-of-denomination",
    "href": "assignments/assignment-05-logistic-regression.html#effect-of-denomination",
    "title": "Assignment 05",
    "section": "Effect of Denomination",
    "text": "Effect of Denomination\n\nAre there sample differences in the proportion of people who support same-sex marriage between the denominations? Explain by using evidence from the sample data.\n\nFit a logistic model to the data using denomination to predict variation in support for same-sex marriage (Model 2). In this model, use Protestant as the reference group.\n\nInterpret the effect associated with the Jewish coefficient in terms of (a) log-odds, and (b) odds."
  },
  {
    "objectID": "assignments/assignment-05-logistic-regression.html#effect-of-attendance-and-denomination",
    "href": "assignments/assignment-05-logistic-regression.html#effect-of-attendance-and-denomination",
    "title": "Assignment 05",
    "section": "Effect of Attendance and Denomination",
    "text": "Effect of Attendance and Denomination\nFit the logistic model that includes all the adopted effects for religous service attendance and the effects of denomination to predict variation in support of same-sex marriage. (Note: This model will be referred to as Model 3.)\n\nCompute and report one of the pseudo-\\(R^2\\) values for Model 3. Also provide an interpretation of this measure."
  },
  {
    "objectID": "assignments/assignment-05-logistic-regression.html#adding-covariates",
    "href": "assignments/assignment-05-logistic-regression.html#adding-covariates",
    "title": "Assignment 05",
    "section": "Adding Covariates",
    "text": "Adding Covariates\nNow you will examine three potential covariates (friends, age, and female) that have been linked in the substantive literature to support of same-sex marriage.\n\nTo help think about which covariates should be included in your model, create a correlation matrix of the outcome, and all three covariates. By referring to this matrix, clearly indicate the order in which you will add covariates into the model.\n\nFit three logistic models based on the order of importance of the three covariates that also include the effects of religious attendance, and denomination. For example, the first of these three models would include the effects of religious attendance, denomination, and the most important covariate you identifed in Question 8. The second model would include the effects of religious attendance, denomination, and the two most important covariates you identified. Finally, the third model would include the effects of religious attendance, denomination, and all three covariates.\n\nWhich of these three models should be adopted as you final model (or set of models)? Justify your response by providing any statistical evidence you used in reaching your decision. (2pts.)\nCreate a table of results from your set of fitted models. This table should include Models 1–3 and also any model(s) you adopted from the previous question. Like other regression tables you have created, be sure to include the estimated coefficients and standard errors for each of the effects included in the models. (To be consistent with ASA recommendations, do not include p-values or stars.) Also include the AICc values for each model. (2pts.)"
  },
  {
    "objectID": "assignments/assignment-05-logistic-regression.html#interpreting-the-final-adopted-model",
    "href": "assignments/assignment-05-logistic-regression.html#interpreting-the-final-adopted-model",
    "title": "Assignment 05",
    "section": "Interpreting the Final Adopted Model",
    "text": "Interpreting the Final Adopted Model\n\nCreate a plot that visually displays the results of your final adopted fitted model. Be sure to visually show the effects the focal predictors (religious service attendance and denomination). Also show any pertinent covariates you think are necessary to include. (Think about how the inclusion of the covariates help readers better understand the effects of the focal predictors.) (2pts.)\nWrite a few sentences that tell the data narrative about the effects of religious service attendance and denomination on the support of same-sex marriage. Use the models in your table of model results to help create this narrative. Keep the focus on the focal variables in this narrative."
  },
  {
    "objectID": "assignments/assignment-06-lmer-unconditional-longitudinal-models.html",
    "href": "assignments/assignment-06-lmer-unconditional-longitudinal-models.html",
    "title": "Assignment 06",
    "section": "",
    "text": "This goal of this assignment is to give you experience working with mixed-effects regression models to analyze longitudinal data. In this assignment, you will use the data from the file nhl.csv to examine longitudinal variation in cost of attending an NHL game."
  },
  {
    "objectID": "assignments/assignment-06-lmer-unconditional-longitudinal-models.html#instructions",
    "href": "assignments/assignment-06-lmer-unconditional-longitudinal-models.html#instructions",
    "title": "Assignment 06",
    "section": "Instructions",
    "text": "Instructions\nSubmit either your QMD and HTML file or, if you are not using Quarto, a PDF file of your responses to the following questions. Please adhere to the following guidelines for further formatting your assignment:\n\nAll graphics should be resized so that they do not take up more room than necessary and should have an appropriate caption.\nAny typed mathematics (equations, matrices, vectors, etc.) should be appropriately typeset within the document using Markdown’s equation typesetting.\nAll syntax should be hidden (i.e., not displayed) unless specifically asked for.\nAny messages or warnings produced (e.g., from loading packages) should also be hidden.\n\nThis assignment is worth 13 points."
  },
  {
    "objectID": "assignments/assignment-06-lmer-unconditional-longitudinal-models.html#preparation",
    "href": "assignments/assignment-06-lmer-unconditional-longitudinal-models.html#preparation",
    "title": "Assignment 06",
    "section": "Preparation",
    "text": "Preparation\nAfter importing the data set, create a new variable called c_year that centers the year values at 2002. To center a variable we subtract the value we want to center at. (e.g., \\(\\mathtt{c\\_year}=\\mathtt{year}-2002\\)). This variable will represent the number of years since 2002.\nFor all analyses in this assignment, unless otherwise requested, use the c_year variable and not the year variable."
  },
  {
    "objectID": "assignments/assignment-06-lmer-unconditional-longitudinal-models.html#data-exploration",
    "href": "assignments/assignment-06-lmer-unconditional-longitudinal-models.html#data-exploration",
    "title": "Assignment 06",
    "section": "Data Exploration",
    "text": "Data Exploration\n\nCreate and report a plot to display the cost of attending an NHL game (FCI) over time for each team (team profiles). In this plot, all teams should be in the same panel. Also add the profile based on the mean cost of attending an NHL game FCI over time. Make the teams’ profiles slightly transparent so that the mean profile is easily visible.\nBased on the average growth profile, describe how the average cost of attending an NHL game has changed over time. Your description of the growth pattern should allude to the functional form for the model."
  },
  {
    "objectID": "assignments/assignment-06-lmer-unconditional-longitudinal-models.html#fitting-and-evaluating-models",
    "href": "assignments/assignment-06-lmer-unconditional-longitudinal-models.html#fitting-and-evaluating-models",
    "title": "Assignment 06",
    "section": "Fitting and Evaluating Models",
    "text": "Fitting and Evaluating Models\nFit the following three models. For each of the growth models, include only a random-effect of intercept in the model. Use maximum likelihood estimation to fit all the models.\n\nModel A: Unconditional random intercepts model\nModel B: Unconditional linear growth model\nModel C: Unconditional quadratic growth model\n\n\nCreate and report a table that includes the estimated variance components for each of the three fitted models.\nUse the estimated variance components from Model A to determine the proportion of variation unaccounted for at the between- and within-team levels. Report these values.\nUse the estimated variance components to determine the variation accounted for at the between- and within-team levels based on Model B. Report these values.\nWhich source of variation did Model B most account for? Explain why you would expect this based on the predictor included in Model B."
  },
  {
    "objectID": "assignments/assignment-06-lmer-unconditional-longitudinal-models.html#adopting-and-evaluating-an-unconditional-growth-model",
    "href": "assignments/assignment-06-lmer-unconditional-longitudinal-models.html#adopting-and-evaluating-an-unconditional-growth-model",
    "title": "Assignment 06",
    "section": "Adopting and Evaluating an Unconditional Growth Model",
    "text": "Adopting and Evaluating an Unconditional Growth Model\n\nBased on the AICc, which model (A, B, or C) has the most empirical evidence? Explain.\nCreate and report a density plot of the residuals and a scatterplot of the residuals versus the fitted values for the model you identified in Question 8. You can use the augment() function from {broom.mixed} to obtain both the fitted values (.fitted) and residuals (.resid). We evaluate these residual plots using the same methods we do when we fit an lm() model, so add any confidence envelopes, smoothers, or other guides to these plots that will aid in your evaluation.\nEvaluate the assumptions of normality, homogeneity of variance, and that the average residual is equal to zero based on the plots you created in Question 9."
  },
  {
    "objectID": "assignments/assignment-06-lmer-unconditional-longitudinal-models.html#reporting-the-results-from-the-adopted-model",
    "href": "assignments/assignment-06-lmer-unconditional-longitudinal-models.html#reporting-the-results-from-the-adopted-model",
    "title": "Assignment 06",
    "section": "Reporting the Results from the Adopted Model",
    "text": "Reporting the Results from the Adopted Model\n\nWrite the global fitted equation for the model you adopted in Question 7.\nWrite the team-specific fitted equation for the Minnesota Wild based on the model you adopted in Question 7.\nCreate a plot showing the predicted cost of attending an NHL game over time based on the global fitted model you reported in Question 10. Be sure to include a caption.\nAdd a line to the plot you created in Question #12 (don’t re-create the plot) showing the predicted cost of attending a Minnesota Wild game. Update the caption by adding an additional sentence to differentiate the Minnesota Wild curve from the global curve."
  },
  {
    "objectID": "assignments/assignment-07-longitudinal-analysis-ii.html",
    "href": "assignments/assignment-07-longitudinal-analysis-ii.html",
    "title": "Assignment 07",
    "section": "",
    "text": "This goal of this assignment is to give you more experience working with mixed-effects regression models to analyze longitudinal data. In this assignment, you will again use the data from the file nhl.csv to examine longitudinal variation in cost of attending an NHL game."
  },
  {
    "objectID": "assignments/assignment-07-longitudinal-analysis-ii.html#instructions",
    "href": "assignments/assignment-07-longitudinal-analysis-ii.html#instructions",
    "title": "Assignment 07",
    "section": "Instructions",
    "text": "Instructions\nSubmit either your QMD and HTML file or, if you are not using Quarto, a PDF file of your responses to the following questions. Please adhere to the following guidelines for further formatting your assignment:\n\nAll graphics should be resized so that they do not take up more room than necessary and should have an appropriate caption.\nAny typed mathematics (equations, matrices, vectors, etc.) should be appropriately typeset within the document using Markdown’s equation typesetting.\nAll syntax should be hidden (i.e., not displayed) unless specifically asked for.\nAny messages or warnings produced (e.g., from loading packages) should also be hidden.\n\nThis assignment is worth 12 points."
  },
  {
    "objectID": "assignments/assignment-07-longitudinal-analysis-ii.html#preparation",
    "href": "assignments/assignment-07-longitudinal-analysis-ii.html#preparation",
    "title": "Assignment 07",
    "section": "Preparation",
    "text": "Preparation\nAfter importing the data set, create a new variable called c_year that centers the year values at 2002. This variable will represent the number of years since 2002. For all analyses in this assignment, unless otherwise requested, use the c_year variable and not the year variable."
  },
  {
    "objectID": "assignments/assignment-07-longitudinal-analysis-ii.html#data-exploration",
    "href": "assignments/assignment-07-longitudinal-analysis-ii.html#data-exploration",
    "title": "Assignment 07",
    "section": "Data Exploration",
    "text": "Data Exploration\n\nCreate and report a plot to display the cost of attending an NHL game (FCI) over time facetted on whether or not high school hockey is a tradition in the team’s location. This plot should include the average and team profiles for each facet. Make the teams’ profiles slightly transparent so that the mean profile is easily visible.\nDo the teams’ profiles indicate that a random-effect of linear growth should be included in the candidate models? Explain.\nBased on the plot created in Question #1, indicate whether models that include an effect of high school hockey tradition should be included in the candidate set of models. If this effect should be included, does the data suggest a main-effect or an interaction-effect with time? Explain. (2pts)"
  },
  {
    "objectID": "assignments/assignment-07-longitudinal-analysis-ii.html#fitting-and-evaluating-models",
    "href": "assignments/assignment-07-longitudinal-analysis-ii.html#fitting-and-evaluating-models",
    "title": "Assignment 07",
    "section": "Fitting and Evaluating Models",
    "text": "Fitting and Evaluating Models\nFit the following five models:\n\nModel A: Unconditional random intercepts model\nModel B: Unconditional growth model (with only intercept random-effect)\nModel C: Unconditional growth model (with both intercept and linear growth random-effects)\nModel D: Conditional growth model with main-effect of high school hockey (with both intercept and linear growth random-effects)\nModel E: Conditional growth model with interaction-effect between high school hockey and time (with both intercept and linear growth random-effects)\n\nUse maximum likelihood estimation to fit all the models.\n\nCreate and report a table of regression results. In this table Include the following information for each of the five candidate models (5pts):\n\nEstimated fixed-effect coefficients and standard errors;\nEstimated variance components;\nModel-level deviance and AICc measures; and\nAny notes necessary to help the reader"
  },
  {
    "objectID": "assignments/assignment-07-longitudinal-analysis-ii.html#most-empirically-supported-model",
    "href": "assignments/assignment-07-longitudinal-analysis-ii.html#most-empirically-supported-model",
    "title": "Assignment 07",
    "section": "Most Empirically Supported Model",
    "text": "Most Empirically Supported Model\nBased on the AICc values use the model with the most empirical evidence to answer the remaining questions.\n\nWrite the multilevel equations for the statistical model (not the fitted equations) for the model with the most empirical evidence. Don’t forget to include the assumptions!\nWrite the team-specific fitted equation for the Minnesota Wild based on the model that has the most empirical evidence.\nUse the results from the fitted model with the most empirical evidence to create a plot that displays the predicted average cost of attending an NHL game as a function of time for teams that have a tradition of high school and those that do not.\nAdd a line to the plot you created in Question #7 (don’t re-create the plot) showing the predicted cost of attending a Minnesota Wild game.\nWrite a caption for your plot (3–4 sentences) that help a reader understand the effect of time and high school hockey tradition on cost of attending an NHL game and also how the cost of attending a Wild game compares to the average team over time. (2pts)"
  },
  {
    "objectID": "codebooks/carbon.html",
    "href": "codebooks/carbon.html",
    "title": "carbon.csv",
    "section": "",
    "text": "Carbon dioxide emissions are the primary driver of global climate change. Some of the biggest predictors of these emissions include economic growth, industrialization, and urbanization. The data in carbon.csv are from 2017 and include some predictors of carbon dioxide emissions for several countries around the world. The variables are:\n\ncountry: Country name\nregion: Region of the world\nco2: Carbon dioxide measure from the burning of fossil fuels (metric tons per person)\nwealth: A measure of a country’s wealth based on its GDP; higher values indicate wealthier countries\nurbanization: Annual urban population growth (as a percentage)\n\n\nPreview\n\n# Import data\ncarbon = readr::read_csv(file = \"https://raw.githubusercontent.com/zief0002/bespectacled-antelope/main/data/carbon.csv\")\n\n# View data\ncarbon\n\n# A tibble: 189 × 5\n   country             region      co2 wealth urbanization\n   &lt;chr&gt;               &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;        &lt;dbl&gt;\n 1 Afghanistan         Asia      0.254   1.07        3.35 \n 2 Albania             Europe    1.59    3.75        1.32 \n 3 Algeria             Africa    3.69    3.54        2.81 \n 4 Angola              Africa    1.12    2.71        4.31 \n 5 Antigua and Barbuda Americas  5.88    4.36        0.432\n 6 Argentina           Americas  4.41    4.49        1.15 \n 7 Armenia             Europe    1.89    3.72        0.309\n 8 Australia           Oceania  16.9     5.60        1.66 \n 9 Austria             Europe    7.75    5.82        0.836\n10 Azerbaijan          Europe    3.7     3.79        1.47 \n# ℹ 179 more rows\n\n\n\n\nReferences\nWorld Bank (2019). World Bank open data."
  },
  {
    "objectID": "codebooks/fertility.html",
    "href": "codebooks/fertility.html",
    "title": "fertility.csv",
    "section": "",
    "text": "Human overpopulation is a growing concern and has been associated with depletion of Earth’s natural resources (water is a big one that ) and degredation of the environment. This, in turn, has social and economic consequences such as global tension over resources such as water and food, higher cost of living and higher unemployment rates. The data in fertility.csv were collected from several sources (e.g., World Bank) and are thought to correlate with fertility rates, a measure directly linked to population. The variables are:\n\ncountry: Country name\nregion: Region of the world\nfertility_rate: Average number of children that would be born to a woman if she were to live to the end of her childbearing years and bear children in accordance with age-specific fertility rates.\neduc_female: Average number of years of formal education (schooling) for females\ninfant_mortality: Number of infants dying before reaching one year of age, per 1,000 live births in a given year.\ncontraceptive: Percentage of women who are practicing, or whose sexual partners are practicing, any form of contraception. It is usually measured for women ages 15–49 who are married or in union.\ngni_class: Categorization based on country’s gross national income per capita (calculated using the World Bank Atlas method)\n\nLow: Low-income economies; GNI per capita of $1,025 or less;\nLow/Middle: Lower-middle-income economies; GNI per capita between $1,026 and $3,995;\nUpper/Middle: Upper middle-income economies; GNI per capita between $3,996 and $12,375;\nUpper: High-income economies; GNI per capita of $12,376 or more.\n\nhigh_gni: Dummy variable indicating if the country is has an upper-middle or high income economy (low- or low/middle-income = 0; upper/middle or upper income = 1)\n\n\n\nPreview\n\n# Import Data\nfertility = readr::read_csv(file = \"https://raw.githubusercontent.com/zief0002/rustic-penguin/main/data/fertility.csv\")\n\n# View data\nfertility\n\n# A tibble: 124 × 8\n   country      region fertility_rate educ_female infant_mortality contraceptive\n   &lt;chr&gt;        &lt;chr&gt;           &lt;dbl&gt;       &lt;dbl&gt;            &lt;dbl&gt;         &lt;dbl&gt;\n 1 Albania      Europ…           1.49         9.1             15              46\n 2 Algeria      Middl…           2.78         5.9             17.2            57\n 3 Armenia      Europ…           1.39        10.8             14.7            57\n 4 Austria      Europ…           1.42         8.9              3.3            66\n 5 Azerbaijan   Europ…           1.92        10.5             30.8            55\n 6 Bahamas, The Latin…           1.97        11.1             13.9            45\n 7 Bangladesh   South…           2.5          4.6             33.1            62\n 8 Belgium      Europ…           1.65        10.5              3.4            67\n 9 Belize       Latin…           3.08         9.2             15.7            51\n10 Benin        Sub-S…           5.13         2               58.5            16\n# ℹ 114 more rows\n# ℹ 2 more variables: gni_class &lt;chr&gt;, high_gni &lt;dbl&gt;\n\n\n\n\n\nReferences\nRoser, M. (2017). Fertility rate. Our world in data.\nUNICEF. (2016). State of the world’s children 2016. United Nations Population Division’s World Contraceptive Use, household surveys including Demographic and Health Surveys and Multiple Indicator Cluster Surveys.\nWorld Bank (2019). World Bank open data."
  },
  {
    "objectID": "codebooks/graduation.html",
    "href": "codebooks/graduation.html",
    "title": "graduation.csv",
    "section": "",
    "text": "The data in graduation.csv include student-level attributes for \\(n=2,344\\) randomly sampled students who were first-year, full-time students from the 2002 cohort at a large, midwestern research university. Any students who transferred to another institution were removed from the data. The source of these data is Jones-White, Radcliffe, Lorenz, & Soria (2014). The attributes, collected for these students are:\n\nstudent: Student ID number in the dataset\ndegree: Did the student obtain a degree (i.e., graduate) from the institution? (No; Yes)\nact: Student’s ACT score (If the student reported a SAT score, a concordance table was used to transform the score to a comparable ACT score.)\nscholarship: Amount of scholarship offered to student (in thousands of dollars)\nap_courses: Number of Advanced Placement credits at time of enrollment\nfirst_gen: Is the student a first generation college student? (No; Yes)\nnon_traditional: Is the student a non-traditional student (older than 19 years old at the time of enrollment)? (No; Yes)\n\n\nPreview\n\n# Import data\ngraduation = readr::read_csv(file = \"https://raw.githubusercontent.com/zief0002/bespectacled-antelope/main/data/graduation.csv\")\n\n# View data\ngraduation\n\n# A tibble: 2,344 × 7\n   student degree   act scholarship ap_courses first_gen non_traditional\n     &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;          \n 1       1 Yes       21         0            0 No        No             \n 2       2 Yes       19         0            0 No        No             \n 3       3 Yes       27         0            0 Yes       No             \n 4       4 Yes       25         0.5          0 Yes       No             \n 5       5 No        28         0           17 Yes       No             \n 6       6 Yes       21         0            0 No        Yes            \n 7       7 Yes       27         0            8 Yes       No             \n 8       8 No        20         0            0 No        No             \n 9       9 Yes       26         0            0 Yes       No             \n10      10 Yes       25         0            4 Yes       No             \n# ℹ 2,334 more rows\n\n\n\n\n\n\n\n\n\n\nReferences\n\nJones-White, D. R., Radcliffe, P. M., Lorenz, L. M., & Soria, K. M. (2014). Priced out?: The influence of financial aid on the educational trajectories of first-year students starting college at a large research university. Research in Higher Education, 55(4), 329–350."
  },
  {
    "objectID": "codebooks/mammals.html",
    "href": "codebooks/mammals.html",
    "title": "mammals.csv",
    "section": "",
    "text": "The data in mammals.csv come from Allison & Cicchetti (1976) and contain data on 62 species of mammals. The attributes include:\n\nspecies: Common name of species\nbody_weight: Body weight, in kg\nbrain_weight: Brain weight, in g\nslow_wave: Average slow wave (non-dreaming) sleep per day, in hours\nparadox: Average paradoxical (dreaming) sleep per day, in hours\ntotal_sleep: Average sleep per day, in hours\nlifespan: Average lifespan, in years\ngestation: Average gestation period, in days\npredation: Rating of degree to which species are preyed upon, 1 (low)–5 (high)\nexposure: Rating of sleep exposure from 1 (low exposure; e.g., sleep in a burrow or den)–5 (maximally exposed sleep)\ndanger: Rating of predatory danger based on predation and exposure scales, 1 (low danger)–5 (high degreee of danger)\n\n\nPreview\n\n# Import data\nmammals = readr::read_csv(file = \"https://raw.githubusercontent.com/zief0002/bespectacled-antelope/main/data/mammals.csv\")\n\n# View data\nmammals\n\n# A tibble: 62 × 11\n   species       body_weight brain_weight slow_wave paradox total_sleep lifespan\n   &lt;chr&gt;               &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;\n 1 African elep…    6654           5712        NA      NA           3.3     38.6\n 2 African gian…       1              6.6       6.3     2           8.3      4.5\n 3 Arctic fox          3.38          44.5      NA      NA          12.5     14  \n 4 Arctic groun…       0.92           5.7      NA      NA          16.5     NA  \n 5 Asian elepha…    2547           4603         2.1     1.8         3.9     69  \n 6 Baboon             10.6          180.        9.1     0.7         9.8     27  \n 7 Big brown bat       0.023          0.3      15.8     3.9        19.7     19  \n 8 Brazilian ta…     160            169         5.2     1           6.2     30.4\n 9 Cat                 3.3           25.6      10.9     3.6        14.5     28  \n10 Chimpanzee         52.2          440         8.3     1.4         9.7     50  \n# ℹ 52 more rows\n# ℹ 4 more variables: gestation &lt;dbl&gt;, predation &lt;dbl&gt;, exposure &lt;dbl&gt;,\n#   danger &lt;dbl&gt;\n\n\n\n\n\n\n\n\n\n\nReferences\n\nAllison, T., & Cicchetti, D. V. (1976). Sleep in mammals: Ecological and constitutional correlates. Science, 194(4266), 732–734. doi:10.1126/science.982039"
  },
  {
    "objectID": "codebooks/minneapolis.html",
    "href": "codebooks/minneapolis.html",
    "title": "minneapolis.csv",
    "section": "",
    "text": "The data in minneapolis.csv were provided in Long (2012). They constitute a sample of \\(n=22\\) students taken from a much larger dataset collected by the Minneapolis Public School District. The data were collected to comply with the No Child Left Behind Act of 2001 and began during the 2004-05 school year. The variables included in the data are:\n\nstudent_id: De-identified student ID number\nreading_score: Reading achievement score\\(^\\dagger\\).\ngrade: Grade-level.\nspecial_ed: Is the student recieving special education services?\nattendance: Proportion of attendance based on the number of school days the student attended school during the four year of the study.\n\n\\(^\\dagger\\)The reading achievement scores are based on the reading section of th Northwest Achievement Levels Test (NALT), a multiple-choice, adaptive assessment of students’ academic achievement. The NALT raw scores were converted to vertically equated scaled scores using an IRT model. Higher scale scores indicate more reading achievement.\n\nPreview\n\n# Import data\nmpls = readr::read_csv(file = \"https://raw.githubusercontent.com/zief0002/bespectacled-antelope/main/data/minneapolis.csv\")\n\n# View data\nmpls\n\n# A tibble: 88 × 5\n   student_id reading_score grade special_ed attendance\n        &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;\n 1          1           172     5 No               0.94\n 2          1           185     6 No               0.94\n 3          1           179     7 No               0.94\n 4          1           194     8 No               0.94\n 5          2           200     5 No               0.91\n 6          2           210     6 No               0.91\n 7          2           209     7 No               0.91\n 8          2           210     8 No               0.91\n 9          3           191     5 No               0.97\n10          3           199     6 No               0.97\n# ℹ 78 more rows\n\n\n\n\n\n\n\n\n\n\nReferences\n\nLong, J. D. (2012). Longitudinal data analysis for the behavioral sciences using R. Thousand Oaks, CA: Sage Publications, Inc."
  },
  {
    "objectID": "codebooks/mn-schools.html",
    "href": "codebooks/mn-schools.html",
    "title": "mn-schools.csv",
    "section": "",
    "text": "The data in mnSchools.csv were collected from http://www.collegeresults.org and contain 2011 institutional data for \\(n=33\\) Minnesota colleges and universities. The attributes include:\n\nname: College/university name\ngrad: Six-year graduation rate (as a percentage)\nsector: Educational sector (Public; Private)\nsat: Estimated median composite SAT score (in hundreds)\ntuition: Amount of tuition and required fees covering a full academic year for a typical student (in thousands of U.S. dollars)\n\n\n\nPreview\n\n# Import Data\nmn = readr::read_csv(file = \"https://raw.githubusercontent.com/zief0002/rustic-penguin/main/data/mn-schools.csv\")\n\n# View data\nmn\n\n# A tibble: 33 × 5\n   name                               grad sector    sat tuition\n   &lt;chr&gt;                             &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1 Augsburg College                   65.2 Private  1030    39.3\n 2 Bethany Lutheran College           52.6 Private  1065    30.5\n 3 Bethel University, Saint Paul, MN  73.3 Private  1145    39.4\n 4 Carleton College                   92.6 Private  1400    54.3\n 5 College of Saint Benedict          81.1 Private  1185    43.2\n 6 Concordia College at Moorhead      69.4 Private  1145    36.6\n 7 Concordia University-Saint Paul    47.9 Private   990    37.8\n 8 Crossroads College                 26.9 Private   970    25.3\n 9 Crown College                      51.3 Private  1030    33.2\n10 Gustavus Adolphus College          81.7 Private  1225    43.8\n# ℹ 23 more rows"
  },
  {
    "objectID": "codebooks/nhl.html",
    "href": "codebooks/nhl.html",
    "title": "nhl.csv",
    "section": "",
    "text": "Each season, Team Marketing Report (TMR) computes the cost of taking a family of four to a professional sports contest for each of the major sporting leagues. Costs are determined by telephone calls with representatives of the teams, venues and concessionaires. Identical questions were asked in all interviews. Prices for Canadian teams were converted to U.S. dollars and comparison prices were converted using a recent exchange rate.\nThe data in nhl.csv includes data on the cost of attending an NHL game over nine seasons for the current 31 NHL teams. The attributes include:\n\nteam: NHL team name\nfci: Fan cost index (FCI) for each season. There are no data for 2012, since that year the NHL was locked out. The FCI comprises the prices of four (4) average-price tickets, two (2) small draft beers, four (4) small soft drinks, four (4) regular-size hot dogs, parking for one (1) car, two (2) game programs and two (2) least-expensive, adult-size adjustable caps. Costs were determined by telephone calls with representatives of the teams, venues and concessionaires. Identical questions were asked in all interviews.\nyear: NHL season (e.g., 2002 indicates the 2002–2003 NHL season)\nhs_hockey: An dummy coded variable that indicates whether there is state organized high school hockey in the team’s location (0 = no; 1 = yes). This is a proxy for whether there is a hockey tradition in the team’s location.\n\n\nPreview\n\n# Import data\nnhl = readr::read_csv(file = \"https://raw.githubusercontent.com/zief0002/bespectacled-antelope/main/data/nhl.csv\")\n\n# View data\nnhl\n\n# A tibble: 279 × 4\n   team              fci  year hs_hockey\n   &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n 1 Anaheim Ducks    212.  2002         0\n 2 Anaheim Ducks    229.  2003         0\n 3 Anaheim Ducks    211.  2006         0\n 4 Anaheim Ducks    260.  2007         0\n 5 Anaheim Ducks    274.  2008         0\n 6 Anaheim Ducks    285.  2010         0\n 7 Anaheim Ducks    239.  2011         0\n 8 Anaheim Ducks    285.  2013         0\n 9 Anaheim Ducks    289.  2014         0\n10 Arizona Coyotes  214.  2002         0\n# ℹ 269 more rows"
  },
  {
    "objectID": "codebooks/riverview.html",
    "href": "codebooks/riverview.html",
    "title": "riverview.csv",
    "section": "",
    "text": "The data in riverview.csv come from Lewis-Beck & Lewis-Beck (2016) and contain five attributes collected from a random sample of \\(n=32\\) employees working for the city of Riverview, a hypothetical midwestern city. The attributes include:\n\neducation: Years of formal education\nincome: Annual income (in thousands of U.S. dollars)\nseniority: Years of seniority\ngender: Employee’s gender\nparty: Political party affiliation\n\n\n\nPreview\n\n# Import Data\ncity = readr::read_csv(file = \"https://raw.githubusercontent.com/zief0002/rustic-penguin/main/data/riverview.csv\")\n\n# View data\ncity\n\n# A tibble: 32 × 5\n   education income seniority gender     party      \n       &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      \n 1         8   26.4         9 female     Independent\n 2         8   37.4         7 Not female Democrat   \n 3        10   34.2        16 female     Independent\n 4        10   25.5         1 female     Republican \n 5        10   47.0        14 Not female Democrat   \n 6        12   46.5        11 female     Democrat   \n 7        12   52.5        16 female     Independent\n 8        12   37.7        14 Not female Democrat   \n 9        12   50.3        24 Not female Democrat   \n10        14   32.6         5 female     Independent\n# ℹ 22 more rows\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\nLewis-Beck, C., & Lewis-Beck, M. (2016). Applied regression: An introduction (2nd ed.). Thousand Oaks, CA: Sage."
  },
  {
    "objectID": "codebooks/same-sex-marriage.html",
    "href": "codebooks/same-sex-marriage.html",
    "title": "same-sex-marriage.csv",
    "section": "",
    "text": "The data in same-sex-marriage.csv were collected from the 2008 American National Election Study, conducted jointly by the University of Michigan and Stanford University. These particular data consist of 1,746 American’s responses. The attributes are:\n\nsupport: Dummy-coded variable indicating whether the respondent supports gay marriage? (1=Yes; 0=No)\nattendance: Frequency the respondent attends religious services (0=Never; 1=Few times a year; 2=Once or twice a month; 3=Almost every week; 4=Every week)\ndenomination: Respondent’s religious denomination? (Catholic; Jewish; Protestant; Other)\nfriends: Does the respondent have family or friends that are LGBT? (1=Yes; 0=No)\nage: Respondent’s age, in years\nfemale: Dummy-coded variable indicating whether the respondent is female (1=Yes; 0=No)\n\n\nPreview\n\n# Import data\nsame_sex = readr::read_csv(file = \"https://raw.githubusercontent.com/zief0002/bespectacled-antelope/main/data/same-sex-marriage.csv\")\n\n# View data\nsame_sex\n\n# A tibble: 1,746 × 6\n   support attendance denomination friends   age female\n     &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1       0          3 Protestant         0    58      1\n 2       0          4 Protestant         0    39      1\n 3       0          1 Catholic           1    50      0\n 4       0          3 Protestant         0    72      0\n 5       0          0 Other              0    71      1\n 6       0          4 Protestant         1    66      1\n 7       0          4 Protestant         0    56      0\n 8       0          4 Protestant         0    40      1\n 9       0          4 Protestant         0    55      0\n10       0          2 Protestant         1    84      1\n# ℹ 1,736 more rows"
  },
  {
    "objectID": "codebooks/vocabulary.html",
    "href": "codebooks/vocabulary.html",
    "title": "vocabulary.csv",
    "section": "",
    "text": "The data in vocabulary.csv, adapted from data provided by Bock (1975), come from the Laboratory School of the University of Chicago and include scaled test scores across four grades from the vocabulary section of the Cooperative Reading Test for \\(n=64\\) students. The attributes in the dataset include:\n\nid: The student ID number\nvocab_08: The scaled vocabulary test score in 8th grade\nvocab_09: The scaled vocabulary test score in 9th grade\nvocab_10: The scaled vocabulary test score in 10th grade\nvocab_11: The scaled vocabulary test score in 11th grade\nfemale: Dummy coded sex variable (0 = Not female, 1 = Female)\n\n\nPreview\n\n# Import data\nvocab = readr::read_csv(file = \"https://raw.githubusercontent.com/zief0002/bespectacled-antelope/main/data/vocabulary.csv\")\n\n# View data\nvocab\n\n# A tibble: 64 × 6\n      id vocab_08 vocab_09 vocab_10 vocab_11 female\n   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n 1     1     1.75     2.6      3.76     3.68      1\n 2     2     0.9      2.47     2.44     3.43      0\n 3     3     0.8      0.93     0.4      2.27      0\n 4     4     2.42     4.15     4.56     4.21      1\n 5     5    -1.31    -1.31    -0.66    -2.22      0\n 6     6    -1.56     1.67     0.18     2.33      0\n 7     7     1.09     1.5      0.52     2.33      0\n 8     8    -1.92     1.03     0.5      3.04      0\n 9     9    -1.61     0.29     0.73     3.24      0\n10    10     2.47     3.64     2.87     5.38      1\n# ℹ 54 more rows\n\n\n\n\n\n\n\n\n\n\nReferences\n\nBock, R. D. (1975). Multivariate statistical methods in behavioral research. New York: McGraw-Hill."
  },
  {
    "objectID": "codebooks/wine.html",
    "href": "codebooks/wine.html",
    "title": "wine.csv",
    "section": "",
    "text": "The data in wine.csv includes data on 200 different wines. These data are a subset of a larger database (\\(n = 6,613\\)) from wine.com, one of the biggest e-commerce wine retailers in the U.S. It allows customers to buy wine according to any price range, grape variety, country of origin, etc. The data were made available at http://insightmine.com/. The attributes include:\n\nwine: Wine name\nvintage: Year the wine was produced (centered so that 0 = 2008, 1 = 2009, etc.)\nregion: Region of the world where the wine was produced. These data include seven regions (Australia, California, France, Italy, New Zealand, South Africa, South America).\nvarietal: Grape varietal These data include nine varietals (Cabernet Sauvignon, Chardonnay, Merlot, Pinot Noir, Sauvignon Blanc, Syrah/Shiraz, Zinfandel, Other Red, Other Whites).\nrating: Wine rating on a 100-pt. scale (these are from sources such as Wine Spectator, the Wine Advocate, and the Wine Enthusiast)\nprice: Price in U.S. dollars\n\n\nPreview\n\n# Import data\nwine = readr::read_csv(file = \"https://raw.githubusercontent.com/zief0002/bespectacled-antelope/main/data/wine.csv\")\n\n# View data\nwine\n\n# A tibble: 200 × 6\n   wine                                     vintage region varietal rating price\n   &lt;chr&gt;                                      &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1 Rust en Vrede Cabernet Sauvignon               4 South… Caberne…     91  30.0\n 2 Bonterra Organically Grown Merlot              4 Calif… Merlot       90  15.0\n 3 Allegrini Palazzo della Torre                  2 Italy  Other R…     90  20.0\n 4 Marcarini Barolo Brunate                       2 Italy  Other R…     94  56.0\n 5 Chateau Beausejour Duffau                      3 France Other R…     96  98.0\n 6 Clos du Marquis                                2 France Other R…     96  60.0\n 7 Altocedro Ano Cero Malbec                      4 South… Other R…     92  19.0\n 8 Stag's Leap Wine Cellars Artemis Cabern…       4 Calif… Caberne…     91  33.0\n 9 Duckhorn Three Palms Merlot                    3 Calif… Merlot       95  89  \n10 Migration Russian River Pinot Noir (375…       4 Calif… Pinot N…     93  20.0\n# ℹ 190 more rows"
  },
  {
    "objectID": "codebooks/woods.html",
    "href": "codebooks/woods.html",
    "title": "woods.csv",
    "section": "",
    "text": "The data in woods.csv were collected from several sources to assesses the effect of political corruption on state environmental policy (Woods, 2008)."
  },
  {
    "objectID": "codebooks/woods.html#attributes",
    "href": "codebooks/woods.html#attributes",
    "title": "woods.csv",
    "section": "Attributes",
    "text": "Attributes\n\nstate: Two-letter state postal code\nenv_prog_str: Environmental program strength is measured via Hall and Kerr’s (1991) Green Policy Index, a composite score that represents 67 state policy initiatives in a variety of environmental arenas, including air, water, and hazardous waste. The index includes indicators such as the sanctions available to the appropriate agencies in each state, the size of the state’s pollution monitoring program, and the size of the state’s program budget, as well as a variety of specific policy indicators.\ncorrupt: Aggregate number of convictions (per 100 officials) during the Reagan Administration (between 1981 and 1987). Aggregating across several years helps eliminate spikes that occur because a single investigation may result in multiple convictions. The average state had 1.42 convictions per 100 officials over that period.\nwealth: Gross state product per capita, in thousands of dollars. This is an indicator of a state’s financial resources.\ntoxic_waste: Natural logarithm of the total tons of toxic waste emitted in air, water based on the U.S. EPA’s Toxic Release Inventory. Higher values indicate a more severe problem.\ndem_party_control: Represents the average amount of Democratic Party control of state political institutions in the 1980s measured as a proportion.\ninterparty_comp: Holbrook and Van Dunk’s (1993) district-level measure is included as an indicator of interparty competition.\npublic_env: Indication of public attitudes toward environmental protection in the state. It is calculated from the 1988–1992 NES Senate Election Study, which asked the following question: “Should federal spending on the environment be increased, decreased, or stay the same?” Individual responses are coded 1 (decrease), 2 (same), and 3 (increase) and then averaged.\nmanuf_groups: The number of manufacturing groups registered to lobby in the state in 1990 standardized by gross state product to account for the fact that the density of organized interests is greater in states with larger economies.\nenv_groups: The number of environmental groups registered to lobby in the state in 1990 standardized by gross state product to account for the fact that the density of organized interests is greater in states with larger economies."
  },
  {
    "objectID": "codebooks/woods.html#preview",
    "href": "codebooks/woods.html#preview",
    "title": "woods.csv",
    "section": "Preview",
    "text": "Preview\n\n# Import data\nwoods = readr::read_csv(file = \"https://raw.githubusercontent.com/zief0002/bespectacled-antelope/main/data/woods.csv\")\n\n# View data\nwoods |&gt;\n  print(width = Inf)\n\n# A tibble: 50 × 10\n   state env_prog_str corrupt wealth toxic_waste dem_party_control\n   &lt;chr&gt;        &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;       &lt;dbl&gt;             &lt;dbl&gt;\n 1 AK               8   0.586   44.3        1.31             0.625\n 2 AL               6   0.630   18.9        4.70             0.800\n 3 AR               5   0.132   18.4        3.71             0.900\n 4 AZ              13   0.318   19.3        4.19             0.300\n 5 CA              34   0.498   25.5        4.69             0.600\n 6 CO              17   0.212   23.8        2.75             0.5  \n 7 CT              29   0.213   30.2        3.63             0.900\n 8 DE              15   0.301   34.3        2.15             0.300\n 9 FL              24   0.604   19.9        4.12             0.800\n10 GA              12   0.546   22.7        4.46             1    \n   interparty_comp public_env manuf_groups env_groups\n             &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt;\n 1           53.5        2.42     0.000654  0.000346 \n 2           27.3        2.37     0.000538  0.000064 \n 3            9.26       2.42     0.000636  0.0000455\n 4           33.9        2.52     0.000783  0.000148 \n 5           47.3        2.7      0.000259  0.000014 \n 6           40.2        2.5      0.000521  0.000242 \n 7           52.8        2.59     0.000586  0.000192 \n 8           39.7        2.60     0.00105   0.000127 \n 9           31.1        2.61     0.000875  0.000246 \n10           16.2        2.49     0.000651  0.000143 \n# ℹ 40 more rows"
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Data",
    "section": "",
    "text": "Below are the links to the data sets and data codebooks used in the notes, scripts, and assignments."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "EPsy 8251",
    "section": "",
    "text": "EPsy 8251: Methods in Data Analysis for Educational Research I is the first course in an entry-level, doctoral sequence for students in education. The two semester sequence provides in-depth coverage of widely used statistical methods and models and prepares students for advanced statistical coursework. EPsy 8251 provides rigorous coverage of estimation and hypothesis testing with a particular focus on the General Linear Model.\n\n\n\nIn this class, we will work together to develop a learning community that is inclusive and respectful, and where every student is supported in the learning process. As a class full of diverse individuals (reflected by differences in race, culture, age, religion, gender identity, sexual orientation, socioeconomic background, abilities, professional goals, and other social identities and life experiences) I expect that different students may need different things to support and promote their learning. The TAs and I will do everything we can to help with this, but as we only know what we know, we need you to communicate with us if things are not working for you or you need something we are not providing. I hope you all feel comfortable in helping to promote an inclusive classroom through respecting one another’s individual differences, speaking up, and challenging oppressive/problematic ideas. Finally, I look forward to learning from each of you and the experiences you bring to the class."
  },
  {
    "objectID": "index.html#classroom",
    "href": "index.html#classroom",
    "title": "",
    "section": "Classroom",
    "text": "Classroom\n\nMonday/Wednesday (2:30pm–3:45pm): Scott Hall 4"
  },
  {
    "objectID": "index.html#textbooks",
    "href": "index.html#textbooks",
    "title": "",
    "section": "Textbooks",
    "text": "Textbooks\nThe course textbook is available via the University of Minnesota library.\n\nRequired: Lewis-Beck, C., & Lewis-Beck, M. (2016). Applied regression: An introduction. Thousand Oaks, CA: Sage. doi: 10.4135/9781483396774\n\nThere are two other free, online books that the lectures for the class are drawn from:\n\nComputational Toolkit for Educational Scientists\nStatistical Modeling and Computation for Educational Scientists"
  },
  {
    "objectID": "index.html#other-resources",
    "href": "index.html#other-resources",
    "title": "EPsy 8251",
    "section": "Other Resources",
    "text": "Other Resources\n\nStatistical Modeling and Computation for Educational Scientists\n\nLearn about simple and multiple regression\nLearn about Ordinary Least squares (OLS)\nLearn about correlation/standardized regression\nLearn about regrression assumptions\nLearn about dummy coding for dichotomous/polychotomous predictors\nLearn about interaction effects\n\nComputational Toolkit for Educational Scientists\n\nLearn basics of R\nLearn {dplyr} for wrangling data\nLearn {ggplot2} for visualizing data"
  },
  {
    "objectID": "readings/01-welcome-to-8252.html",
    "href": "readings/01-welcome-to-8252.html",
    "title": "📖 Welcome to EPsy 8264",
    "section": "",
    "text": "Here are some things to do before the first class:\n\nDownload and read through the course syllabus\nFamiliarize yourself with the course website\nUpdate R to the most recent version (v. 4.2.2; Innocent and Trusting)\nUpdate RStudio Desktop to the most recent version (v. 2022.12.0+353)\nUpdate all of your R packages (In the Packages tab in RStudio click Update)\nUpdate the {educate} package (to at least v.0.3.0.1). This is a github package, so see the instructions in Installing Packages from GitHub\n\nWe will also start Day 1 with a short review of the regression content from EPsy 8251.\n\nRemind yourself of the EPsy 8251 content\n\nComputational Toolkit for Educational Scientists\nStatistical Modeling and Computation for Educational Scientists"
  },
  {
    "objectID": "readings/02-project-organization.html",
    "href": "readings/02-project-organization.html",
    "title": "📖 Project Management",
    "section": "",
    "text": "Before next class, look at your computer files and your organization of those files. Here are some things to reflect on:\n\nAre your files organized into folders/directories? Or are they all in your Downloads folder?\n\nHow did you organize all the data files, notes, etc. from EPsy 8251?\n\nIf I asked you to find a specific file, could you locate it without using “Search”?\nCan you tell what is in a particular file by just looking at its name?\nDo your file names contain spaces? What about characters that aren’t letters, numbers, dashes, or underscores?\nAre your file names consistent (all lower case letters, or all title case)? Or are they all different?"
  },
  {
    "objectID": "readings/03-introduction-to-quarto-day-02.html",
    "href": "readings/03-introduction-to-quarto-day-02.html",
    "title": "Introduction to Quarto (Day 2)",
    "section": "",
    "text": "For the next class you will need a citation manager. If you don’t have a citation manager, I strongly recommend Zotero.\n\nInstall Zotero.\n\nIf you use a different citation manager, you will need to figure out how to export a .BIB file from your citation manager."
  },
  {
    "objectID": "readings/03-introduction-to-quarto-day-02.html#install-a-citation-manager",
    "href": "readings/03-introduction-to-quarto-day-02.html#install-a-citation-manager",
    "title": "Introduction to Quarto (Day 2)",
    "section": "",
    "text": "For the next class you will need a citation manager. If you don’t have a citation manager, I strongly recommend Zotero.\n\nInstall Zotero.\n\nIf you use a different citation manager, you will need to figure out how to export a .BIB file from your citation manager."
  },
  {
    "objectID": "readings/03-introduction-to-quarto-day-02.html#download-and-import-two-papers",
    "href": "readings/03-introduction-to-quarto-day-02.html#download-and-import-two-papers",
    "title": "Introduction to Quarto (Day 2)",
    "section": "Download and Import Two Papers",
    "text": "Download and Import Two Papers\nDownload the following two papers from the UMN library:\n\nCarmichael, L. (1954). Laziness and the scholarly life. The Scientific Monthly, 78(4), 208–213.\nRoss, C. T., Winterhalder, B., & McElreath, R. (2020). Racial disparities in police use of deadly force against unarmed individuals persist after appropriately benchmarking shooting data on violent crime rates. Social Psychological and Personality Science, 194855062091607. https://doi.org/10.1177/1948550620916071\n\nImport both of these into Zotero by dragging the PDFs into Zotero. After they are imported, check that all the metadata is correct. (Page numbers often need to be updated in Zotero.)"
  },
  {
    "objectID": "readings/03-introduction-to-quarto.html",
    "href": "readings/03-introduction-to-quarto.html",
    "title": "Introduction to Quarto",
    "section": "",
    "text": "Head over to https://quarto.org/ and click on the big, blue “Get Started” button. Then follow the instructions to install Quarto.\nOpen RStudio. (Note if RStudio was open when you installed Quarto, quit and re-open it.)\n\nAt this point in RStudio, if you go to File &gt; New File... you should see options to choose Quarto Document and Quarto Presentation."
  },
  {
    "objectID": "readings/03-introduction-to-quarto.html#install-quarto",
    "href": "readings/03-introduction-to-quarto.html#install-quarto",
    "title": "Introduction to Quarto",
    "section": "",
    "text": "Head over to https://quarto.org/ and click on the big, blue “Get Started” button. Then follow the instructions to install Quarto.\nOpen RStudio. (Note if RStudio was open when you installed Quarto, quit and re-open it.)\n\nAt this point in RStudio, if you go to File &gt; New File... you should see options to choose Quarto Document and Quarto Presentation."
  },
  {
    "objectID": "readings/03-introduction-to-quarto.html#help-and-tutorials",
    "href": "readings/03-introduction-to-quarto.html#help-and-tutorials",
    "title": "Introduction to Quarto",
    "section": "Help and Tutorials",
    "text": "Help and Tutorials\nThere are several guides and tutorials available on the Quarto website. These include:\n\nCreating basic content (e.g., figures, tables, diagrams, footnotes, citations)\nIncluding computation/syntax\nGenerating output in different formats (e.g., HTML, PDF, MS Word)\nCreating slides and presentations (These notes are a Quarto presentation!)\nBuilding websites\nBuilding books\nCreating interactive dashboards"
  },
  {
    "objectID": "readings/03-introduction-to-quarto.html#create-your-first-quarto-document",
    "href": "readings/03-introduction-to-quarto.html#create-your-first-quarto-document",
    "title": "Introduction to Quarto",
    "section": "Create Your First Quarto Document",
    "text": "Create Your First Quarto Document\nClick the Tutorial: Hello Quarto link on the left-side of the “Getting Started” page. Make sure that the RStudio tool is selected at the top of the tutorial.\n\nWork through this tutorial. This will introduce you to some of the basic concepts of creating and rendering a Quarto document."
  },
  {
    "objectID": "readings/03-introduction-to-quarto.html#learning-how-to-integrate-r-code",
    "href": "readings/03-introduction-to-quarto.html#learning-how-to-integrate-r-code",
    "title": "Introduction to Quarto",
    "section": "Learning How to Integrate R Code",
    "text": "Learning How to Integrate R Code\nAlso work through the Tutorial: Computations. Again, before starting, ensure that the RStudio tool is selected at the top of the tutorial."
  },
  {
    "objectID": "readings/03-introduction-to-quarto.html#learn-more",
    "href": "readings/03-introduction-to-quarto.html#learn-more",
    "title": "Introduction to Quarto",
    "section": "Learn More",
    "text": "Learn More\nCheck out the Welcome to Quarto Workshop. This is a 2 1/2 hour workshop introducing people to Quarto.\n\nWelcome to Quarto Workshop! (YouTube Video)\n\nIf you are familiar with RMarkdown and want to learn how to switch over to Quarto, here is a nice 30 minute talk by Mine Çetinkaya-Rundel that helps you do that.\n\n2022 Toronto Workshop on Reproducibility - Mine Çetinkaya-Rundel (YouTube Video)"
  },
  {
    "objectID": "readings/04-probability-distributions.html",
    "href": "readings/04-probability-distributions.html",
    "title": "Introduction to Probability Distributions",
    "section": "",
    "text": "If you need to refresh your knowledge about probability distributions, I recommend reading Section 3.1.1: (Probability Basics) in Fox (2009). You could also go through the Kahn Academy: Random Variables and Probability Distributions tutorial.\nHere is some ideas you will need to be familiar with from those readings/tutorials:\nBelow, I introduce some ideas about probability distributions (especially continuous probability distributions) that will be useful for understanding likelihood."
  },
  {
    "objectID": "readings/04-probability-distributions.html#probability-density",
    "href": "readings/04-probability-distributions.html#probability-density",
    "title": "Introduction to Probability Distributions",
    "section": "Probability Density",
    "text": "Probability Density\nIn a continuous distribution we also need to account be able to talk about the fact that some outcomes are more likely than other outcomes. For example, in our standard normal distribution outcomes near zero are more probable than outcomes near 1, which are more probable than outcomes near 2, etc. Since we can’t use probability to do this (remember the probability of each outcome is the same, namely 0), we use something called probability density. This is akin to a relative probability, so outcomes with a higher probability density are more likely than outcomes with a lower probability density.\nThe mapping of all the outcomes to their probability densities is called a probability density function (PDF). Thus the equation or “bell-shaped” curve describing the standard normal distribution in Figure 1 is technically a PDF Here are some laws governing PDFs:\n\nProbability densities are always positive.\nThe probability of an outcome x between a and b equals the integral (area under the curve) between a and b of the probability density function. That is:\n\n\\[\np(a \\leq x \\leq b) = \\int_a^b p(x) dx\n\\]\n\nThe area under the curve from negative infinity to positive infinity is 1. That is:\n\n\\[\np(-\\infty \\leq x \\leq +\\infty) = \\int_{-\\infty}^{+\\infty} p(x) = 1\n\\]\nNext we will look at the PDF for a normal distribution."
  },
  {
    "objectID": "readings/04-probability-distributions.html#other-useful-r-functions-for-working-with-normal-probability-distributions",
    "href": "readings/04-probability-distributions.html#other-useful-r-functions-for-working-with-normal-probability-distributions",
    "title": "Introduction to Probability Distributions",
    "section": "Other Useful R Functions for Working with Normal Probability Distributions",
    "text": "Other Useful R Functions for Working with Normal Probability Distributions\nWe use dnorm() when we want to compute the probability density associated with a particular x-value in a given normal distribution. There are three other functions that are quite useful for working with the normal probability distribution:\n\npnorm() : To compute the probability (area under the PDF)\nqnorm() : To compute the \\(x\\) value given a particular probability\nrnorm() : To draw a random observation from the distribution\n\nEach of these function also requires the arguments mean= and sd=. Below we will examine how to use each of these additional functions."
  },
  {
    "objectID": "readings/04-probability-distributions.html#pnorm-computing-cumulative-probability-density",
    "href": "readings/04-probability-distributions.html#pnorm-computing-cumulative-probability-density",
    "title": "Introduction to Probability Distributions",
    "section": "pnorm(): Computing Cumulative Probability Density",
    "text": "pnorm(): Computing Cumulative Probability Density\nThe function pnorm() computes the area under the PDF curve from \\(-\\infty\\) to some x-value. (Sometimes this is referred to as the cumulative probability density of x.) It is important to note that the PDF is defined such that the entire area under the curve is equal to 1. Because of this, we can also think about using area under the curve as an analog to probability in a continuous distribution.\nFor example, we might ask about the probability of observing an x-value that is less than or equal to 65 given it is from a \\(\\mathcal{N}(50,10)\\) distribution. Symbolically, we want to find:\n\\[\nP\\bigg(x \\leq 65 \\mid \\mathcal{N}(50,10)\\bigg)\n\\]\nThis is akin to finding the proportion of the area under the \\(\\mathcal{N}(50,10)\\) PDF that is to the left of 65. The figure below shows a graphical depiction of the cumulative probability density for \\(x=65\\).\n\n\nCode\n# Create dataset\nfig_03 = data.frame(\n  X = seq(from = 10, to = 90, by = 0.01)\n  ) %&gt;% \n  mutate(\n    Y = dnorm(x = X, mean = 50, sd = 10)\n    )\n\n# Filter out X&lt;=65\nshaded = fig_03 %&gt;%\n  filter(X &lt;= 65)\n\n# Create plot\nggplot(data = fig_03, aes(x = X, y = Y)) +\n  geom_ribbon(data = shaded, ymin = -10, aes(ymax = Y), color = \"#bbbbbb\", alpha = 0.4) +\n  geom_line() +\n  xlab(\"X\") +\n  ylab(\"Probability density\") +\n  theme_light()\n\n\n\n\n\nFigure 3: Plot of the probability density function (PDF) for a \\(\\mathcal{N}(50,10)\\) distribution. The area that is shaded grey (relative to the total area under the PDF) represents the cumulative probability density for \\(x=65\\).\n\n\n\n\nWe can compute the cumulative probability density using the pnorm() function. The “p” stand for “probability”.\n\n# Find P(x&lt;=65 | N(50,10) )\npnorm(q = 65, mean = 50, sd = 10)\n\n[1] 0.9331928\n\n\nWe can interpret this as:\n\nThe probability of observing an x-value that is less than or equal to 65 (if it is drawn from a normal distribution with a mean of 50 and standard deviation of 10) is 0.933.\n\nIn mathematics, the area under a curve is called an integral. The grey-shaded area in the previous figure can also be expressed as an integral of the probability density function:\n\\[\n\\int_{-\\infty}^{65} p(x) dx\n\\]\nwhere \\(p(x)\\) is the PDF for the normal distribution.\nThe most common application for finding the cumulative density is to compute a p-value. The p-value is just the area under the distribution (curve) that is AT LEAST as extreme as some observed value. For example, assume we computed a test statistic of \\(z=2.5\\), and were evaluating whether this was different from 0 (two-tailed test). Graphically, we want to determine the proportion of the area under the PDF that is shaded grey in the figure below.\n\n\nCode\n# Create data\nfig_04 = data.frame(\n  X = seq(from = -4, to = 4, by = 0.01)\n  ) %&gt;% \n  mutate(\n    Y = dnorm(x = X, mean = 0, sd = 1)\n    )\n\n# Filter data for shading\nshade_01 = fig_04 %&gt;%\n  filter(X &gt;= 2.5)\n\nshade_02 = fig_04 %&gt;%\n  filter(X &lt;= -2.5)\n\n# Create plot\nggplot(data = fig_04, aes(x = X, y = Y)) +\n  geom_ribbon(data = shade_01, ymin = -10, aes(ymax = Y), color = \"#bbbbbb\", alpha = 0.4) +\n  geom_ribbon(data = shade_02, ymin = -10, aes(ymax = Y), color = \"#bbbbbb\", alpha = 0.4) +\n  geom_line() +\n  xlab(\"z\") +\n  ylab(\"Probability density\") +\n  theme_light()\n\n\n\n\n\nFigure 4: Plot of the probability density function (PDF) for the standard normal distribution (\\(M=0\\), \\(SD=1\\)). The cumulative density representing the p-value for a two-tailed test evaluating whether \\(\\mu=0\\) using an observed mean of 2.5 is also displayed.\n\n\n\n\nIf the distribution of the test statistic is normally distributed, we can use pnorm() to compute the p-value. If we assume the test statistic, z, has been scaled to use standardized units, the standard deviation we use in pnorm() will be sd=1. The mean is based on the value being tested in the null hypothesis. In most null hypotheses, we are testing a difference from 0 (e.g., \\(H_0: \\mu=0\\), \\(H_0: \\beta=0\\)), so we would use mean=0 in the pnorm() function.\nRemember, pnorm() computes the proportion of the area under the curve TO THE LEFT of a particular value. Here we will compute the area to the left of \\(-2.5\\) and then double it to produce the actual p-value. (We can double it because the normal distribution is symmetric so the area to the left of \\(-2.5\\) is the same as the area to the right of \\(+2.5\\).)\n\n# Compute the p-value based on z=2.5\n2 * pnorm(q = -2.5, mean = 0, sd = 1)\n\n[1] 0.01241933\n\n\nWe interpret this p-value as:\n\nThe probability of observing a statistic at least as extreme as 2.5, assuming the null hypothesis is true, is 0.012. This is evidence against the null hypothesis since the data are inconsistent with the assumed hypothesis.\n\n\n\nqnorm(): Computing Quantiles\nThe qnorm() function is essentially the inverse of the pnorm() function. The pnorm() function computes the cumulative probability GIVEN a particular quantile (x-value). The qnorm() function computes the quantile GIVEN a cumulative probability. For example, in the \\(\\mathcal{N}(50, 10)\\) distribution, half of the area under the PDF is below the x-value (quantile) of 50.\nTo use the qnorm() function to give the x-value (quantile) that defines the lower 0.5 of the area under the \\(\\mathcal{N}(50, 10)\\) PDF, the syntax would be:\n\n# Find the quantile that has a cumulative density of 0.5 in the N(50, 10) distribution\nqnorm(p = 0.5, mean = 50, sd = 10)\n\n[1] 50\n\n\n\n\n\nrnorm(): Generating Random Observations\nThe rnorm() function can be used to generate random observations drawn from a specified normal distribution. Aside from the mean= and sd= arguments, we also need to specify the number of observations to generate by including the argument n=. For example, to generate 15 observations drawn from a \\(\\mathcal{N}(50,10)\\) distribution we would use the following syntax:\n\n# Generate 15 observations from N(50,10)\nset.seed(100)\nrnorm(n = 15, mean = 50, sd = 10)\n\n [1] 44.97808 51.31531 49.21083 58.86785 51.16971 53.18630 44.18209 57.14533\n [9] 41.74741 46.40138 50.89886 50.96274 47.98366 57.39840 51.23380\n\n\nThe set.seed() function sets the state of the random number generator used in R so that the results are reproducible. If you don’t use set.seed() you will get a different set of observations each time you run rnorm(). Here we set the starting seed to 100, but you can set this to any integer you want."
  },
  {
    "objectID": "readings/04-probability-distributions.html#computing-f-from-the-anova-partitioning",
    "href": "readings/04-probability-distributions.html#computing-f-from-the-anova-partitioning",
    "title": "Introduction to Probability Distributions",
    "section": "Computing F from the ANOVA Partitioning",
    "text": "Computing F from the ANOVA Partitioning\nWe can also compute the model-level F-statistic directly using the partitioning of variation from the ANOVA table.\n\n# Partition the variation\nanova(lm.1)\n\n\n\n  \n\n\n\nThe F-statistic is a ratio of the mean square for the model and the mean square for the error. To compute a mean square we use the general formula:\n\\[\n\\mathrm{MS} = \\frac{\\mathrm{SS}}{\\mathrm{df}}\n\\]\nThe model includes both the education and seniority predictor, so we combine the SS and df. The MS model is:\n\\[\n\\begin{split}\n\\mathrm{MS}_{\\mathrm{Model}} &= \\frac{\\mathrm{SS}_{\\mathrm{Model}}}{\\mathrm{df}_{\\mathrm{Model}}} \\\\[1ex]\n&= \\frac{4147.3 + 722.9}{1 + 1} \\\\[1ex]\n&= \\frac{4870.2}{2} \\\\[1ex]\n&= 2435.1\n\\end{split}\n\\]\nThe MS error is:\n\\[\n\\begin{split}\n\\mathrm{MS}_{\\mathrm{Error}} &= \\frac{\\mathrm{SS}_{\\mathrm{Error}}}{\\mathrm{df}_{\\mathrm{Error}}} \\\\[1ex]\n&= \\frac{1695.3 }{29} \\\\[1ex]\n&= 58.5\n\\end{split}\n\\]\nThen, we compute the F-statistic by computing the ratio of these two mean squares.\n\\[\n\\begin{split}\nF &= \\frac{\\mathrm{MS}_{\\mathrm{Model}}}{\\mathrm{MS}_{\\mathrm{Error}}} \\\\[1ex]\n&= \\frac{2435.1}{58.5} \\\\[1ex]\n&= 41.6\n\\end{split}\n\\]\nSince a mean square represents the average amount of variation (per degree of freedom), we can see that F is a ratio between the average amount of variation explained by the model and the average amount of variation unexplained by the model. In our example, this ratio is 41.6; on average the model explains 41.6 times the variation that is unexplained.\nNote that this is an identical computation (although reframed) as the initial computation for F. We can use mathematics to show this equivalence:\n\\[\n\\begin{split}\nF &= \\frac{R^2}{1-R^2} \\times \\frac{\\mathrm{df}_{\\mathrm{Error}}}{\\mathrm{df}_{\\mathrm{Model}}} \\\\[1ex]\n&= \\frac{\\frac{\\mathrm{SS}_{\\mathrm{Model}}}{\\mathrm{SS}_{\\mathrm{Total}}}}{\\frac{\\mathrm{SS}_{\\mathrm{Error}}}{\\mathrm{SS}_{\\mathrm{Total}}}} \\times \\frac{\\mathrm{df}_{\\mathrm{Error}}}{\\mathrm{df}_{\\mathrm{Model}}} \\\\[1ex]\n&= \\frac{\\mathrm{SS}_{\\mathrm{Model}}}{\\mathrm{SS}_{\\mathrm{Error}}} \\times \\frac{\\mathrm{df}_{\\mathrm{Error}}}{\\mathrm{df}_{\\mathrm{Model}}} \\\\[1ex]\n&= \\frac{\\mathrm{SS}_{\\mathrm{Model}}}{\\mathrm{df}_{\\mathrm{Model}}} \\times \\frac{\\mathrm{df}_{\\mathrm{Error}}}{\\mathrm{SS}_{\\mathrm{Error}}} \\\\[1ex]\n&= \\mathrm{MS}_{\\mathrm{Model}} \\times \\frac{1}{\\mathrm{MS}_{\\mathrm{Error}}}\\\\[1ex]\n&= \\frac{\\mathrm{MS}_{\\mathrm{Model}}}{\\mathrm{MS}_{\\mathrm{Error}}}\n\\end{split}\n\\]"
  },
  {
    "objectID": "readings/04-probability-distributions.html#testing-the-model-level-null-hypothesis",
    "href": "readings/04-probability-distributions.html#testing-the-model-level-null-hypothesis",
    "title": "Introduction to Probability Distributions",
    "section": "Testing the Model-Level Null Hypothesis",
    "text": "Testing the Model-Level Null Hypothesis\nWe evaluate our test statistic (F in this case) in the appropriate test distribution, in this case an F-distribution with 2 and 29 degrees of freedom. The figure below, shows the \\(F(2,29)\\)-distribution as a solid, black line. The p-value is the area under the curve that is at least as extreme as the observed F-value of 41.7.\n\n\nCode\n# Create data\nfig_11 = data.frame(\n  X = seq(from = 0, to = 50, by = 0.01)\n  ) %&gt;%\n  mutate(\n    Y = df(x = X, df1 = 2, df2 = 29)\n    )\n\n# Filter shaded area\nshade = fig_11 %&gt;%\n  filter(X &gt;= 41.7)\n\n# Create plot\nggplot(data = fig_11, aes(x = X, y = Y)) +\n  geom_line() +\n  theme_bw() +\n  geom_ribbon(data = shade, ymin = -10, aes(ymax = Y), color = \"#bbbbbb\", alpha = 0.4)\n\n\n\n\n\nFigure 11: Plot of the probability density function (PDF) for the \\(F(2,~29)\\)-distribution. The cumulative density representing the p-value for a test evaluating whether \\(\\rho^2=0\\) using an observed F-statistic of 41.7 is also displayed.\n\n\n\n\nThe computation using the cumulative density function, pf(), to obtain the p-value is:\n\n# p-value for F(2,29)=41.7\n1 - pf(41.7, df1 = 2, df2 = 29)\n\n[1] 0.000000002942114\n\n\nBecause we want the upper-tail, rather than taking the difference from 1, we can also use the lower.tail=FALSE argument in pf().\n\n# p-value for F(2,29)=41.7\npf(41.7, df1 = 2, df2 = 29, lower.tail = FALSE)\n\n[1] 0.000000002942114"
  },
  {
    "objectID": "readings/04-probability-distributions.html#mean-squares-are-variance-estimates",
    "href": "readings/04-probability-distributions.html#mean-squares-are-variance-estimates",
    "title": "Introduction to Probability Distributions",
    "section": "Mean Squares are Variance Estimates",
    "text": "Mean Squares are Variance Estimates\nMean squares are also estimates of the variance. Consider the computational formula for the sample variance,\n\\[\n\\hat{\\sigma}^2 = \\frac{\\sum(Y - \\bar{Y})^2}{n-1}\n\\]\nThis is the total sum of squares divided by the total df. The variance of the outcome variable is interpreted as the average amount of variation in the outcome variable (in the squared metric). Thus, it is also referred to as the mean square total.\nWhen we compute an F-statistic, we are finding the ratio of two different variance estimates—one based on the model (explained variance) and one based on the error (unexplained variance). Under the null hypothesis that \\(\\rho^2 = 0\\), we are assuming that all the variance is unexplained. In that case, our F-statistic would be close to zero. When the model explains a significant amount of variation, the numerator gets larger relative to the denominator and the F-value is larger.\nThe mean squared error (from the anova() output) plays a special role in regression analysis. It is the variance estimate for the conditional distributions of the residuals in our visual depiction of the distributional assumptions of the residuals underlying linear regression.\n\n\n\n\n\nFigure 12: Visual Depiction of the Distributional Assumptions of the Residuals Underlying Linear Regression\n\n\n\n\nRecall that we made implicit assumptions about the conditional distributions of the residuals, namely that they were identically and normally distributed with a mean of zero and some variance. Based on the estimate of the mean squared error, the variance of each of these distributions is 58.5.\nWhile the variance is a mathematical convenience, the standard deviation is often a better descriptor of the variation in a distribution since it is measured in the original metric. The standard deviation fro the residuals (error) is 7.6. Because the residuals are statistics (summaries computed from sample data), their standard deviation is referred to as a “standard error”.\n\nThe residual standard error (RSE) is sometimes referred to as the Root Mean Squared Error (RMSE).\n\n\n# Compute RMSE\nsqrt(58.5)\n\n[1] 7.648529\n\n\nWhy is this value important? It gives the expected variation in the conditional residual distributions, which is a measure of the average amount of error. For example, since all of the conditional distributions of the residuals are assumed to be normally distributed, we would expect that 95% of the residuals would fall between \\(\\pm2\\) standard errors from 0; or, in this case, between \\(-15.3\\) and \\(+15.3\\). Observations with residuals that are more extreme may be regression outliers.\nMore importantly, it is a value that we need to estimate in order to specify the model."
  },
  {
    "objectID": "readings/04-probability-distributions.html#confidencecompatibility-intervals-for-the-coefficients",
    "href": "readings/04-probability-distributions.html#confidencecompatibility-intervals-for-the-coefficients",
    "title": "Introduction to Probability Distributions",
    "section": "Confidence/Compatibility Intervals for the Coefficients",
    "text": "Confidence/Compatibility Intervals for the Coefficients\nThe confidence interval for the kth regression coefficient is computed as:\n\\[\n\\mathrm{CI} = \\hat\\beta_k \\pm t^{*}(\\mathrm{SE}_{\\hat\\beta_k})\n\\]\nwhere \\(t^*\\) is the quantile of the t-distribution that defines the confidence level for the interval. (This t-distribution, again, has degrees-of-freedom equal to the error df in the model.) The confidence level is related to the alpha level (type I error rate) used in inference. Namely,\n\\[\n\\mathrm{Confidence~Level} = 1 - \\alpha\n\\]\nSo, if you use \\(\\alpha=.05\\), then the confidence level would be \\(.95\\), and we would call this a 95% confidence interval. The alpha value also helps determine the quantile we use in the CI formula,\n\\[\nt^* = (1-\\frac{\\alpha}{2}) ~ \\mathrm{quantile}\n\\] For the example using \\(\\alpha=.05\\), a 95% confidence interval, the \\(t^*\\) value would be associated with the quantile of 0.975. We would denote this as:\n\\[\nt^{*}_{.975}\n\\]\nSay we wanted to find the 95% confidence interval for the education coefficient. We know that the estimated coefficient for education is 2.25, and the standard error for this estimate is 0.335. We also know that based on the model fitted, the residual df is 29. We need to find the 0.975th quantile in the t-distribution with 29 df.\n\n# Find 0.975th quantile\nqt(p = 0.975, df = 29)\n\n[1] 2.04523\n\n\nNow we can use all of this information to compute the confidence interval:\n\\[\n\\begin{split}\n95\\%~CI  &= 2.25 \\pm 2.04523(0.335) \\\\[1ex]\n&= \\big[1.56,~2.94\\big]\n\\end{split}\n\\]"
  },
  {
    "objectID": "readings/04-probability-distributions.html#footnotes",
    "href": "readings/04-probability-distributions.html#footnotes",
    "title": "Introduction to Probability Distributions",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nRemember, the mean and standard deviations in the population are called “parameters”.↩︎\nWhether this is actually t-distributed depends on whether the model assumptions are met.↩︎"
  },
  {
    "objectID": "readings/05-polynomial-effects.html",
    "href": "readings/05-polynomial-effects.html",
    "title": "📖 Polynomial Effects",
    "section": "",
    "text": "Required\nRefresh your knowledge about parabolas and quadratic functions by going though the Khan Academy Parabolas Intro.\n\nAdditional Resources\nIn addition to the notes and what we cover in class, there are many other resources for learning about polynomial functions. Here are some resources that may be helpful in that endeavor:\n\nVarsity Tutors: Quadratic Function\nMath Centre: Polynomial Function\nOverfitting: A guided tour"
  },
  {
    "objectID": "readings/06-information-criteria-and-model-selection.html",
    "href": "readings/06-information-criteria-and-model-selection.html",
    "title": "📖 Polynomial Effects",
    "section": "",
    "text": "Required\nRead the following:\n\nElliott, L. P., & Brook, B. W. (2007). Revisiting Chamberlin: Multiple working hypotheses for the 21st century. BioScience, 57(7), 608–614. doi: 10.1641/B570708\n\n\nAdditional Resources\nIn addition to the notes and what we cover in class, there are many other resources for learning about information criteria and model selection. Here are some resources that may be helpful in that endeavor:\n\nAnderson, D. R. (2008). Model based inference in the life sciences: A primer on evidence. New York: Springer. [Optional Textbook]\nBurnham, K. P., & Anderson, D. R. (2002). Model selection and multimodel inference: A practical information-theoretic approach. New York: Springer.\nBurnham, K. P., Anderson, D. R., & Huyvaert, K. P. (2010). AIC model selection and multimodel inference in behavioral ecology: Some background, observations, and comparisons. Behavioral Ecology and Sociobiology, 65(1), 23–35."
  },
  {
    "objectID": "readings/07-logarithmic-transformations-predictor.html",
    "href": "readings/07-logarithmic-transformations-predictor.html",
    "title": "📖 Log-Transforming the Predictor",
    "section": "",
    "text": "Required\nRefresh your knowledge about logarithms:\n\nKhan Academy Intro to Logarithms Tutorial\n\n\nAdditional Resources\nIn addition to the notes and what we cover in class, there are many other resources for learning about log-transformations. Here are some resources that may be helpful in that endeavor:\n\nMath with Bad Drawings: ABC Book of e"
  },
  {
    "objectID": "readings/08-logarithmic-transformations-outcome.html",
    "href": "readings/08-logarithmic-transformations-outcome.html",
    "title": "📖 Log-Transforming the Outcome",
    "section": "",
    "text": "Required\nRead the following:\n\nOsborne, Jason (2002). Notes on the use of data transformations. Practical Assessment, Research & Evaluation, 8(6).\n\n\nAdditional Resources\nIn addition to the notes and what we cover in class, there are many other resources for learning about log-transformations. Here are some resources that may be helpful in that endeavor:\n\nUCLA IDRE: Interpreting Log-Transformed Variables in Regression\nInterpret Regression Coefficient Estimates"
  },
  {
    "objectID": "readings/11-logistic-regression.html",
    "href": "readings/11-logistic-regression.html",
    "title": "📖 Logistic Regression Model",
    "section": "",
    "text": "Required\nWatch the following::\n\n3Blue1Brown. Exponential growth and epidemics. YouTube video.\n\nWhen you watch this video, focus on the ideas and not the equations.\n\nAdditional Resources\nIn addition to the notes and what we cover in class, there are many other resources for learning about logistic regression models. Here are some resources that may be helpful in that endeavor:\n\nFox, J. (2009). The Binomial and Bernoulli distributions (Section 3.2.1). In A mathematical primer for social statistics. Sage. [Required Textbook]\nDunteman, G. H., & Ho, M.-H. R. (2005). An introduction to generalized linear models. Sage.\nFox, J. (2015). Applied regression analysis and generalized linear models (3rd ed.). Sage"
  },
  {
    "objectID": "readings/13-introduction-to-mixed-effects-models.html",
    "href": "readings/13-introduction-to-mixed-effects-models.html",
    "title": "📖 Introduction to Mixed-Effects Models",
    "section": "",
    "text": "Required\nRead the following::\n\nGrolemund, G., & Wickham, H. (2017). Relational data. In R for data science: Visualize, model, transform, tidy, and import data. O’Reilly."
  },
  {
    "objectID": "readings/15-lmer-other-random-effects-and-covariates.html",
    "href": "readings/15-lmer-other-random-effects-and-covariates.html",
    "title": "📖 LMER: Other Random-Effects and Covariates",
    "section": "",
    "text": "Required\nRead the following::\n\nUchikoshi, Y. (2005). Narrative development in bilingual kindergarteners: Can Arthur help? Developmental Psychology, 41(3), 464–478. doi: 10.1037/0012-1649.41.3.464\n\n\nAdditional Resources\nIn addition to the notes and what we cover in class, there are many other resources for learning about mixed-effects regression models. Here are some resources that may be helpful in that endeavor:\n\nLong, J. D. (2012). Longitudinal data analysis for the behavioral sciences using R. Thousand Oaks, CA: Sage.\nEstimation of the Log-Likelihood in lmer"
  },
  {
    "objectID": "readings/16-lmer-alt-representations-and-assumptions.html",
    "href": "readings/16-lmer-alt-representations-and-assumptions.html",
    "title": "📖 LMER: Alternative Representations and Assumptions",
    "section": "",
    "text": "Required\nRead the following::\n\nLeyland, A. H., & Groenewegen, P. P. (2020). Reading and writing In Multilevel modelling for public health and health services research: Health in context (pp. 151–169). Springer. doi: 10.1007/978-3-030-34801-4_10\n\n\nAdditional Resources\nIn addition to the notes and what we cover in class, there are many other resources for learning about mixed-effects regression models. Here are some resources that may be helpful in that endeavor:\n\nSection on Residual Analysis in: Loy, A. (2014). HLMdiag: A suite of diagnostics for hierarchical linear models in R. Journal of Statistical Software, 56(5), 1–28."
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Calendar",
    "section": "",
    "text": "The calendar below lists the tentative course topics. The dates listed are subject to change at the instructor’s discretion.\nAs part of the course, there are several articles, papers and technical reports that you will need to read during the semester. Most of the articles themselves are accessible through the University of Minnesota library website (http://www.lib.umn.edu). In order to access the full text of some of the articles, you will need to log in using your University x500 username and password. More detailed information, including references or links to specific readings, are linked under the day’s “Reading”.\n\n\n\n\n\n\n\n\n\nDate\n\n\nReading\n\n\nTopic\n\n\nNotes\n\n\nScript\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\nSept. 07\n\n\n\n\n\nWelcome to EPsy 8251\n\n\n\n\n\n\n\n\n\n\n\nUnit 01: Introduction to Statistical Computing\n\n\n\n\n\n\n\nSept. 12\n\n\n\n\n\nIntroduction to R and RStudio\n\n\n\n\n\n\n\n\n\n\n\n\n\nSept. 14\n\n\n\n\n\nData Wrangling with dplyr\n\n\n\n\n\n\n\n\n\n\n\n\n\nSept. 19\n\n\n\n\n\nPlotting with ggplot2\n\n\n\n\n\n\n\n\n\n\n\nUnit 02: Regression Basics\n\n\n\n\n\n\n\nSept. 21\n\n\n\n\n\nSimple Linear Regression: Description\n\n\n\n\n\n\n\n\n\n\n\n\nSept. 26\n\n\n\n\n\n\n\nSept. 28\n\n\n\n\n\nOrdinary Least Squares (OLS) Estimation\n\n\n\n\n\n\n\n\n\n\n\n\nOct. 03\n\n\n\n\n\nCorrelation and Standardized Regression\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct. 05\n\n\n\n\n\n\n\nOct. 10\n\n\n\n\n\nCoefficient-Level Inference\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct. 12\n\n\n\n\n\n\n\nOct. 17\n\n\n\n\n\nModel-Level Inference\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct. 19\n\n\n\n\n\nUnit 03: Deeper Understanding\n\n\n\n\n\n\n\nOct. 24\n\n\n\n\n\nIntroduction to Multiple Linear Regression\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct. 26\n\n\n\n\n\n\n\n\nOct. 31\n\n\n\n\n\nUnderstanding Statistical Control\n\n\n\n\n\n\n\n\n\n\n\n\n\nNov. 02\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNov. 07\n\n\n\n\n\nAssumptions of the Regression Model\n\n\n\n\n\n\n\n\n\n\n\n\n\nNov. 09\n\n\n\n\n\nUnit 04: Extending the Regression Model\n\n\n\n\n\n\n\nNov. 14\n\n\n\n\n\nDummy Coding Categorical Predictors\n\n\n\n\n\n\n\n\n\n\n\n\n\nNov. 16\n\n\n\n\n\n\n\nNov. 21\n\n\n\n\n\nPolychotomous Categorical Predictors\n\n\n\n\n\n\n\n\n\n\n\n\n\nNov. 23\n\n\n\n\n\n\n\nNO CLASS—Mental Health Day\n\n\n\n\n\n\n\n\n\n\nNov. 28\n\n\n\n\n\nPolychotomous Categorical Predictors\n\n\n\n\n\n\n\n\n\n\n\n\n\nNov. 30\n\n\n\n\n\n\n\nDec. 05\n\n\n\n\n\nIntroduction to Interaction Effects\n\n\n\n\n\n\n\n\n\n\n\n\n\nDec. 07\n\n\n\n\n\n\n\nDec. 12\n\n\n\n\n\nMore Interaction Effects\n\n\n\n\n\n\n\n\n\n\n\n\nDec. 14"
  },
  {
    "objectID": "umn-policies.html",
    "href": "umn-policies.html",
    "title": "University of Minnesota Policies and Procedures",
    "section": "",
    "text": "Academic freedom is a cornerstone of the University. Within the scope and content of the course as defined by the instructor, it includes the freedom to discuss relevant matters in the classroom. Along with this freedom comes responsibility. Students are encouraged to develop the capacity for critical judgment and to engage in a sustained and independent search for truth. Students are free to take reasoned exception to the views offered in any course of study and to reserve judgment about matters of opinion, but they are responsible for learning the content of any course of study for which they are enrolled.1 Reports of concerns about academic freedom are taken seriously, and there are individuals and offices available for help. Contact the instructor (Andrew Zieffler; zief0002@umn.edu), the Department Chair (Kristen McMaster; mcmas004@umn.edu), your adviser, the associate dean of the college (Tabitha Grier-Reed; grier001@umn.edu), or the Vice Provost for Faculty and Academic Affairs in the Office of the Provost (Rebecca Ropers; ropers@umn.edu)."
  },
  {
    "objectID": "umn-policies.html#classroom",
    "href": "umn-policies.html#classroom",
    "title": "University of Minnesota Policies and Procedures",
    "section": "Classroom",
    "text": "Classroom\n\nMonday/Wednesday (2:30pm–3:45pm): Scott Hall 4"
  },
  {
    "objectID": "umn-policies.html#textbooks",
    "href": "umn-policies.html#textbooks",
    "title": "University of Minnesota Policies and Procedures",
    "section": "Textbooks",
    "text": "Textbooks\nThe course textbook is available via the University of Minnesota library.\n\nRequired: Applied regression: An introduction [required]\n\nThere are two other free, online books that the lectures for the class are drawn from:\n\nComputational Toolkit for Educational Scientists\nStatistical Modeling and Computation for Educational Scientists"
  },
  {
    "objectID": "umn-policies.html#other-resources",
    "href": "umn-policies.html#other-resources",
    "title": "University of Minnesota Policies and Procedures",
    "section": "Other Resources",
    "text": "Other Resources\n\nStatistical Modeling and Computation for Educational Scientists\n\nLearn about simple and multiple regression\nLearn about Ordinary Least squares (OLS)\nLearn about correlation/standardized regression\nLearn about regrression assumptions\nLearn about dummy coding for dichotomous/polychotomous predictors\nLearn about interaction effects\n\nComputational Toolkit for Educational Scientists\n\nLearn basics of R\nLearn {dplyr} for wrangling data\nLearn {ggplot2} for visualizing data"
  },
  {
    "objectID": "umn-policies.html#footnotes",
    "href": "umn-policies.html#footnotes",
    "title": "University of Minnesota Policies and Procedures",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLanguage adapted from the American Association of University Professors “Joint Statement on Rights and Freedoms of Students”.↩︎"
  },
  {
    "objectID": "umn-policies.html#equity-diversity-equal-opportunity-and-affirmative-action",
    "href": "umn-policies.html#equity-diversity-equal-opportunity-and-affirmative-action",
    "title": "University of Minnesota Policies and Procedures",
    "section": "Equity, Diversity, Equal Opportunity, and Affirmative Action",
    "text": "Equity, Diversity, Equal Opportunity, and Affirmative Action\nThe University will provide equal access to and opportunity in its programs and facilities, without regard to race, color, creed, religion, national origin, gender, age, marital status, disability, public assistance status, veteran status, sexual orientation, gender identity, or gender expression. For more information, please consult Board of Regents Policy."
  },
  {
    "objectID": "umn-policies.html#academic-freedom-and-responsibility",
    "href": "umn-policies.html#academic-freedom-and-responsibility",
    "title": "University of Minnesota Policies and Procedures",
    "section": "",
    "text": "Academic freedom is a cornerstone of the University. Within the scope and content of the course as defined by the instructor, it includes the freedom to discuss relevant matters in the classroom. Along with this freedom comes responsibility. Students are encouraged to develop the capacity for critical judgment and to engage in a sustained and independent search for truth. Students are free to take reasoned exception to the views offered in any course of study and to reserve judgment about matters of opinion, but they are responsible for learning the content of any course of study for which they are enrolled.1 Reports of concerns about academic freedom are taken seriously, and there are individuals and offices available for help. Contact the instructor (Andrew Zieffler; zief0002@umn.edu), the Department Chair (Kristen McMaster; mcmas004@umn.edu), your adviser, the associate dean of the college (Tabitha Grier-Reed; grier001@umn.edu), or the Vice Provost for Faculty and Academic Affairs in the Office of the Provost (Rebecca Ropers; ropers@umn.edu)."
  },
  {
    "objectID": "umn-policies.html#appropriate-student-use-of-class-notes-and-course-materials",
    "href": "umn-policies.html#appropriate-student-use-of-class-notes-and-course-materials",
    "title": "University of Minnesota Policies and Procedures",
    "section": "Appropriate Student Use of Class Notes and Course Materials",
    "text": "Appropriate Student Use of Class Notes and Course Materials\nTaking notes is a means of recording information but more importantly of personally absorbing and integrating the educational experience. However, broadly disseminating class notes beyond the classroom community or accepting compensation for taking and distributing classroom notes undermines instructor interests in their intellectual work product while not substantially furthering instructor and student interests in effective learning. Such actions violate shared norms and standards of the academic community. For additional information, please see: https://policy.umn.edu/education/studentresp."
  },
  {
    "objectID": "umn-policies.html#disability-accommodations",
    "href": "umn-policies.html#disability-accommodations",
    "title": "University of Minnesota Policies and Procedures",
    "section": "Disability Accommodations",
    "text": "Disability Accommodations\nThe University of Minnesota views disability as an important aspect of diversity, and is committed to providing equitable access to learning opportunities for all students. The Disability Resource Center (DRC) is the campus office that collaborates with students who have disabilities to provide and/or arrange reasonable accommodations.\n\nIf you have, or think you have, a disability in any area such as, mental health, attention, learning, chronic health, sensory, or physical, please contact the DRC office on your campus (612.626.1333) to arrange a confidential discussion regarding equitable access and reasonable accommodations.\nStudents with short-term disabilities, such as a broken arm, can often work with instructors to minimize classroom barriers. In situations where additional assistance is needed, students should contact the DRC as noted above.\nIf you are registered with the DRC and have a disability accommodation letter dated for this semester or this year, please contact your instructor early in the semester to review how the accommodations will be applied in the course.\nIf you are registered with the DRC and have questions or concerns about your accommodations please contact your (access consultant/disability specialist).\n\nAdditional information is available on the DRC website or e-mail drc@umn.edu with questions."
  },
  {
    "objectID": "umn-policies.html#makeup-work-for-legitimate-absences",
    "href": "umn-policies.html#makeup-work-for-legitimate-absences",
    "title": "University of Minnesota Policies and Procedures",
    "section": "Makeup Work for Legitimate Absences",
    "text": "Makeup Work for Legitimate Absences\nStudents will not be penalized for absence during the semester due to unavoidable or legitimate circumstances. Such circumstances include verified illness, participation in intercollegiate athletic events, subpoenas, jury duty, military service, bereavement, and religious observances. Such circumstances do not include voting in local, state, or national elections. For complete information, please see: https://policy.umn.edu/education/makeupwork."
  },
  {
    "objectID": "umn-policies.html#mental-health-and-stress-management",
    "href": "umn-policies.html#mental-health-and-stress-management",
    "title": "University of Minnesota Policies and Procedures",
    "section": "Mental Health and Stress Management",
    "text": "Mental Health and Stress Management\nAs a student you may experience a range of issues that can cause barriers to learning, such as strained relationships, increased anxiety, alcohol/drug problems, feeling down, difficulty concentrating and/ or lack of motivation. These mental health concerns or stressful events may lead to diminished academic performance and may reduce your ability to participate in daily activities. University of Minnesota services are available to assist you. You can learn more about the broad range of confidential mental health services available on campus via the Student Mental Health Website."
  },
  {
    "objectID": "umn-policies.html#scholastic-dishonesty",
    "href": "umn-policies.html#scholastic-dishonesty",
    "title": "University of Minnesota Policies and Procedures",
    "section": "Scholastic Dishonesty",
    "text": "Scholastic Dishonesty\nYou are expected to do your own academic work and cite sources as necessary. Failing to do so is scholastic dishonesty. Scholastic dishonesty means plagiarizing; cheating on assignments or examinations; engaging in unauthorized collaboration on academic work; taking, acquiring, or using test materials without faculty permission; submitting false or incomplete records of academic achievement; acting alone or in cooperation with another to falsify records or to obtain dishonestly grades, honors, awards, or professional endorsement; altering, forging, or misusing a University academic record; or fabricating or falsifying data, research procedures, or data analysis. (Student Conduct Code)\nIf it is determined that a student has cheated, the student may be given an “F” or an “N” for the course, and may face additional sanctions from the University. For additional information, please see the policy statement. The Office for Community Standards has compiled a useful list of Frequently Asked Questions pertaining to scholastic dishonesty. If you have additional questions, please clarify with your instructor for the course. Your instructor can respond to your specific questions regarding what would constitute scholastic dishonesty in the context of a particular class—e.g., whether collaboration on assignments is permitted, requirements and methods for citing sources, if electronic aids are permitted or prohibited during an exam."
  },
  {
    "objectID": "umn-policies.html#senate-academic-workload-policy",
    "href": "umn-policies.html#senate-academic-workload-policy",
    "title": "University of Minnesota Policies and Procedures",
    "section": "Senate Academic Workload Policy",
    "text": "Senate Academic Workload Policy\nOne conventional credit is hereby defined as equivalent to three hours of learning effort per week, averaged over an appropriate time interval, necessary for an average student taking that course to achieve an average grade in that course. It is expected that the academic work required of graduate and professional students will exceed three hours per credit per week or 45 hours per semester."
  },
  {
    "objectID": "umn-policies.html#sexual-assault-and-higher-education-training-modules-and-information",
    "href": "umn-policies.html#sexual-assault-and-higher-education-training-modules-and-information",
    "title": "University of Minnesota Policies and Procedures",
    "section": "Sexual Assault and Higher Education: Training Modules and Information",
    "text": "Sexual Assault and Higher Education: Training Modules and Information\nThe Department of Educational Psychology supports the efforts of the University of Minnesota towards prevention of sexual assault. We encourage all students to participate in the free online training that has been established for undergraduate students and graduate students. The training highlights pertinent issues regarding sexual assault, including, but not limited to: defining healthy relationships, consent, bystander intervention, and gender roles. The guide for the training in your My Training page is available at https://it.umn.edu/training-guide-preventing-responding. Additionally, to learn more about how you can help reduce sexual assault at the University of Minnesota, please visit the Aurora Center."
  },
  {
    "objectID": "umn-policies.html#sexual-harassment",
    "href": "umn-policies.html#sexual-harassment",
    "title": "University of Minnesota Policies and Procedures",
    "section": "Sexual Harassment",
    "text": "Sexual Harassment\n“Sexual harassment” means unwelcome sexual advances, requests for sexual favors, and/or other verbal or physical conduct of a sexual nature. Such conduct has the purpose or effect of unreasonably interfering with an individual’s work or academic performance or creating an intimidating, hostile, or offensive working or academic environment in any University activity or program. Such behavior is not acceptable in the University setting. For additional information, please consult Board of Regents Policy"
  },
  {
    "objectID": "umn-policies.html#student-conduct-code",
    "href": "umn-policies.html#student-conduct-code",
    "title": "University of Minnesota Policies and Procedures",
    "section": "Student Conduct Code",
    "text": "Student Conduct Code\nThe University seeks an environment that promotes academic achievement and integrity, that is protective of free inquiry, and that serves the educational mission of the University. Similarly, the University seeks a community that is free from violence, threats, and intimidation; that is respectful of the rights, opportunities, and welfare of students, faculty, staff, and guests of the University; and that does not threaten the physical or mental health or safety of members of the University community. As a student at the University you are expected adhere to Board of Regents Policy: Student Conduct Code.\nNote that the conduct code specifically addresses disruptive classroom conduct, which means “engaging in behavior that substantially or repeatedly interrupts either the instructor’s ability to teach or student learning. The classroom extends to any setting where a student is engaged in work toward academic credit or satisfaction of program-based requirements or related activities.”"
  },
  {
    "objectID": "umn-policies.html#use-of-personal-electronic-devices-in-the-classroom",
    "href": "umn-policies.html#use-of-personal-electronic-devices-in-the-classroom",
    "title": "University of Minnesota Policies and Procedures",
    "section": "Use of Personal Electronic Devices in the Classroom",
    "text": "Use of Personal Electronic Devices in the Classroom\nUsing personal electronic devices in the classroom setting can hinder instruction and learning, not only for the student using the device but also for other students in the class. To this end, the University establishes the right of each faculty member to determine if and how personal electronic devices are allowed to be used in the classroom. For complete information, please reference: https://policy.umn.edu/education/studentresp."
  },
  {
    "objectID": "umn-policies.html#grading-and-transcripts",
    "href": "umn-policies.html#grading-and-transcripts",
    "title": "University of Minnesota Policies and Procedures",
    "section": "Grading and Transcripts",
    "text": "Grading and Transcripts\nUniversity Grading Scales The University has two distinct grading scales: A–F and S–N.\nA–F grading scale. The A–F grading scale allows the following grades and corresponding GPA points:\n\n\n\n\n\n\n\n\nGrade\nGPA Points\nDefinitions for undergraduate credit\n\n\n\n\nA\n4.000\nRepresents achievement that significantly exceeds expectations in the course.\n\n\nA-\n3.667\n\n\n\nB+\n3.333\n\n\n\nB\n3.000\nRepresents achievement that is above the minimum expectations in the course.\n\n\nB-\n2.667\n\n\n\nC+\n2.333\n\n\n\nC\n2.000\nRepresents achievement that meets the minimum expectations in the course.\n\n\nC-\n1.667\n\n\n\nD+\n1.333\n\n\n\nD\n1.000\nRepresents achievement that partially meets the minimum expectations in the course. Credit is earned but it may not fulfill major or program requirements.\n\n\nF\n0.000\nRepresents failure in the course and no credit is earned.\n\n\n\nS–N grading scale. The S–N grading scale allows for the following grades and corresponding GPA points:\n\n\n\nGrade\nGPA Points\nDefinitions for undergraduate credit\n\n\n\n\nS\n0.000\nSatisfactory (equivalent to a C- or better).\n\n\nN\n0.667\nNot Satisfactory\n\n\n\nFor additional information, please refer to: https://policy.umn.edu/education/gradingtranscripts."
  },
  {
    "objectID": "index.html#welcome-to-epsy-8251",
    "href": "index.html#welcome-to-epsy-8251",
    "title": "EPsy 8251",
    "section": "",
    "text": "Methods in Data Analysis for Educational Research I is the first course in an entry-level, doctoral sequence for students in education. The two semester sequence provides in-depth coverage of widely used statistical methods and models and prepares students for advanced statistical coursework. EPsy 8251 provides rigorous coverage of estimation and hypothesis testing with a particular focus on the General Linear Model.\n\n\nIn this class, we will work together to develop a learning community that is inclusive and respectful, and where every student is supported in the learning process. As a class full of diverse individuals (reflected by differences in race, culture, age, religion, gender identity, sexual orientation, socioeconomic background, abilities, professional goals, and other social identities and life experiences) I expect that different students may need different things to support and promote their learning. The TAs and I will do everything we can to help with this, but as we only know what we know, we need you to communicate with us if things are not working for you or you need something we are not providing. I hope you all feel comfortable in helping to promote an inclusive classroom through respecting one another’s individual differences, speaking up, and challenging oppressive/problematic ideas. Finally, I look forward to learning from each of you and the experiences you bring to the class."
  },
  {
    "objectID": "index.html#welcome-to-epsy-8251-methods-in-data-analysis-for-educational-research-i",
    "href": "index.html#welcome-to-epsy-8251-methods-in-data-analysis-for-educational-research-i",
    "title": "",
    "section": "Welcome to EPsy 8251: Methods in Data Analysis for Educational Research I",
    "text": "Welcome to EPsy 8251: Methods in Data Analysis for Educational Research I\nEPsy 8251: Methods in Data Analysis for Educational Research I is the first course in an entry-level, doctoral sequence for students in education. The two semester sequence provides in-depth coverage of widely used statistical methods and models and prepares students for advanced statistical coursework. EPsy 8251 provides rigorous coverage of estimation and hypothesis testing with a particular focus on the General Linear Model. The roadmap for the course is:\n\n\n\n\n\n\n\n\n\nEPsy 8251 is a 3 credit course. It is expected that the academic work required of Graduate School and professional school students will exceed three hours per credit per week (see Expected Student Academic Work Policy). In my experience, it is typical for students to spend 10–15 hours a week on this course. As with every class, some students will spend more time than that on this course, while others will spend less time than that—it all depends on your prior experiences with statistics and computing. If you find yourself consistently spending more than 20 hours a week on the course, please make an appointment to see the instructor so that we can strategize about how to best optimize how you are devoting time to the course. \n\nA Note on Inclusion and Respect\nIn this class, we will work together to develop a learning community that is inclusive and respectful, and where every student is supported in the learning process. As a class full of diverse individuals (reflected by differences in race, culture, age, religion, gender identity, sexual orientation, socioeconomic background, abilities, professional goals, and other social identities and life experiences) I expect that different students may need different things to support and promote their learning. The TAs and I will do everything we can to help with this, but as we only know what we know, we need you to communicate with us if things are not working for you or you need something we are not providing. I hope you all feel comfortable in helping to promote an inclusive classroom through respecting one another’s individual differences, speaking up, and challenging oppressive/problematic ideas. Finally, I look forward to learning from each of you and the experiences you bring to the class."
  },
  {
    "objectID": "index.html#course-requirements",
    "href": "index.html#course-requirements",
    "title": "EPsy 8251",
    "section": "Course Requirements",
    "text": "Course Requirements\nStudents will complete eight homework assignments. The homework assignments and due dates will be posted on the Assignments page of the course website. These assignments include problems that will help you learn the course material through reflection and practice. Submit each assignment as a PDF file via email to the TA.\nTo foster cooperation and collaboration, you are permitted to form groups of no larger than three to work on the homework. Submit only one assignment per group, and list the names of each group member on the assignment. Each assignment will be scored and this score will be given to all individuals in the group. From past experience, student collaborations work most fluidly when everyone in the group has chosen the same grading option for the course (e.g., A/F, S/N, etc.).\nIf you work alone on the assignments, you need to truly work alone. To protect against running afoul of the scholastic dishonesty policy, students working alone are not permitted to interact with any other student in regards to the assignment, including discussion, obtaining help, etc.\n\n\nEvaluation of Student Performance\nCourse grades will be based entirely on performance on the homework assignments. The points from the eight homework assignment will be pooled to compute a percentage in the class, which will be converted to a final course grade using:\n\n\n\nCutoff\nGrade\nCutoff\nGrade\nCutoff\nGrade\n\n\n\n\n93%\nA\n83%\nB\n73%\nC\n\n\n90%\nA–\n80%\nB–\n70%\nC–\n\n\n87%\nB+\n77%\nC+\n63%\nD\n\n\n\nStudents who earn below 63% will receive the letter grade of F. If you are taking the course S/N, the minimum criterion to receive an S is 80% (the equivalent of a B– letter grade). Any student who does not complete all homework assignments without making prior arrangements with the instructor will receive a grade of F/N.\n\n\n\nIncomplete\nAn incomplete will be assigned only in extraordinary circumstances (e.g., hospitalization). An incomplete is an arranged grade which requires a written contract between instructor and student that includes by when and how the incomplete will be satisfied. Incomplete contract forms are available at https://z.umn.edu/incompletegradecontract or from program staff in 250 EdSciB.\n\n\n\nAccessing Course Grades\nShortly after the course, you may access your grades online at myU. Assignments will be handed back in class or during office hours. Uncollected assignments will be retained for six weeks after the course and then discarded.\n\n\n\nStress Management\nStress management is an important piece of the skill set needed for success in graduate school. Pet Away Worry & Stress (PAWS) is one of the many resources available to students. Find out more at https://boynton.umn.edu/paws.\n\nYou can follow Tilly the Therapy Chicken on Twitter (@TherapyChicken)."
  },
  {
    "objectID": "index.html#evaluation-of-student-performance",
    "href": "index.html#evaluation-of-student-performance",
    "title": "EPsy 8251",
    "section": "Evaluation of Student Performance",
    "text": "Evaluation of Student Performance\nCourse grades will be based entirely on performance on the homework assignments. The points from the eight homework assignment will be pooled to compute a percentage in the class, which will be converted to a final course grade using:\n\n\n\nCutoff\nGrade\nCutoff\nGrade\nCutoff\nGrade\n\n\n\n\n93%\nA\n83%\nB\n73%\nC\n\n\n90%\nA–\n80%\nB–\n70%\nC–\n\n\n87%\nB+\n77%\nC+\n63%\nD\n\n\n\nStudents who earn below 63% will receive the letter grade of F. If you are taking the course S/N, the minimum criterion to receive an S is 80% (the equivalent of a B– letter grade). Any student who does not complete all homework assignments without making prior arrangements with the instructor will receive a grade of F/N."
  },
  {
    "objectID": "index.html#technology",
    "href": "index.html#technology",
    "title": "EPsy 8251",
    "section": "Technology",
    "text": "Technology\nThe course uses technology on a regular basis during both instruction and assessments (e.g., homework assignments, exams, etc.). Student difficulty with obtaining or operating the various software programs and technologies—including printer trouble—will not be acceptable as an excuse for late work. Due to the variation in computer types and systems, the instructor or TA may not be able to assist in trouble shooting all problems you may have."
  },
  {
    "objectID": "index.html#statistical-computing",
    "href": "index.html#statistical-computing",
    "title": "",
    "section": "Statistical Computing",
    "text": "Statistical Computing\nStatistical computing is an integral part of statistical work, and subsequently, EPsy 8251. To support your learning in this area, this course will emphasize the use of R. R is a free software environment for statistical computing and graphics. It compiles and runs on a wide variety of UNIX platforms, Windows and MacOS https://www.r-project.org. It should be noted that while some R syntax and programming is taught during class time, there is also a fair amount that you may need to learn on your own outside of class. There are several tutorials and resources linked from the course website to help you learn R.\nYou can install R and RStudio onto your local machine. (There are instructions for how to do this on the course website.) You are responsible for getting things to work on your computer. While it should be straightforward, each OS and computer has their quirks. I can try to help you with this if you are having trouble.\n\n\nTechnology Policy\nThe course uses technology on a regular basis during both instruction and assessments (e.g., homework assignments, exams, etc.). Student difficulty with obtaining or operating the various software programs and technologies—including printer trouble—will not be acceptable as an excuse for late work. Due to the variation in computer types and systems, the instructor or TA may not be able to assist in trouble shooting all problems you may have.\n\n\n\n\nCEHD Policy on Recording Classes\nAll class sessions may be recorded by the instructor using the procedures in the CEHD Policy on Recording Classes, with or without prior notice. Students should assume that a class session is being recorded unless otherwise notified. No person (student or otherwise) may record a class without express written permission from the instructor or an authorized administrator implementing a disability accommodation. All permitted recordings are governed by this policy’s limits on distribution and redistribution of recordings."
  },
  {
    "objectID": "index.html#image-attribution",
    "href": "index.html#image-attribution",
    "title": "",
    "section": "Image Attribution",
    "text": "Image Attribution\n\nThe icon in the Inclusion Note was created by Freepik - Flaticon.\nThe icon in the Technology Policy was created by Freepik - Flaticon.\nThe icon of Tilly the Therapy Chicken in the Stress Managment note is used with permissin of the PAWS program.\nThe icon in the CEHD Policy on Recording note was created by Hilmy Abiyyu A. - Flaticon.\nThe icon in the Missing Class Policy was created by Freepik - Flaticon."
  },
  {
    "objectID": "index.html#course-requirements-and-grading",
    "href": "index.html#course-requirements-and-grading",
    "title": "",
    "section": "Course Requirements and Grading",
    "text": "Course Requirements and Grading\nStudents will complete eight homework assignments. The homework assignments and due dates will be posted on the Assignments page of the course website. These assignments include problems that will help you learn the course material through reflection and practice. Submit each assignment as a PDF file via email to the TA.\nTo foster cooperation and collaboration, you are permitted to form groups of no larger than three to work on the homework. Submit only one assignment per group, and list the names of each group member on the assignment. Each assignment will be scored and this score will be given to all individuals in the group. From past experience, student collaborations work most fluidly when everyone in the group has chosen the same grading option for the course (e.g., A/F, S/N, etc.).\nIf you work alone on the assignments, you need to truly work alone. To protect against running afoul of the scholastic dishonesty policy, students working alone are not permitted to interact with any other student in regards to the assignment, including discussion, obtaining help, etc.\n\n\nPolicy for Missing Class and Making up Missed/Late Work\nStudents are responsible for planning their schedules to avoid excessive conflicts with course requirements and must notify the instructor of unavoidable scheduling conflicts as early as possible. For circumstances where absences are unavoidable, accommodations for makeup work will be made according to University Policy. If you miss class:\n\nEmail the instructor as soon as you know you will be missing class.\nStudents are expected to obtain notes from a classmate of class material missed.\nPlease note that I will not be recording class sessions at the request of individual students, nor will I be Zooming students in to the class. Although, if you can arrange it with a classmate, they can Zoom you in.\n\nIf you are zooming in a classmate, please let the instructor know.\n\nIf you will be gone the day an assignment is due, you will need to make arrangements with the instructor about when you will turn in the assignment.\n\nIf you do not communicate with the instructor and make arrangements for turning in work when you are absent, the assignment will receive a 0.\n\n\n\nEvaluation of Student Performance\nCourse grades will be based entirely on performance on the homework assignments. The points from the eight homework assignment will be pooled to compute a percentage in the class, which will be converted to a final course grade using:\n\n\n\n\n\n\n\n\nCutoff\nGrade\nDefinition for Graduate Credit\n\n\n\n\n93%–100%\nA\nFor exceptional work, well above the minimum criteria\n\n\n90%–92%\nA–\nFor outstanding work, well above the minimum criteria\n\n\n87%–89%\nB+\nFor excellent work, significant above the minimum criteria\n\n\n83%–86%\nB\nFor work above the minimum criteria\n\n\n80%–82%\nB–\n\n\n\n77%–79%\nC+\n\n\n\n73%–76%\nC\nFor work which meets the course requirements in every respect\n\n\n70%–72%\nC–\n\n\n\n63%–69%\nD\nWorthy of credit even though it fails to meet the course requirements\n\n\n0%–62%\nF\nFailed to meet minimum course requirements\n\n\n\nIf you are taking the course S/N, the minimum criterion to receive an S is 80% (the equivalent of a B– letter grade). The S grade does not carry grade points and is not part of the GPA calculation, but the credits will count toward the student’s degree program if allowed by the college, campus, or program.\nAny student who does not complete all homework assignments without making prior arrangements with the instructor will receive a grade of F/N.\n\n\n\nIncomplete\nInstructors may assign the registration symbol “I” for Incomplete if, at the time the incomplete is requested: (1) the student has successfully completed a substantial portion of the work of the course; and (2) due to extraordinary circumstances (as determined by the instructor), the student was prevented from completing the work of the course on time. The assignment of an “I” requires a written agreement with the student specifying the time and manner in which the student will complete the course requirements. For more information see Grading and Transcripts.\n\n\n\nAccessing Course Grades\nShortly after the course, you may access your grades online at myU. Assignments will be handed back in class or during office hours. Uncollected assignments will be retained for six weeks after the course and then discarded.\n\n\n\nStress Management\nStress management is an important piece of the skill set needed for success in graduate school. Pet Away Worry & Stress (PAWS) is one of the many resources available to students. Find out more at https://boynton.umn.edu/paws.\n\nYou can follow Tilly the Therapy Chicken on Twitter (@TherapyChicken)."
  },
  {
    "objectID": "umn-policies.html#sexual-harassment-sexual-assault-stalking-and-relationship-violence",
    "href": "umn-policies.html#sexual-harassment-sexual-assault-stalking-and-relationship-violence",
    "title": "University of Minnesota Policies and Procedures",
    "section": "Sexual Harassment, Sexual Assault, Stalking and Relationship Violence",
    "text": "Sexual Harassment, Sexual Assault, Stalking and Relationship Violence\nThe University prohibits sexual misconduct, and encourages anyone experiencing sexual misconduct to access resources for personal support and reporting. If you want to speak confidentially with someone about an experience of sexual misconduct, please contact your campus resources including the Aurora Center, Boynton Mental Health or Student Counseling Services https://eoaa.umn.edu/report-misconduct. If you want to report sexual misconduct, or have questions about the University’s policies and procedures related to sexual misconduct, please contact your campus Title IX office or relevant policy contacts.\nInstructors are required to share information they learn about possible sexual misconduct with the campus Title IX office that addresses these concerns. This allows a Title IX staff member to reach out to those who have experienced sexual misconduct to provide information about personal support resources and options for investigation. You may talk to instructors about concerns related to sexual misconduct, and they will provide support and keep the information you share private to the extent possible given their University role. https://regents.umn.edu/sites/regents.umn.edu/files/2019-09/policy_sexual_harassment_sexual_assault_stalking_and_relationship_violence.pdf"
  },
  {
    "objectID": "assignments/assignment-01-introduction-to-computing.html",
    "href": "assignments/assignment-01-introduction-to-computing.html",
    "title": "Assignment 01",
    "section": "",
    "text": "The goal of this assignment is to give you experience working with the R statistical computing environment. In this assignment, you will use the data from the file broadband.csv to examine broadband access in the United States."
  },
  {
    "objectID": "assignments/assignment-01-introduction-to-computing.html#instructions",
    "href": "assignments/assignment-01-introduction-to-computing.html#instructions",
    "title": "Assignment 01",
    "section": "Instructions",
    "text": "Instructions\nSubmit a printed document of your responses to the following questions. Please adhere to the following guidelines for further formatting your assignment:\n\nAll plots should be resized so that they do not take up more room than necessary.\nAll figures and tables should have a name (e.g., Figure 1) and an appropriate caption.\n\nThis assignment is worth 11 points."
  },
  {
    "objectID": "assignments/assignment-01-introduction-to-computing.html#preparation",
    "href": "assignments/assignment-01-introduction-to-computing.html#preparation",
    "title": "Assignment 01",
    "section": "Preparation",
    "text": "Preparation\nIf you have not already installed them, you will need to install the {dplyr} and {ggplot2} packages to complete this assignment. Once these have been installed successfully, you should not need to install them again. Remember: Install once; load every session.\nOpen a new script file. Save the script file as Assignment-01.R. Save all of the R syntax you use to answer the questions on this assignment in this script file.\nDenote each question in the script file using comments. For example,\n##################################################\n### Question 1\n##################################################\n\n&lt;&lt; syntax &gt;&gt;\nAdd comments throughout your syntax as liberally as you feel is necessary to help you recall what the syntax does in the future. Although you do not need to submit this in with your assignment, it will be useful for building good coding habits and potentially for future assignments."
  },
  {
    "objectID": "assignments/assignment-01-introduction-to-computing.html#part-i-minnesota-broadband-access",
    "href": "assignments/assignment-01-introduction-to-computing.html#part-i-minnesota-broadband-access",
    "title": "Assignment 01",
    "section": "Part I: Minnesota Broadband Access",
    "text": "Part I: Minnesota Broadband Access\nAfter importing the broadband data into RStudio, create a new data frame object that only includes counties in Minnesota (postal code for Minnesota is MN). Hint: Use filter(). Assign the Minnesota broadband data into a new object called mn. Use the mn data to answer the following questions.\n\nUse the geom_density() function from the {ggplot2} package to create a density plot of the distribution of the microsoft_useage variable for all 87 counties in Minnesota. You can see many examples of how to do this here. Include this plot in a word-processed document. Resize the plot so it does not take up any more space than necessary. Be sure the plot has appropriate labels and has a caption.\nUse the geom_density() function from the {ggplot2} package to create a density plot of the distribution of the microsoft_useage variable for the counties in Minnesota, but this time use color= or fill= to color by whether the county is a metropolitan or non-metropolitan county. (Hint: You should get two density plots.). Include this plot in a word-processed document. Resize the plot so it does not take up any more space than necessary. Be sure the plot has appropriate labels and has a caption.\nCompute the mean and the standard deviation of the microsoft_useage variable for metro and non-metro counties. Report these values.\nBy referring to the values, and the plots, what can you say about broadband access in Minnesota? Is there a broadband access gap?"
  },
  {
    "objectID": "assignments/assignment-01-introduction-to-computing.html#part-ii-state-level-broadband-access",
    "href": "assignments/assignment-01-introduction-to-computing.html#part-ii-state-level-broadband-access",
    "title": "Assignment 01",
    "section": "Part II: State-Level Broadband Access",
    "text": "Part II: State-Level Broadband Access\nIn this section, you will use all the data in broadband.csv to create state-level summaries (means and standard deviations) of the microsoft_useage and the fcc_availability variables. Filter out Washington DC so it is not included. (Hint: Make sure that every state has mean and standard deviation values when you are done!) Use this state-level data to answer the following questions.\n\nBased on the Microsoft and FCC estimates, does it seem that the FCC estimates (which are provided by the ISPs) are “overestimating” America’s broadband access? Explain.\nUsing the Microsoft estimates, which states have the best and worst broadband access based on the means? Which states are the most and least homogenous (across counties) in terms of their broadband access?\nCreate a table that includes the Microsoft and FCC estimates of broadband access you computed for the states in bordering Minnesota: Minnesota (MN), Iowa (IA), South Dakota (SD), North Dakota (ND), and Wisconsin (WI). Report these values in a word-processed table alphabetically by state. To format this table: Examine the structure and formatting of Table 2 at https://zief0002.github.io/musings/creating-tables-to-present-statistical-results.html. Notice that variables are presented in rows and summary statistics are presented in columns. Mimic the format and structure of this table to create a table to present the numerical summary information asked for in this question. Finally, give your table a name (e.g., Table 1) and an appropriate caption."
  },
  {
    "objectID": "assignments/assignment-01-introduction-to-computing.html#part-iii-state-level-broadband-access-versus-poverty",
    "href": "assignments/assignment-01-introduction-to-computing.html#part-iii-state-level-broadband-access-versus-poverty",
    "title": "Assignment 01",
    "section": "Part III: State-Level Broadband Access versus Poverty",
    "text": "Part III: State-Level Broadband Access versus Poverty\nIn this section, you will use all the data in broadband.csv to create state-level means of the microsoft_useage and the pct_poverty variables. (Hint: Make sure that every state has mean values when you are done!) Use this state-level data to answer the following questions.\n\nUse ggplot() to create a scatterplot of the relationship between the proportion of users with access to broadband and the proportion of people living in poverty using the state-level data. (Put the proportion living in poverty on the x-axis.) Change the axis labels so that both the x- and y-axis have labels that suitably describe the variables being plotted. (For help on this, read the Axes page of the Cookbook for R website.) Include this plot in a word-processed document. Resize the plot so it does not take up any more space than necessary. Finally, give your figure a name (e.g., Figure 1) and an appropriate caption.\nBased on these data, describe the relationship between poverty and broadband access.\nIdentify Minnesota on your plot. (You do not have to use R to do this; you can use an image editor, or do it by hand.) You can either create a new plot or just include Minnesota on the plot you include for Question 8.\nBased on its location in the plot, how does Minnesota compare to the other states in terms of broadband access? What about in terms of poverty? Explain."
  },
  {
    "objectID": "codebooks/broadband.html",
    "href": "codebooks/broadband.html",
    "title": "broadband.csv",
    "section": "",
    "text": "Russell Brandom and William Joel in an article for The Verge wrote,\n\n“If broadband access was a problem before 2020, the pandemic turned it into a crisis. As everyday businesses moved online, city council meetings or court proceedings became near-inaccessible to anyone whose connection couldn’t support a Zoom call.”\n\nBut, who in America has access to broadband internet? As part of their ongoing work to improve software and service performance and security, Microsoft collected router speed data from individuals who accessed their cloud services. After aggregating and anonomyzing these data, they made these data available publicly to help researchers and policymakers understand and improve problems related to broadband access.\nThe data in broadband.csv, collected by Microsoft (2021), give us much better insight as to the true broadband access (defined as internet download speeds of at least 25 Mbps) of Americans as, to date, most studies of broadband access have use data collated by the FCC that is based on individual Internet Service Providers’ descriptions of the areas they serve. To better contextualize this, the data have also been augmented with several county-level poverty and education indicators. The variables are:\n\nstate: State postal code\ncounty: County name\nfips: Five-digit Federal Information Processing Standards code which uniquely identified counties and county equivalents in the United States\nrural_urban: Rural-urban continuum code\n\n1: Metropolitan - Counties in metropolitan areas of 1 million population or more\n2: Metropolitan - Counties in metropolitan areas of 250,000 to 1 million population\n3: Metropolitan - Counties in metropolitan areas of fewer than 250,000 population\n4: Nonmetropolitan - Urban population of 20,000 or more, adjacent to a metropolitan area\n5: Nonmetropolitan - Urban population of 20,000 or more, not adjacent to a metropolitan area\n6: Nonmetropolitan - Urban population of 2,500 to 19,999, adjacent to a metropolitan area\n7: Nonmetropolitan - Urban population of 2,500 to 19,999, not adjacent to a metropolitan area\n8: Nonmetropolitan - Completely rural or less than 2,500 urban population, adjacent to a metropolitan area\n9: Nonmetropolitan - Completely rural or less than 2,500 urban population, not adjacent to a metropolitan area\n\nmetro: Classification of the county as metropolitan (metro) or nonmetropolitan (nonmetro) based on the rural-urban continuum code\nfcc_availability: Proportion of people in the county with access to fixed terrestrial broadband at speeds of 25 Mbps/3 Mbps as of the end of 2019 as measured by the FCC\nmicrosoft_useage: Proportion of people in the county that use the internet at broadband speeds estimated by Microsoft\npct_poverty: Estimate of the percentage of people (of all ages) in the county living in poverty in 2019\nmedian_income: Estimate of median household income in the county in 2019\nlt_hs_2019: Percentage of the county with less than a high school diploma (2015–2019)\nhs_2019: Percentage of the county with a high school diploma (2015–2019)\nsome_college_2019: Percentage of the county with some college or an associate’s degree (2015–2019)\ncollege_2019: Percentage of the county with a bachelor’s degree, or higher (2015–2019)\n\n\n\nPreview\n\n# Import data\nbroadband = readr::read_csv(file = \"https://raw.githubusercontent.com/zief0002/rustic-penguin/main/data/broadband.csv\")\n\n# View data\nbroadband\n\n# A tibble: 3,143 × 13\n   state county         fips rural_urban metro fcc_availability microsoft_useage\n   &lt;chr&gt; &lt;chr&gt;         &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;            &lt;dbl&gt;            &lt;dbl&gt;\n 1 AL    Autauga Coun…  1001           2 Metro             0.81             0.28\n 2 AL    Baldwin Coun…  1003           3 Metro             0.88             0.3 \n 3 AL    Barbour Coun…  1005           6 Nonm…             0.59             0.18\n 4 AL    Bibb County    1007           1 Metro             0.29             0.07\n 5 AL    Blount County  1009           1 Metro             0.69             0.09\n 6 AL    Bullock Coun…  1011           6 Nonm…             0.06             0.05\n 7 AL    Butler County  1013           6 Nonm…             0.78             0.11\n 8 AL    Calhoun Coun…  1015           3 Metro             0.93             0.32\n 9 AL    Chambers Cou…  1017           6 Nonm…             0.82             0.34\n10 AL    Cherokee Cou…  1019           6 Nonm…             0.99             0.1 \n# ℹ 3,133 more rows\n# ℹ 6 more variables: pct_poverty &lt;dbl&gt;, median_income &lt;dbl&gt;, lt_hs_2019 &lt;dbl&gt;,\n#   hs_2019 &lt;dbl&gt;, some_college_2019 &lt;dbl&gt;, college_2019 &lt;dbl&gt;\n\n\n\n\n\nReferences\nBrandom, R., & Joel, W. (2021, May 10). This is a map of America’s broadband problem: A county-by-county look at the broadband gap. The Verge.\nEconomic Research Service, U.S. Department of Agriculture. (2021). County-Level Data Sets.\nMicrosoft. (2021). United States Broadband Usage Percentages Dataset. Github repository."
  },
  {
    "objectID": "codebooks/comic-characters.html",
    "href": "codebooks/comic-characters.html",
    "title": "comic-characters.csv",
    "section": "",
    "text": "The data in comic-characters.csv contains data on 14 attributes for 23,272 comic characters. These data were scraped in 2014 from from Marvel Wikia and DC Wikia by FiveThirtyEight and used in the story Comic Books Are Still Made By Men, For Men And About Men. The variables are:\n\ncharacter: The name of the character\ncomic: If the character appears in DC Comics or Marvel Comics\nreality: Comic reality the character appears in\nidentity: The identity status of the character (Secret Identity, Public identity, No Dual Identity)\nalignment: If the character is Good, Bad or Neutral\neye_color: Eye color of the character\nhair_color: Hair color of the character\nsex: Sex of the character (e.g. Male, Female, etc.)\nlgbtq: If the character is identified as LGBTQ\nlgbtq_note: Additional information if the character is identified as LGBTQ\nalive: If the character is alive or deceased\nappearances: The number of appearances of the character in comic books (as of September 2, 2014)\nfirst_appear_date: The month and year of the character’s first appearance in a comic book, if available\nfirst_appear_year: The year of the character’s first appearance in a comic book, if available\n\n\n\nPreview\n\n# Import Data\ncomics = readr::read_csv(file = \"https://raw.githubusercontent.com/zief0002/rustic-penguin/main/data/comic-characters.csv\")\n\n# View data\ncomics\n\n# A tibble: 23,272 × 14\n   character   comic reality identity alignment eye_color hair_color sex   lgbtq\n   &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;\n 1 \"14\"        Marv… Earth-… Secret … Bad       &lt;NA&gt;      &lt;NA&gt;       Fema… No   \n 2 \"88\"        Marv… Earth-… Public … Bad       Blue      Blond      Male  No   \n 3 \"99\"        Marv… Earth-… Secret … Neutral   Blue      &lt;NA&gt;       Male  No   \n 4 \"107\"       Marv… Earth-… Secret … Neutral   Green     &lt;NA&gt;       Male  No   \n 5 \"'Spinner\"  Marv… Earth-… Secret … Good      &lt;NA&gt;      &lt;NA&gt;       Male  No   \n 6 \"\\\"Thumper… Marv… Earth-… Secret … Bad       &lt;NA&gt;      Bald       Male  No   \n 7 \"11-Ball\"   Marv… Earth-… Secret … Bad       &lt;NA&gt;      &lt;NA&gt;       Male  No   \n 8 \"115 (Legi… Marv… Earth-… Secret … Neutral   Blue      White      Fema… No   \n 9 \"181 (Legi… Marv… Earth-… Secret … Neutral   &lt;NA&gt;      &lt;NA&gt;       &lt;NA&gt;  No   \n10 \"1X\"        Marv… Earth-… Public … Good      &lt;NA&gt;      Blond      Male  No   \n# ℹ 23,262 more rows\n# ℹ 5 more variables: lgbtq_note &lt;chr&gt;, alive &lt;chr&gt;, appearances &lt;dbl&gt;,\n#   first_appear_date &lt;chr&gt;, first_appear_year &lt;dbl&gt;"
  },
  {
    "objectID": "codebooks/colleges-bordering-mn.html",
    "href": "codebooks/colleges-bordering-mn.html",
    "title": "colleges-bordering-mn.csv",
    "section": "",
    "text": "The data in colleges-bordering-mn.csv contains institutional data for 104 colleges and universities in the five state area (MN, IA, WI, ND, and SD). These data were collected by the Department of Education for the 2013 College Score Card. The variables are:\n\nname: Name of college/university\ntuition_in_state: In-state tuition and fees\ntuition_out_state: Out-of-state tuition and fees\nstate: State postal abbreviation (IA = Iowa, MN = Minnesota, ND = North Dakota, SD = South Dakota, WI = Wisconsin)\npublic: Dummy-coded variable indicating educational sector (1 = public school, 0 = private school)\nmain: Dummy-coded variable indicating whether the campus is the school’s main campus? (1 = yes, 0 = no)\nadmission: Admission rate\nact75: 75th percentile of the ACT cumulative scores\navg_fac_salary: Average faculty salary (per month)\ncompletion: Four-year completion rate for first-time, full-time students\npct_pell: Percentage of undergraduates who receive a Pell grant\n\n\n\nPreview\n\n# Import Data\ncolleges = readr::read_csv(file = \"https://raw.githubusercontent.com/zief0002/rustic-penguin/main/data/colleges-bordering-mn.csv\")\n\n# View data\ncolleges\n\n# A tibble: 104 × 11\n   name    tuition_in_state tuition_out_state state public  main admission act75\n   &lt;chr&gt;              &lt;dbl&gt;             &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1 Buena …            29448             29448 IA         0     1     0.696    25\n 2 Centra…            30700             30700 IA         0     1     0.658    26\n 3 Clarke…            28000             28000 IA         0     1     0.775    25\n 4 Coe Co…            35730             35730 IA         0     1     0.617    28\n 5 Cornel…            36430             36430 IA         0     1     0.641    29\n 6 Dordt …            26540             26540 IA         0     1     0.754    28\n 7 Drake …            30889             30889 IA         0     1     0.660    30\n 8 Univer…            25520             25520 IA         0     1     0.774    22\n 9 Gracel…            23530             23530 IA         0     1     0.502    24\n10 Grand …            22628             22628 IA         0     1     0.954    23\n# ℹ 94 more rows\n# ℹ 3 more variables: avg_fac_salary &lt;dbl&gt;, completion &lt;dbl&gt;, pct_pell &lt;dbl&gt;"
  },
  {
    "objectID": "codebooks/evaluations.html",
    "href": "codebooks/evaluations.html",
    "title": "evaluations.csv",
    "section": "",
    "text": "The data in evaluations.csv come from Hamermesh & Parker (2005) and were made available by Gelman & Hill (2007). This data were collected from student evaluations of instructors’ beauty and teaching quality for several courses at the University of Texas. The teaching evaluations were conducted at the end of the semester, and the beauty judgments were made later, by six students who had not attended the classes and were not aware of the course evaluations. The variables are:\n\nprof_id: Professor ID number\navg_eval: Average course rating\nnum_courses: Number of courses for which the professor has evaluations\nnum_students: Number of students enrolled in the professor’s courses\nperc_evaluating: Average percentage of enrolled students who completed an evaluation\nbeauty: Measure of the professor’s beauty composed of the average score on six standardized beauty ratings\ntenured: Is the professor tenured? (0 = non-tenured; 1 = tenured)\nnative_english: Is the professor a native English speaker? (0 = non-native English speaker; 1 = native English speaker)\nage: Professor’s age (in years)\nfemale: Is the professor female? (0 = not female; 1 = female)\n\n\n\nPreview\n\n# Import Data\nevaluations = readr::read_csv(file = \"https://raw.githubusercontent.com/zief0002/rustic-penguin/main/data/evaluations.csv\")\n\n# View data\nevaluations\n\n# A tibble: 94 × 10\n   prof_id avg_eval num_courses num_students perc_evaluating  beauty tenured\n     &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;           &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1       1     4              4          416            62.0  0.202        0\n 2       2     3.53           3          104            87.0 -0.826        1\n 3       3     3.45           2          250            78.5 -0.660        1\n 4       4     4.01           8          223            84.3 -0.766        1\n 5       5     4.35           6          331            81.8  1.42         0\n 6       6     4.44           7         1849            59.8  0.500        1\n 7       7     3.84           5          191            81.8 -0.214        0\n 8       8     4.03           7          174            84.6 -0.347        1\n 9       9     4.26           7          303            73.0  0.0613       0\n10      10     4.56          10          191            82.1  0.453        0\n# ℹ 84 more rows\n# ℹ 3 more variables: native_english &lt;dbl&gt;, age &lt;dbl&gt;, female &lt;dbl&gt;\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\nGelman, A., & Hill, J. (2007). Data analysis using regression and multilevel/hierarchical models. New York: Cambridge University Press.\n\n\nHamermesh, D. S., & Parker, A. M. (2005). Beauty in the classroom: Instructors’ pulchritude and putative pedagogical productivity. Economics of Education Review, 24, 369–376."
  },
  {
    "objectID": "codebooks/gapminder.html",
    "href": "codebooks/gapminder.html",
    "title": "gapminder.csv",
    "section": "",
    "text": "The data in gapminder.csv contains data on 6 attributes for 193 countries. These data, based on 2017 measures collected by World Bank, were compiled by gapminder.org and made available free under the CC-BY license. The variables are:\n\ncountry: The name of the country\nregion: The world region (Africa, Americas, Asia, or Europe)\nincome: Income per person based on the gross domestic product per person, adjusted for differences in purchasing power (in international dollars, fixed for 2017 prices)\nlife_exp: Average number of years a newborn child would live if current mortality patterns were to stay the same\nco2: Carbon dioxide emissions per person from the burning of fossil fuels (metric tonnes of CO2 per person)\nco2_change: Indicator of whether the carbon dioxide emissions per person from the burning of fossil fuels has increased or decreased since 2007 (10 year span)\npopulation: Population of the country, in millions\n\n\n\nPreview\n\n# Import Data\ngapminder = readr::read_csv(file = \"https://raw.githubusercontent.com/zief0002/rustic-penguin/main/data/gapminder.csv\")\n\n# View data\ngapminder\n\n# A tibble: 193 × 8\n   country      region income income_level life_exp    co2 co2_change population\n   &lt;chr&gt;        &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;\n 1 Afghanistan  Asia     2.03 Level 1          62.7  0.254 increase      37.2   \n 2 Albania      Europe  13.3  Level 3          78.4  1.59  increase       2.88  \n 3 Algeria      Africa  11.6  Level 3          76    3.69  increase      42.2   \n 4 Andorra      Europe  58.3  Level 4          82.1  6.12  decrease       0.077 \n 5 Angola       Africa   6.93 Level 2          64.6  1.12  decrease      30.8   \n 6 Antigua and… Ameri…  21    Level 3          76.2  5.88  increase       0.0963\n 7 Argentina    Ameri…  22.7  Level 3          76.5  4.41  decrease      44.4   \n 8 Armenia      Europe  12.7  Level 3          75.6  1.89  decrease       2.95  \n 9 Australia    Asia    49    Level 4          82.9 16.9   decrease      24.9   \n10 Austria      Europe  55.3  Level 4          82.1  7.75  decrease       8.89  \n# ℹ 183 more rows\n\n\n\n\n\nReferences\nFREE DATA FROM WORLD BANK VIA GAPMINDER.ORG, CC-BY LICENSE"
  },
  {
    "objectID": "codebooks/keith-gpa.html",
    "href": "codebooks/keith-gpa.html",
    "title": "keith-gpa.csv",
    "section": "",
    "text": "This data, stored in keith-gpa.csv includes three attributes on \\(n = 100\\) 8th-grade students. These data come from Keith (2015). The attributes are:\n\ngpa: Overall Grade-point average (GPA) in all subjects (on a standard 100-point scale)\nhomework: Average time spent on homework per week across all subjects (in hours)\nparent_ed: Education-level (in years of schooling) for the parent with the highest level of education\n\n\n\nPreview\n\n# Import Data\nkeith = readr::read_csv(file = \"https://raw.githubusercontent.com/zief0002/rustic-penguin/main/data/keith-gpa.csv\")\n\n# View data\nkeith\n\n# A tibble: 100 × 3\n     gpa homework parent_ed\n   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1    78        2        13\n 2    79        6        14\n 3    79        1        13\n 4    89        5        13\n 5    82        3        16\n 6    77        4        13\n 7    88        5        13\n 8    70        3        13\n 9    86        5        15\n10    80        5        14\n# ℹ 90 more rows\n\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\nKeith, T. V. (2015). Multiple regression and beyond: An introduction to multiple regression and structural equation modeling (2nd ed.). New York: Routledge."
  },
  {
    "objectID": "codebooks/scoobydoo.html",
    "href": "codebooks/scoobydoo.html",
    "title": "scoobydoo.csv",
    "section": "",
    "text": "Scooby-Doo is a popular television and movie franchise in the United States which has been airing for over 50 years! Scoobypedia (an encyclopedia of the franchise) describes the show as follows:\n\nThe show follows the iconic mystery solving detectives, know as Mystery Inc., as they set out to solve crime and unmask criminals, bent on revenge or committing criminal acts for their own personal gain.\n\nMystery Inc. are an amateur crime-solving group of friends composed of Fred Jones (the team’s leader), Daphne Blake (fashionista and musician), Velma Dinkley (bespectacled resident genius), Shaggy Rogers (owner and best friend to Scooby-Doo), and Scooby-Doo (the talking dog mascot).\nThe data in scoobydoo.csv are a subset of data collected by plummye who “[t]ook ~1 year to watch every Scooby-Doo iteration and track every variable.” There are 372 cases in the data.\nThe variables are:\n\ntitle: Title of the episode or movie\nseries_name: Name of the series in which the episode takes place. For movies this is the grouping classification from Scoobypedia.\ndate_aired: Dated aired in United States.\nengagement: Engagement measure based on the log-number of reviews on IMDb. Higher values indicate more engagement.\ncaught_by: Which Mystery Inc. members caught the villain\n\nFred/Daphne/Velma: Villain caught by Fred Jones, Daphne Blake, Velma Dinkley, or some combination of these three Mystery Inc. members;\nShaggy/Scooby: Villain caught by either Shaggy Rogers, Scooby-Doo, or both;\nCombo: Villain caught by a combination of the five Mystery Inc. members;\nOther: Villain caught by non-Mystery Inc. member.\n\nimdb_rating: Weighted average of all individual IMDb ratings (from 1 to 10)\nformat: Type of media (TV or Movie)\ncatchphrase: Number of times any catchphrase (e.g., zoinks, rooby rooby roo) was uttered in the episode or movie\n\n\n\nPreview\n\n# Import Data\nscoobydoo = readr::read_csv(file = \"https://raw.githubusercontent.com/zief0002/rustic-penguin/main/data/scoobydoo.csv\")\n\n# View data\nscoobydoo\n\n# A tibble: 372 × 8\n   title          series_name date_aired engagement caught_by imdb_rating format\n   &lt;chr&gt;          &lt;chr&gt;       &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt; \n 1 What a Night … Scooby Doo… 9/13/69          6.32 Shaggy/S…         8.1 TV    \n 2 A Clue for Sc… Scooby Doo… 9/20/69          6.17 Shaggy/S…         8.1 TV    \n 3 Hassle in the… Scooby Doo… 9/27/69          6.12 Shaggy/S…         8   TV    \n 4 Mine Your Own… Scooby Doo… 10/4/69          6.05 Fred/Dap…         7.8 TV    \n 5 Decoy for a D… Scooby Doo… 10/11/69         5.97 Shaggy/S…         7.5 TV    \n 6 What the Hex … Scooby Doo… 10/18/69         5.95 Fred/Dap…         8.4 TV    \n 7 Never Ape an … Scooby Doo… 10/25/69         5.88 Fred/Dap…         7.6 TV    \n 8 The Backstage… Scooby Doo… 11/8/69          5.92 Shaggy/S…         8.1 TV    \n 9 Bedlam in the… Scooby Doo… 11/15/69         5.85 Shaggy/S…         8   TV    \n10 A Gaggle of G… Scooby Doo… 11/22/69         5.94 Shaggy/S…         8.5 TV    \n# ℹ 362 more rows\n# ℹ 1 more variable: catchphrase &lt;dbl&gt;"
  },
  {
    "objectID": "codebooks/spice-girls.html",
    "href": "codebooks/spice-girls.html",
    "title": "spice-girls.csv",
    "section": "",
    "text": "The data in spice-girls.csv contain five attributes about the Spice Girls. The attributes include:\n\nspice_name: Nickname of the Spice Girl\nage: Age the Spice Girl joined the band\noriginal_member: Spice Girl was an original member (TRUE; FALSE)\nsolo_nominations: Number of award nominations as a solo artist\nreal_name: Real name of the Spice Girl\n\n\n\nPreview\n\n# Import Data\nspice = readr::read_csv(file = \"https://raw.githubusercontent.com/zief0002/rustic-penguin/main/data/spice_girls.csv\")\n\n# View Data\nspice\n\n# A tibble: 5 × 5\n  spice_name   age original_member solo_nominations real_name\n  &lt;chr&gt;      &lt;dbl&gt; &lt;lgl&gt;                      &lt;dbl&gt; &lt;chr&gt;    \n1 Scary         19 TRUE                           4 Mel B    \n2 Sporty        20 TRUE                          26 Mel C    \n3 Baby          18 FALSE                         14 Emma     \n4 Ginger        22 TRUE                          13 Geri     \n5 Posh          20 FALSE                         12 Victoria"
  },
  {
    "objectID": "codebooks/state-education.html",
    "href": "codebooks/state-education.html",
    "title": "state-education.csv",
    "section": "",
    "text": "This data, stored in state-education.csv includes state-level aggregate data on six attributes. The attributes, collected for all 50 states and the District of Columbia, are:\n\nstate: State name\npostal: State postal code\nregion: Region of the country identified by the National Education Association (Far West, Great Lakes, Mid East, New England, Plains, Rocky Mountains, Southeast, Southwest)\nsat_participate: Percentage of students in the state who took the SAT.\nsat_ebrw: Average score on the Evidence-Based Reading and Writing (EBRW) scale in the state.\nsat_math: Average score on the math scale in the state.\nsat_total: Average total SAT score in the state.\nsalary_2020_21: Average 2020–2021 public teacher salary in the state.\n\nNote: All of the SAT data is based on students in the class of 2020 who took the current SAT during high school. The SAT is made up of three sections: (1) Reading, (2) Writing and Language (also just called Writing), and (3) Math. The Math section is scored on a scale of 200–800. The Reading and Writing sections are combined to give a Evidence-Based Reading and Writing (EBRW) score, also ranging from 200–800. By combining the Math and EBRW scores, we get a total SAT score ranging from 400–1600.\n\n\nPreview\n\n# Import Data\nstate_educ = readr::read_csv(file = \"https://raw.githubusercontent.com/zief0002/rustic-penguin/main/data/state-education.csv\")\n\n# View data\nstate_educ\n\n# A tibble: 51 × 8\n   state               postal region sat_participate sat_ebrw sat_math sat_total\n   &lt;chr&gt;               &lt;chr&gt;  &lt;chr&gt;            &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1 Alabama             AL     South…               7      576      551      1127\n 2 Alaska              AK     Far W…              37      555      543      1098\n 3 Arizona             AZ     South…              29      571      568      1139\n 4 Arkansas            AR     South…               4      590      567      1157\n 5 California          CA     Far W…              67      527      522      1049\n 6 Colorado            CO     Rocky…             100      511      501      1012\n 7 Connecticut         CT     New E…             100      527      512      1039\n 8 Delaware            DE     Mid E…             100      497      481       978\n 9 District of Columb… DC     Mid E…             100      498      482       979\n10 Florida             FL     South…             100      512      479       992\n# ℹ 41 more rows\n# ℹ 1 more variable: salary_2020_21 &lt;dbl&gt;\n\n\n\n\n\nReferences\nCollege Board. (2020). SAT suite of assessments annual report. Author.\nNEA Research. (2021). Rankings of the states 2020 and estimates of school statistics 2021. National Education Association."
  },
  {
    "objectID": "codebooks/substance-family.html",
    "href": "codebooks/substance-family.html",
    "title": "substance-family.csv",
    "section": "",
    "text": "This data, stored in substance-family.csv includes four attributes on \\(n = 910\\) 10th-grade students. These data come from Keith (2015). The attributes are:\n\nsubstance_use: Composite based on student-reported use of cigarettes (How many cigarettes smoked per day), alcohol (In lifetime, number of times had alcohol to drink), and marijuana (In lifetime, number of times used marijuana). To compute this composite index, the three self-reported values were standardized and then averaged.\nfamily_structure: Adult composition of the household with three levels (Two-parent family, One-parent, one guardian, andSingle-parent family)\nfemale: Dummy-coded sex variable (0 = Not female; 1 = Female)\ngpa: Composite GPA on a 10-pt scale\n\n\n\nPreview\n\n# Import Data\nfamily = readr::read_csv(file = \"https://raw.githubusercontent.com/zief0002/rustic-penguin/main/data/substance-family.csv\")\n\n# View data\nfamily\n\n# A tibble: 910 × 4\n   substance_use family_structure     female   gpa\n           &lt;dbl&gt; &lt;chr&gt;                 &lt;dbl&gt; &lt;dbl&gt;\n 1       -0.129  Two-parent family         1   3.8\n 2        0.0143 Two-parent family         0   2.5\n 3       -0.594  Two-parent family         1   2.8\n 4       -0.439  Single-parent family      0   3.5\n 5       -0.284  Two-parent family         1   3.3\n 6       -0.284  Two-parent family         0   2.5\n 7       -0.594  Two-parent family         1   2.3\n 8       -0.284  Two-parent family         1   2.5\n 9        3.21   Two-parent family         0   3  \n10       -0.594  Two-parent family         0   3  \n# ℹ 900 more rows\n\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\nKeith, T. V. (2015). Multiple regression and beyond: An introduction to multiple regression and structural equation modeling (2nd ed.). New York: Routledge."
  },
  {
    "objectID": "codebooks/work-demands.html",
    "href": "codebooks/work-demands.html",
    "title": "work-demands.csv",
    "section": "",
    "text": "Research has documented the importance of a healthy work-life balance. The data in work-demands.csv were simulated to mimic the effects of boundary-spanning work (receiving work-related contact outside of normal work hours) found in the literature on psychological outcomes. The variables in the data are:\n\nguilt: Standardized measure of guilt (as a transitory affective state) related to home-life balance. This was measured using survey questions such as, “In the past seven days, on how many days have you felt guilty?” Higher values indicate more perceived guilt.\nbound_span_work: Standardized measure of boundary-spanning work demands. Measured using survey items such as, “how often do coworkers, supervisors, managers, customers, or clients contact you about work-related matters outside normal work hours?”. Higher values indicate higher degrees of boundary-spanning work demands.\nfemale: Dummy-coded indicator of sex (0 = Not female; 1 = Female)\nauthority: Standardized measure of job authority. Measured using survey items such as: “Do you influence or set the rate of pay received by others?” and “Do you have the authority to hire or fire others?”. Higher values indicate more authority.\nmarried: Dummy-coded indicator of marital status (0 = Not married; 1 = Married)\n\n\n\nPreview\n\n# Import Data\nwork = readr::read_csv(file = \"https://raw.githubusercontent.com/zief0002/rustic-penguin/main/data/work-demands.csv\")\n\n# View data\nwork\n\n# A tibble: 300 × 5\n     guilt bound_span_work female authority married\n     &lt;dbl&gt;           &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n 1 -1.64           -1.13        1     2.65        1\n 2 -0.0272          0.718       1     1.38        1\n 3  0.276          -0.0164      1     1.14        1\n 4  1.65            1.29        1     1.13        1\n 5 -0.149          -0.153       1     0.646       1\n 6 -1.47           -2.04        1     1.23        1\n 7 -2.09           -1.18        1     3.03        1\n 8  1.27            0.204       1     0.171       1\n 9  0.140           0.0152      1     1.23        1\n10  0.423          -1.26        1    -0.908       1\n# ℹ 290 more rows\n\n\n\n\n\nReferences\nGlavin, P., Schieman, S., & Reid, S. (2011). Boundary-spanning work demands and their consequences for guilt and psychological distress. Journal of Health and Social Behavior, 52(1) 43–57. doi: 10.1177/0022146510395023"
  },
  {
    "objectID": "mission-statements.html",
    "href": "mission-statements.html",
    "title": "Mission Statements",
    "section": "",
    "text": "Quantitative Methods in Education Mission Statement\nQME strives to be a premier program recognized for leadership, innovation, and excellence, and to enable human potential through the advancement of education. QME prepares students to become cutting-edge professionals in educational measurement, evaluation, statistics, and statistics education, through excellence in teaching, research, and service; and through investigating and developing research methodology in education.\n\n\n\nDepartment of Educational Psychology Mission Statement\nEducational psychology involves the study of cognitive, emotional, and social learning processes that underlie education and human development across the lifespan. Research in educational psychology advances scientific knowledge of those processes and their application in diverse educational and community settings. The department provides training in the psychological foundations of education, research methods, and the practice and science of counseling psychology, school psychology, and special education. Faculty and students provide leadership and consultation to the state, the nation, and the international community in each area of educational psychology. The department’s scholarship and teaching enhance professional practice in schools and universities, community mental health agencies, business and industrial organizations, early childhood programs, and government agencies. Adopted by the Department. of Educational Psychology faculty October 27, 2004\n\n\n\nCollege of Education + Human Development Mission Statement\nThe mission of the University of Minnesota College of Education and Human Development is to contribute to a just and sustainable future through engagement with the local and global communities to enhance human learning and development at all stages of the life span."
  },
  {
    "objectID": "readings/01-welcome-to-8251.html",
    "href": "readings/01-welcome-to-8251.html",
    "title": "📖 Welcome to EPsy 8251",
    "section": "",
    "text": "Here are some things to do before the first class:\nRequired\n\nRead through and familiarize yourself with the course website. This website acts as the syllabus for the course.\nRead Preface for Statistical Modeling and Computation for Educational Scientists [Computational Toolkit]"
  },
  {
    "objectID": "readings/02-introduction-to-r-and-rstudio.html",
    "href": "readings/02-introduction-to-r-and-rstudio.html",
    "title": "📖 Introduction to R and RStudio",
    "section": "",
    "text": "Required\n\nDownload and install the most recent version of R (version 4.3.1; Beagle Scouts) on your computer.\nDownload and install the most recent version of and RStudio Desktop (version 2023.06.0+421) on your computer.\n\nSee R and RStudio Installation and Setup for help.\nRead the following:\n\nGetting Started with R [Computational Toolkit]\nData Structures in R [Computational Toolkit]\n\nAdditional Resources\n\nRStudio Cheatsheets\nRStudio Keyboard Shortcuts\nR Cookbook [book; UMN Library]\ntidy data video [video]\nTeacups, Giraffes, and Statistics: Learn statistics & R coding through a series of modules that teach in an intuitive, playful, and approachable way"
  },
  {
    "objectID": "readings/03-data-wrangling-with-dplyr.html",
    "href": "readings/03-data-wrangling-with-dplyr.html",
    "title": "📖 Data Wrangling with dplyr",
    "section": "",
    "text": "Required\n\nData Wrangling with dplyr [Computational Toolkit]\n\nAdditional Resources\n\nData Transformation with dplyr Cheatsheet\nData Wrangling Part 1: Basic to Advanced Ways to Select Columns\nData Wrangling Part 2: Transforming your columns into the right shape\nData Wrangling Part 3: Basic and more advanced ways to filter rows\nData Wrangling Part 4: Summarizing and slicing your data\nCreate tidyverse inspired clothing. See how Amelia McNamara created a tidyverse dress and make your own clothing, shower curtain, etc. with the Spoonflower printed fabric she made available."
  },
  {
    "objectID": "readings/04-plotting-with-ggplot.html",
    "href": "readings/04-plotting-with-ggplot.html",
    "title": "Plotting with ggplot2",
    "section": "",
    "text": "Required\n\nVisualizing Data with ggplot2 [Computational Toolkit]\n\nAdditional Resources\n\nData Visualization with ggplot2 Cheatsheet\nCookbook for R: Graphs\nPlotting with ggplot: Part 1 [video]\nPlotting with ggplot: Part 2 [video]"
  },
  {
    "objectID": "readings/04-plotting-with-ggplot2.html",
    "href": "readings/04-plotting-with-ggplot2.html",
    "title": "📖 Plotting with ggplot2",
    "section": "",
    "text": "Required\n\nVisualizing Data with ggplot2 [Computational Toolkit]\n\nAdditional Resources\n\nData Visualization with ggplot2 Cheatsheet\nCookbook for R: Graphs\nPlotting with ggplot: Part 1 [video]\nPlotting with ggplot: Part 2 [video]"
  },
  {
    "objectID": "readings/05-simple-linear-regression-description.html",
    "href": "readings/05-simple-linear-regression-description.html",
    "title": "📖 Simple Linear Regression: Description",
    "section": "",
    "text": "Required\n\nLewis-Beck & Lewis-Beck [Chap. 1]\nClayton, A. (2020, Oct. 28). How eugenics shaped statistics: Exposing the damned lies of three science pioneers. Nautilus, 92 (Frontiers)."
  },
  {
    "objectID": "readings/05-simple-linear-regression-description.html#probability-density",
    "href": "readings/05-simple-linear-regression-description.html#probability-density",
    "title": "Introduction to Probability Distributions",
    "section": "Probability Density",
    "text": "Probability Density\nIn a continuous distribution we also need to account be able to talk about the fact that some outcomes are more likely than other outcomes. For example, in our standard normal distribution outcomes near zero are more probable than outcomes near 1, which are more probable than outcomes near 2, etc. Since we can’t use probability to do this (remember the probability of each outcome is the same, namely 0), we use something called probability density. This is akin to a relative probability, so outcomes with a higher probability density are more likely than outcomes with a lower probability density.\nThe mapping of all the outcomes to their probability densities is called a probability density function (PDF). Thus the equation or “bell-shaped” curve describing the standard normal distribution in Figure 1 is technically a PDF Here are some laws governing PDFs:\n\nProbability densities are always positive.\nThe probability of an outcome x between a and b equals the integral (area under the curve) between a and b of the probability density function. That is:\n\n\\[\np(a \\leq x \\leq b) = \\int_a^b p(x) dx\n\\]\n\nThe area under the curve from negative infinity to positive infinity is 1. That is:\n\n\\[\np(-\\infty \\leq x \\leq +\\infty) = \\int_{-\\infty}^{+\\infty} p(x) = 1\n\\]\nNext we will look at the PDF for a normal distribution."
  },
  {
    "objectID": "readings/05-simple-linear-regression-description.html#other-useful-r-functions-for-working-with-normal-probability-distributions",
    "href": "readings/05-simple-linear-regression-description.html#other-useful-r-functions-for-working-with-normal-probability-distributions",
    "title": "Introduction to Probability Distributions",
    "section": "Other Useful R Functions for Working with Normal Probability Distributions",
    "text": "Other Useful R Functions for Working with Normal Probability Distributions\nWe use dnorm() when we want to compute the probability density associated with a particular x-value in a given normal distribution. There are three other functions that are quite useful for working with the normal probability distribution:\n\npnorm() : To compute the probability (area under the PDF)\nqnorm() : To compute the \\(x\\) value given a particular probability\nrnorm() : To draw a random observation from the distribution\n\nEach of these function also requires the arguments mean= and sd=. Below we will examine how to use each of these additional functions."
  },
  {
    "objectID": "readings/05-simple-linear-regression-description.html#pnorm-computing-cumulative-probability-density",
    "href": "readings/05-simple-linear-regression-description.html#pnorm-computing-cumulative-probability-density",
    "title": "Introduction to Probability Distributions",
    "section": "pnorm(): Computing Cumulative Probability Density",
    "text": "pnorm(): Computing Cumulative Probability Density\nThe function pnorm() computes the area under the PDF curve from \\(-\\infty\\) to some x-value. (Sometimes this is referred to as the cumulative probability density of x.) It is important to note that the PDF is defined such that the entire area under the curve is equal to 1. Because of this, we can also think about using area under the curve as an analog to probability in a continuous distribution.\nFor example, we might ask about the probability of observing an x-value that is less than or equal to 65 given it is from a \\(\\mathcal{N}(50,10)\\) distribution. Symbolically, we want to find:\n\\[\nP\\bigg(x \\leq 65 \\mid \\mathcal{N}(50,10)\\bigg)\n\\]\nThis is akin to finding the proportion of the area under the \\(\\mathcal{N}(50,10)\\) PDF that is to the left of 65. The figure below shows a graphical depiction of the cumulative probability density for \\(x=65\\).\n\n\nCode\n# Create dataset\nfig_03 = data.frame(\n  X = seq(from = 10, to = 90, by = 0.01)\n  ) %&gt;% \n  mutate(\n    Y = dnorm(x = X, mean = 50, sd = 10)\n    )\n\n# Filter out X&lt;=65\nshaded = fig_03 %&gt;%\n  filter(X &lt;= 65)\n\n# Create plot\nggplot(data = fig_03, aes(x = X, y = Y)) +\n  geom_ribbon(data = shaded, ymin = -10, aes(ymax = Y), color = \"#bbbbbb\", alpha = 0.4) +\n  geom_line() +\n  xlab(\"X\") +\n  ylab(\"Probability density\") +\n  theme_light()\n\n\n\n\n\nFigure 3: Plot of the probability density function (PDF) for a \\(\\mathcal{N}(50,10)\\) distribution. The area that is shaded grey (relative to the total area under the PDF) represents the cumulative probability density for \\(x=65\\).\n\n\n\n\nWe can compute the cumulative probability density using the pnorm() function. The “p” stand for “probability”.\n\n# Find P(x&lt;=65 | N(50,10) )\npnorm(q = 65, mean = 50, sd = 10)\n\n[1] 0.9331928\n\n\nWe can interpret this as:\n\nThe probability of observing an x-value that is less than or equal to 65 (if it is drawn from a normal distribution with a mean of 50 and standard deviation of 10) is 0.933.\n\nIn mathematics, the area under a curve is called an integral. The grey-shaded area in the previous figure can also be expressed as an integral of the probability density function:\n\\[\n\\int_{-\\infty}^{65} p(x) dx\n\\]\nwhere \\(p(x)\\) is the PDF for the normal distribution.\nThe most common application for finding the cumulative density is to compute a p-value. The p-value is just the area under the distribution (curve) that is AT LEAST as extreme as some observed value. For example, assume we computed a test statistic of \\(z=2.5\\), and were evaluating whether this was different from 0 (two-tailed test). Graphically, we want to determine the proportion of the area under the PDF that is shaded grey in the figure below.\n\n\nCode\n# Create data\nfig_04 = data.frame(\n  X = seq(from = -4, to = 4, by = 0.01)\n  ) %&gt;% \n  mutate(\n    Y = dnorm(x = X, mean = 0, sd = 1)\n    )\n\n# Filter data for shading\nshade_01 = fig_04 %&gt;%\n  filter(X &gt;= 2.5)\n\nshade_02 = fig_04 %&gt;%\n  filter(X &lt;= -2.5)\n\n# Create plot\nggplot(data = fig_04, aes(x = X, y = Y)) +\n  geom_ribbon(data = shade_01, ymin = -10, aes(ymax = Y), color = \"#bbbbbb\", alpha = 0.4) +\n  geom_ribbon(data = shade_02, ymin = -10, aes(ymax = Y), color = \"#bbbbbb\", alpha = 0.4) +\n  geom_line() +\n  xlab(\"z\") +\n  ylab(\"Probability density\") +\n  theme_light()\n\n\n\n\n\nFigure 4: Plot of the probability density function (PDF) for the standard normal distribution (\\(M=0\\), \\(SD=1\\)). The cumulative density representing the p-value for a two-tailed test evaluating whether \\(\\mu=0\\) using an observed mean of 2.5 is also displayed.\n\n\n\n\nIf the distribution of the test statistic is normally distributed, we can use pnorm() to compute the p-value. If we assume the test statistic, z, has been scaled to use standardized units, the standard deviation we use in pnorm() will be sd=1. The mean is based on the value being tested in the null hypothesis. In most null hypotheses, we are testing a difference from 0 (e.g., \\(H_0: \\mu=0\\), \\(H_0: \\beta=0\\)), so we would use mean=0 in the pnorm() function.\nRemember, pnorm() computes the proportion of the area under the curve TO THE LEFT of a particular value. Here we will compute the area to the left of \\(-2.5\\) and then double it to produce the actual p-value. (We can double it because the normal distribution is symmetric so the area to the left of \\(-2.5\\) is the same as the area to the right of \\(+2.5\\).)\n\n# Compute the p-value based on z=2.5\n2 * pnorm(q = -2.5, mean = 0, sd = 1)\n\n[1] 0.01241933\n\n\nWe interpret this p-value as:\n\nThe probability of observing a statistic at least as extreme as 2.5, assuming the null hypothesis is true, is 0.012. This is evidence against the null hypothesis since the data are inconsistent with the assumed hypothesis.\n\n\n\nqnorm(): Computing Quantiles\nThe qnorm() function is essentially the inverse of the pnorm() function. The pnorm() function computes the cumulative probability GIVEN a particular quantile (x-value). The qnorm() function computes the quantile GIVEN a cumulative probability. For example, in the \\(\\mathcal{N}(50, 10)\\) distribution, half of the area under the PDF is below the x-value (quantile) of 50.\nTo use the qnorm() function to give the x-value (quantile) that defines the lower 0.5 of the area under the \\(\\mathcal{N}(50, 10)\\) PDF, the syntax would be:\n\n# Find the quantile that has a cumulative density of 0.5 in the N(50, 10) distribution\nqnorm(p = 0.5, mean = 50, sd = 10)\n\n[1] 50\n\n\n\n\n\nrnorm(): Generating Random Observations\nThe rnorm() function can be used to generate random observations drawn from a specified normal distribution. Aside from the mean= and sd= arguments, we also need to specify the number of observations to generate by including the argument n=. For example, to generate 15 observations drawn from a \\(\\mathcal{N}(50,10)\\) distribution we would use the following syntax:\n\n# Generate 15 observations from N(50,10)\nset.seed(100)\nrnorm(n = 15, mean = 50, sd = 10)\n\n [1] 44.97808 51.31531 49.21083 58.86785 51.16971 53.18630 44.18209 57.14533\n [9] 41.74741 46.40138 50.89886 50.96274 47.98366 57.39840 51.23380\n\n\nThe set.seed() function sets the state of the random number generator used in R so that the results are reproducible. If you don’t use set.seed() you will get a different set of observations each time you run rnorm(). Here we set the starting seed to 100, but you can set this to any integer you want."
  },
  {
    "objectID": "readings/05-simple-linear-regression-description.html#computing-f-from-the-anova-partitioning",
    "href": "readings/05-simple-linear-regression-description.html#computing-f-from-the-anova-partitioning",
    "title": "Introduction to Probability Distributions",
    "section": "Computing F from the ANOVA Partitioning",
    "text": "Computing F from the ANOVA Partitioning\nWe can also compute the model-level F-statistic directly using the partitioning of variation from the ANOVA table.\n\n# Partition the variation\nanova(lm.1)\n\n\n\n  \n\n\n\nThe F-statistic is a ratio of the mean square for the model and the mean square for the error. To compute a mean square we use the general formula:\n\\[\n\\mathrm{MS} = \\frac{\\mathrm{SS}}{\\mathrm{df}}\n\\]\nThe model includes both the education and seniority predictor, so we combine the SS and df. The MS model is:\n\\[\n\\begin{split}\n\\mathrm{MS}_{\\mathrm{Model}} &= \\frac{\\mathrm{SS}_{\\mathrm{Model}}}{\\mathrm{df}_{\\mathrm{Model}}} \\\\[1ex]\n&= \\frac{4147.3 + 722.9}{1 + 1} \\\\[1ex]\n&= \\frac{4870.2}{2} \\\\[1ex]\n&= 2435.1\n\\end{split}\n\\]\nThe MS error is:\n\\[\n\\begin{split}\n\\mathrm{MS}_{\\mathrm{Error}} &= \\frac{\\mathrm{SS}_{\\mathrm{Error}}}{\\mathrm{df}_{\\mathrm{Error}}} \\\\[1ex]\n&= \\frac{1695.3 }{29} \\\\[1ex]\n&= 58.5\n\\end{split}\n\\]\nThen, we compute the F-statistic by computing the ratio of these two mean squares.\n\\[\n\\begin{split}\nF &= \\frac{\\mathrm{MS}_{\\mathrm{Model}}}{\\mathrm{MS}_{\\mathrm{Error}}} \\\\[1ex]\n&= \\frac{2435.1}{58.5} \\\\[1ex]\n&= 41.6\n\\end{split}\n\\]\nSince a mean square represents the average amount of variation (per degree of freedom), we can see that F is a ratio between the average amount of variation explained by the model and the average amount of variation unexplained by the model. In our example, this ratio is 41.6; on average the model explains 41.6 times the variation that is unexplained.\nNote that this is an identical computation (although reframed) as the initial computation for F. We can use mathematics to show this equivalence:\n\\[\n\\begin{split}\nF &= \\frac{R^2}{1-R^2} \\times \\frac{\\mathrm{df}_{\\mathrm{Error}}}{\\mathrm{df}_{\\mathrm{Model}}} \\\\[1ex]\n&= \\frac{\\frac{\\mathrm{SS}_{\\mathrm{Model}}}{\\mathrm{SS}_{\\mathrm{Total}}}}{\\frac{\\mathrm{SS}_{\\mathrm{Error}}}{\\mathrm{SS}_{\\mathrm{Total}}}} \\times \\frac{\\mathrm{df}_{\\mathrm{Error}}}{\\mathrm{df}_{\\mathrm{Model}}} \\\\[1ex]\n&= \\frac{\\mathrm{SS}_{\\mathrm{Model}}}{\\mathrm{SS}_{\\mathrm{Error}}} \\times \\frac{\\mathrm{df}_{\\mathrm{Error}}}{\\mathrm{df}_{\\mathrm{Model}}} \\\\[1ex]\n&= \\frac{\\mathrm{SS}_{\\mathrm{Model}}}{\\mathrm{df}_{\\mathrm{Model}}} \\times \\frac{\\mathrm{df}_{\\mathrm{Error}}}{\\mathrm{SS}_{\\mathrm{Error}}} \\\\[1ex]\n&= \\mathrm{MS}_{\\mathrm{Model}} \\times \\frac{1}{\\mathrm{MS}_{\\mathrm{Error}}}\\\\[1ex]\n&= \\frac{\\mathrm{MS}_{\\mathrm{Model}}}{\\mathrm{MS}_{\\mathrm{Error}}}\n\\end{split}\n\\]"
  },
  {
    "objectID": "readings/05-simple-linear-regression-description.html#testing-the-model-level-null-hypothesis",
    "href": "readings/05-simple-linear-regression-description.html#testing-the-model-level-null-hypothesis",
    "title": "Introduction to Probability Distributions",
    "section": "Testing the Model-Level Null Hypothesis",
    "text": "Testing the Model-Level Null Hypothesis\nWe evaluate our test statistic (F in this case) in the appropriate test distribution, in this case an F-distribution with 2 and 29 degrees of freedom. The figure below, shows the \\(F(2,29)\\)-distribution as a solid, black line. The p-value is the area under the curve that is at least as extreme as the observed F-value of 41.7.\n\n\nCode\n# Create data\nfig_11 = data.frame(\n  X = seq(from = 0, to = 50, by = 0.01)\n  ) %&gt;%\n  mutate(\n    Y = df(x = X, df1 = 2, df2 = 29)\n    )\n\n# Filter shaded area\nshade = fig_11 %&gt;%\n  filter(X &gt;= 41.7)\n\n# Create plot\nggplot(data = fig_11, aes(x = X, y = Y)) +\n  geom_line() +\n  theme_bw() +\n  geom_ribbon(data = shade, ymin = -10, aes(ymax = Y), color = \"#bbbbbb\", alpha = 0.4)\n\n\n\n\n\nFigure 11: Plot of the probability density function (PDF) for the \\(F(2,~29)\\)-distribution. The cumulative density representing the p-value for a test evaluating whether \\(\\rho^2=0\\) using an observed F-statistic of 41.7 is also displayed.\n\n\n\n\nThe computation using the cumulative density function, pf(), to obtain the p-value is:\n\n# p-value for F(2,29)=41.7\n1 - pf(41.7, df1 = 2, df2 = 29)\n\n[1] 0.000000002942114\n\n\nBecause we want the upper-tail, rather than taking the difference from 1, we can also use the lower.tail=FALSE argument in pf().\n\n# p-value for F(2,29)=41.7\npf(41.7, df1 = 2, df2 = 29, lower.tail = FALSE)\n\n[1] 0.000000002942114"
  },
  {
    "objectID": "readings/05-simple-linear-regression-description.html#mean-squares-are-variance-estimates",
    "href": "readings/05-simple-linear-regression-description.html#mean-squares-are-variance-estimates",
    "title": "Introduction to Probability Distributions",
    "section": "Mean Squares are Variance Estimates",
    "text": "Mean Squares are Variance Estimates\nMean squares are also estimates of the variance. Consider the computational formula for the sample variance,\n\\[\n\\hat{\\sigma}^2 = \\frac{\\sum(Y - \\bar{Y})^2}{n-1}\n\\]\nThis is the total sum of squares divided by the total df. The variance of the outcome variable is interpreted as the average amount of variation in the outcome variable (in the squared metric). Thus, it is also referred to as the mean square total.\nWhen we compute an F-statistic, we are finding the ratio of two different variance estimates—one based on the model (explained variance) and one based on the error (unexplained variance). Under the null hypothesis that \\(\\rho^2 = 0\\), we are assuming that all the variance is unexplained. In that case, our F-statistic would be close to zero. When the model explains a significant amount of variation, the numerator gets larger relative to the denominator and the F-value is larger.\nThe mean squared error (from the anova() output) plays a special role in regression analysis. It is the variance estimate for the conditional distributions of the residuals in our visual depiction of the distributional assumptions of the residuals underlying linear regression.\n\n\n\n\n\nFigure 12: Visual Depiction of the Distributional Assumptions of the Residuals Underlying Linear Regression\n\n\n\n\nRecall that we made implicit assumptions about the conditional distributions of the residuals, namely that they were identically and normally distributed with a mean of zero and some variance. Based on the estimate of the mean squared error, the variance of each of these distributions is 58.5.\nWhile the variance is a mathematical convenience, the standard deviation is often a better descriptor of the variation in a distribution since it is measured in the original metric. The standard deviation fro the residuals (error) is 7.6. Because the residuals are statistics (summaries computed from sample data), their standard deviation is referred to as a “standard error”.\n\nThe residual standard error (RSE) is sometimes referred to as the Root Mean Squared Error (RMSE).\n\n\n# Compute RMSE\nsqrt(58.5)\n\n[1] 7.648529\n\n\nWhy is this value important? It gives the expected variation in the conditional residual distributions, which is a measure of the average amount of error. For example, since all of the conditional distributions of the residuals are assumed to be normally distributed, we would expect that 95% of the residuals would fall between \\(\\pm2\\) standard errors from 0; or, in this case, between \\(-15.3\\) and \\(+15.3\\). Observations with residuals that are more extreme may be regression outliers.\nMore importantly, it is a value that we need to estimate in order to specify the model."
  },
  {
    "objectID": "readings/05-simple-linear-regression-description.html#confidencecompatibility-intervals-for-the-coefficients",
    "href": "readings/05-simple-linear-regression-description.html#confidencecompatibility-intervals-for-the-coefficients",
    "title": "Introduction to Probability Distributions",
    "section": "Confidence/Compatibility Intervals for the Coefficients",
    "text": "Confidence/Compatibility Intervals for the Coefficients\nThe confidence interval for the kth regression coefficient is computed as:\n\\[\n\\mathrm{CI} = \\hat\\beta_k \\pm t^{*}(\\mathrm{SE}_{\\hat\\beta_k})\n\\]\nwhere \\(t^*\\) is the quantile of the t-distribution that defines the confidence level for the interval. (This t-distribution, again, has degrees-of-freedom equal to the error df in the model.) The confidence level is related to the alpha level (type I error rate) used in inference. Namely,\n\\[\n\\mathrm{Confidence~Level} = 1 - \\alpha\n\\]\nSo, if you use \\(\\alpha=.05\\), then the confidence level would be \\(.95\\), and we would call this a 95% confidence interval. The alpha value also helps determine the quantile we use in the CI formula,\n\\[\nt^* = (1-\\frac{\\alpha}{2}) ~ \\mathrm{quantile}\n\\] For the example using \\(\\alpha=.05\\), a 95% confidence interval, the \\(t^*\\) value would be associated with the quantile of 0.975. We would denote this as:\n\\[\nt^{*}_{.975}\n\\]\nSay we wanted to find the 95% confidence interval for the education coefficient. We know that the estimated coefficient for education is 2.25, and the standard error for this estimate is 0.335. We also know that based on the model fitted, the residual df is 29. We need to find the 0.975th quantile in the t-distribution with 29 df.\n\n# Find 0.975th quantile\nqt(p = 0.975, df = 29)\n\n[1] 2.04523\n\n\nNow we can use all of this information to compute the confidence interval:\n\\[\n\\begin{split}\n95\\%~CI  &= 2.25 \\pm 2.04523(0.335) \\\\[1ex]\n&= \\big[1.56,~2.94\\big]\n\\end{split}\n\\]"
  },
  {
    "objectID": "readings/05-simple-linear-regression-description.html#footnotes",
    "href": "readings/05-simple-linear-regression-description.html#footnotes",
    "title": "Introduction to Probability Distributions",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nRemember, the mean and standard deviations in the population are called “parameters”.↩︎\nWhether this is actually t-distributed depends on whether the model assumptions are met.↩︎"
  },
  {
    "objectID": "readings/06-ols-estimation.html",
    "href": "readings/06-ols-estimation.html",
    "title": "📖 Ordinary Least Squares (OLS) Estimation",
    "section": "",
    "text": "Required\n\nOrdinary Least Squares Regression: Explained Visually"
  },
  {
    "objectID": "readings/17-polynomial-effects.html",
    "href": "readings/17-polynomial-effects.html",
    "title": "📖 Polynomial Effects",
    "section": "",
    "text": "Required\nRefresh your knowledge about parabolas and quadratic functions by going though the Khan Academy Parabolas Intro.\n\nAdditional Resources\nIn addition to the notes and what we cover in class, there are many other resources for learning about polynomial functions. Here are some resources that may be helpful in that endeavor:\n\nVarsity Tutors: Quadratic Function\nMath Centre: Polynomial Function\nOverfitting: A guided tour"
  },
  {
    "objectID": "readings/07-correlation-and-standardized-regression.html",
    "href": "readings/07-correlation-and-standardized-regression.html",
    "title": "📖 Correlation and Standardized Regression",
    "section": "",
    "text": "Required\n\nEasy Correlation Matrix Analysis in R Using Corrr Package\n\nAdditional Resources\n\ncorrr Package Vignette\nExploring correlations in R with corrr\nGuess the Correlation Game [fun game]\nBring, J. (1994). How to standardize regression coefficients. The American Statistician, 48(3), 209–213."
  },
  {
    "objectID": "readings/08-coefficient-level-inference.html",
    "href": "readings/08-coefficient-level-inference.html",
    "title": "📖 Coefficient-Level Inference",
    "section": "",
    "text": "Required\n\nLewis-Beck & Lewis-Beck [Chap. 2]\nAmrhein, V., Greenland, S., & McShane, B. (2019). Comment: Retire statistical significance. Nature, 567, 305–307.\n\nAdditional Resources\n\nDance of the p-values [video]"
  },
  {
    "objectID": "readings/09-model-level-inference.html",
    "href": "readings/09-model-level-inference.html",
    "title": "📖 Model-Level Inference",
    "section": "",
    "text": "Required\n\nThe Boy Who Cried Type I Error\nType I and Type II Errors Simplified"
  },
  {
    "objectID": "readings/10-introduction-to-multiple-linear-regression.html",
    "href": "readings/10-introduction-to-multiple-linear-regression.html",
    "title": "📖 Introduction to Multiple Linear Regression",
    "section": "",
    "text": "Required\n\nLewis-Beck & Lewis-Beck [Chap. 3]\n\nAdditional Resources\n\nMcDonald, J. H. (2014). Confounding variables. In Handbook of biological statistics (3rd ed., pp. 24–28). Sparky House Publishing."
  },
  {
    "objectID": "readings/11-understanding-statistical-control.html",
    "href": "readings/11-understanding-statistical-control.html",
    "title": "📖 Understanding Statistical Control",
    "section": "",
    "text": "Required\n\nPourhoseingholi, M. A.Baghestani, A. R., & Vahedi, M. (2012). How to control confounding effects by statistical analysis. Gastroenterology and hepatology from bed to bench, 5(2), 79–83."
  },
  {
    "objectID": "readings/12-distributional-assumptions-underlying-the-regression-model.html",
    "href": "readings/12-distributional-assumptions-underlying-the-regression-model.html",
    "title": "📖 Distributional Assumptions Underlying the Regression Model",
    "section": "",
    "text": "Required\n\nRe-read Lewis-Beck & Lewis-Beck [Chap. 3]\n\nAdditional Resources\n\nVisualizing Residuals [blog post]\nOsborne, J. W., & Overbay, A. (2004). The power of outliers (and why researchers should ALWAYS check for them). Practical Assessment, Research & Evaluation, 9(6)."
  },
  {
    "objectID": "readings/13-dichotomous-categorical-predictors.html",
    "href": "readings/13-dichotomous-categorical-predictors.html",
    "title": "📖 Dichotomous Categorical Predictors",
    "section": "",
    "text": "Required\n\nWhat is Dummy Coding? [web article]"
  },
  {
    "objectID": "readings/14-polychotomous-categorical-predictors.html",
    "href": "readings/14-polychotomous-categorical-predictors.html",
    "title": "📖 Polychotomous Categorical Predictors",
    "section": "",
    "text": "Required\n\nTechnical Methods Report: Guidelines for Multiple Testing in Impact Evaluations\n\nAdditional Resources\n\nGelman, A., Hill, J., & Yajima, M. (2012). Why we (usually) don’t have to worry about multiple comparisons. Journal of Research on Educational Effectiveness, 5, 189–211.\nScience isn’t Broken [web article]\nVeazie, P. J. (2006). When to combine hypotheses and adjust for multiple Tests. Health Services Research, 41(3 Pt 1), 804–818. doi: 10.1111/j.1475-6773.2006.00512.x"
  },
  {
    "objectID": "readings/15-introduction-to-interaction-effects.html",
    "href": "readings/15-introduction-to-interaction-effects.html",
    "title": "📖 Introduction to Interaction Effects",
    "section": "",
    "text": "Required\n\nBrambor, T., Clark, W. R., & Golder, M. (2006). Understanding interaction models: Improving empirical analyses. Political Analysis, 14, 63–82.\n\nAdditional resources\n\nFrost, J. (2018). Understanding Interaction Effects in Statistics. [blog post]"
  },
  {
    "objectID": "readings/16-more-interaction-effects.html",
    "href": "readings/16-more-interaction-effects.html",
    "title": "📖 More Interaction Effects",
    "section": "",
    "text": "Required\n\nWilliams, R. (2015). Interpreting interaction effects; interaction effects and centering."
  },
  {
    "objectID": "assignments.html#faqs",
    "href": "assignments.html#faqs",
    "title": "Assignment Due Dates",
    "section": "FAQs",
    "text": "FAQs\nHow do I submit the assignment?\nCreate a PDF of your responses and submit the PDF via email to both the instructor and TA. Also cc any group members. Before you submit the assignment check that:\n\nAll group members’ names are on the assignment.\nAll tables are numbered and have a caption.\nAll figures are numbered and have a caption.\nAll figures are re-sized to not take up more page space than is necessary to read them.\nNo R syntax is included unless the question specifically asked for the syntax to be included. If there is R syntax included, be sure that it is typeset in a monospaced font (e.g., Courier, Inconsolata).\nDo not submit the script file you used unless the directions specifically ask you to submit it.\n\nDo I need to turn in the script file?\nNo…unless the directions specifically ask you to submit the script file. The script file is for your reference. Future assignments will sometimes have questions that reference older assignments, so an organized and well-commented script file is a good idea. Moreover, the TA or myself may ask you to submit your syntax after you submit the assignment to help us interpret mistakes you made on the assignment so that we can provide more thorough feedback.\nWill you provide answer keys to the assignments?\nNo. You will get back several comments on your assignments that address what you did wrong. The educational research is pretty clear that feedback is more helpful to student learning than providing correct answers. If you have further questions or need additional clarification, you are welcome to come to office hours or make an appointment with the instructor or TA.\nWill you go over the answers in class?\nNo. I will address broad concepts if several students/groups made the same mistake, but otherwise, since each student/group makes unique mistakes going over the assignments more broadly is not a good use of our limited class time. The feedback should be helpful in understanding any mistakes you made, but if it isn’t, you are welcome to come to office hours or make an appointment with the instructor or TA.\nWill you look at our assignment prior to us submitting it?\nThe instructor or TA will not “pre-grade” your assignment. If there is a “Preparation” part of the assignment we can ensure that you did that part correctly, and we can also give you feedback about syntax, but we will not tell you if an answer is correct or not. We can also answer clarifying questions (e.g., ‘I know Question 8 is asking about […], but I’m not sure what this question is looking for. Is there another way to restate this question?’). If you completed the assignment without any trouble, then be confident in your work and simply submit the assignment! 😄 If you are unsure about something specific, then ask about that specific thing rather than a general ‘did I do this right’!\nWill we be able to re-do the assignment?\nGenerally no. Since you have the opportunity to work in groups on the assignments, you should be able offset any of the issues that come up that would necessitate a re-submission. The exception to this is that we may allow you to re-do Assignment 1. If we do this, we will send you an email requesting that you resubmit part, or all of Assignment 1. You will not receive much feedback on your assignment prior to your re-submission. This is because during the first assignment you are learning not only content, but also expectations around how to respond to the questions we ask."
  },
  {
    "objectID": "assignments/assignment-01-introduction-to-statistical-computing.html",
    "href": "assignments/assignment-01-introduction-to-statistical-computing.html",
    "title": "Assignment 01",
    "section": "",
    "text": "The goal of this assignment is to give you experience working with the R statistical computing environment. In this assignment, you will use the data from the file broadband.csv to examine broadband access in the United States."
  },
  {
    "objectID": "assignments/assignment-01-introduction-to-statistical-computing.html#instructions",
    "href": "assignments/assignment-01-introduction-to-statistical-computing.html#instructions",
    "title": "Assignment 01",
    "section": "Instructions",
    "text": "Instructions\nSubmit a PDF document of your responses to the following questions. Please adhere to the following guidelines for further formatting your assignment:\n\nAll plots should be resized so that they do not take up more room than necessary.\nAll figures and tables should have a name (e.g., Figure 1) and an appropriate caption.\n\nThis assignment is worth 11 points."
  },
  {
    "objectID": "assignments/assignment-01-introduction-to-statistical-computing.html#preparation",
    "href": "assignments/assignment-01-introduction-to-statistical-computing.html#preparation",
    "title": "Assignment 01",
    "section": "Preparation",
    "text": "Preparation\nIf you have not already installed them, you will need to install the {dplyr} and {ggplot2} packages to complete this assignment. Once these have been installed successfully, you should not need to install them again. Remember: Install once; load every session.\nOpen a new script file. Save the script file as Assignment-01.R. Save all of the R syntax you use to answer the questions on this assignment in this script file.\nDenote each question in the script file using comments. For example,\n##################################################\n### Question 1\n##################################################\n\n&lt;&lt; syntax &gt;&gt;\nAdd comments throughout your syntax as liberally as you feel is necessary to help you recall what the syntax does in the future. Although you do not need to submit this in with your assignment, it will be useful for building good coding habits and potentially for future assignments."
  },
  {
    "objectID": "assignments/assignment-01-introduction-to-statistical-computing.html#part-i-minnesota-broadband-access",
    "href": "assignments/assignment-01-introduction-to-statistical-computing.html#part-i-minnesota-broadband-access",
    "title": "Assignment 01",
    "section": "Part I: Minnesota Broadband Access",
    "text": "Part I: Minnesota Broadband Access\nAfter importing the broadband data into RStudio, create a new data frame object that only includes counties in Minnesota (postal code for Minnesota is MN). Hint: Use filter(). Assign the Minnesota broadband data into a new object called mn. Use the mn data to answer the following questions.\n\nUse the geom_density() function from the {ggplot2} package to create a density plot of the distribution of the microsoft_useage variable for all 87 counties in Minnesota. You can see many examples of how to do this here. Include this plot in a word-processed document. Resize the plot so it does not take up any more space than necessary. Be sure the plot has appropriate labels and has a caption.\nUse the geom_density() function from the {ggplot2} package to create a density plot of the distribution of the microsoft_useage variable for the counties in Minnesota, but this time use color= or fill= to color by whether the county is a metropolitan or non-metropolitan county. (Hint: You should get two density plots.). Include this plot in a word-processed document. Resize the plot so it does not take up any more space than necessary. Be sure the plot has appropriate labels and has a caption.\nCompute the mean and the standard deviation of the microsoft_useage variable for metro and non-metro counties. Report these values.\nBy referring to the values, and the plots, what can you say about broadband access in Minnesota? Is there a broadband access gap?"
  },
  {
    "objectID": "assignments/assignment-01-introduction-to-statistical-computing.html#part-ii-state-level-broadband-access",
    "href": "assignments/assignment-01-introduction-to-statistical-computing.html#part-ii-state-level-broadband-access",
    "title": "Assignment 01",
    "section": "Part II: State-Level Broadband Access",
    "text": "Part II: State-Level Broadband Access\nIn this section, you will use all the data in broadband.csv to create state-level summaries (means and standard deviations) of the microsoft_useage and the fcc_availability variables. Filter out Washington DC so it is not included. (Hint: Make sure that every state has mean and standard deviation values when you are done!) Use this state-level data to answer the following questions.\n\nBased on the Microsoft and FCC estimates, does it seem that the FCC estimates (which are provided by the ISPs) are “overestimating” America’s broadband access? Explain.\nUsing the Microsoft estimates, which states have the best and worst broadband access based on the means? Which states are the most and least homogenous (across counties) in terms of their broadband access?\nCreate a table that includes the Microsoft and FCC estimates of broadband access you computed for the states in bordering Minnesota: Minnesota (MN), Iowa (IA), South Dakota (SD), North Dakota (ND), and Wisconsin (WI). Report these values in a word-processed table alphabetically by state. To format this table: Examine the structure and formatting of Table 2 at https://zief0002.github.io/musings/creating-tables-to-present-statistical-results.html. Notice that variables are presented in rows and summary statistics are presented in columns. Mimic the format and structure of this table to create a table to present the numerical summary information asked for in this question. Finally, give your table a name (e.g., Table 1) and an appropriate caption."
  },
  {
    "objectID": "assignments/assignment-01-introduction-to-statistical-computing.html#part-iii-state-level-broadband-access-versus-poverty",
    "href": "assignments/assignment-01-introduction-to-statistical-computing.html#part-iii-state-level-broadband-access-versus-poverty",
    "title": "Assignment 01",
    "section": "Part III: State-Level Broadband Access versus Poverty",
    "text": "Part III: State-Level Broadband Access versus Poverty\nIn this section, you will use all the data in broadband.csv to create state-level means of the microsoft_useage and the pct_poverty variables. (Hint: Make sure that every state has mean values when you are done!) Use this state-level data to answer the following questions.\n\nUse ggplot() to create a scatterplot of the relationship between the proportion of users with access to broadband and the proportion of people living in poverty using the state-level data. (Put the proportion living in poverty on the x-axis.) Change the axis labels so that both the x- and y-axis have labels that suitably describe the variables being plotted. (For help on this, read the Axes page of the Cookbook for R website.) Include this plot in a word-processed document. Resize the plot so it does not take up any more space than necessary. Finally, give your figure a name (e.g., Figure 1) and an appropriate caption.\nBased on these data, describe the relationship between poverty and broadband access.\nIdentify Minnesota on your plot. (You do not have to use R to do this; you can use an image editor, or do it by hand.) You can either create a new plot or just include Minnesota on the plot you include for Question 8.\nBased on its location in the plot, how does Minnesota compare to the other states in terms of broadband access? What about in terms of poverty? Explain."
  },
  {
    "objectID": "assignments/assignment-01-introduction-to-statistical-computing.html#faqs",
    "href": "assignments/assignment-01-introduction-to-statistical-computing.html#faqs",
    "title": "Assignment 01",
    "section": "FAQs",
    "text": "FAQs\nHow do I submit the assignment?\nCreate a PDF of your responses and submit the PDF via email to both the instructor and TA. Also cc any group members. Before you submit the assignment check that:\nAll group members’ names are on the assignment. All tables are numbered and have a caption. All figures are numbered and have a caption. No R syntax is included unless the question specifically asked for the syntax to be included. If there is R syntax included, be sure that it is typeset in a monospaced font (e.g., Courier, Inconsolata).\nDo I need to turn in the script file?\nNo…unless the directions specifically ask you to submit the script file. The script file is for your reference. Future assignments will sometimes have questions that reference older assignments, so an organized and well-commented script file is a good idea. Moreover, the TA or myself may ask you to submit your syntax after you submit the assignment to help us interpret mistakes you made on the assignment so that we can provide more thorough feedback.\nWill you provide answer keys to the assignments?\nNo. You will get back several comments on your assignments that address what you did wrong. The educational research is pretty clear that feedback is more helpful to student learning than providing correct answers. If you have further questions or need additional clarification, you are welcome to come to office hours or make an appointment with the instructor or TA.\nWill you go over the answers in class?\nNo. I will address broad concepts if several students/groups made the same mistake, but otherwise, since each student/group makes unique mistakes going over the assignments more broadly is not a good use of our limited class time. The feedback should be helpful in understanding any mistakes you made, but if it isn’t, you are welcome to come to office hours or make an appointment with the instructor or TA.\nWill you look at our assignment prior to us submitting it?\nThe instructor or TA will not “pre-grade” your assignment. If there is a “Preparation” part of the assignment we can ensure that you did that part correctly, and we can also give you feedback about syntax, but we will not tell you if an answer is correct or not. We can also answer clarifying questions (e.g., ‘I know Question 8 is asking about […], but I’m not sure what this question is looking for. Is there another way to restate this question?’). If you completed the assignment without any trouble, then be confident in your work and simply submit the assignment! 😄 If you are unsure about something specific, then ask about that specific thing rather than a general ‘did I do this right’!\nWill we be able to re-do the assignment?\nGenerally no. Since you have the opportunity to work in groups on the assignments, you should be able offset any of the issues that come up that would necessitate a re-submission. The exception to this is that we may allow you to re-do Assignment 1. If we do this, we will send you an email requesting that you resubmit part, or all of Assignment 1. You will not receive much feedback on your assignment prior to your re-submission. This is because during the first assignment you are learning not only content, but also expectations around how to respond to the questions we ask."
  },
  {
    "objectID": "assignments/assignment-01-introduction-to-statistical-computing.html#how-do-i-submit-the-assignment",
    "href": "assignments/assignment-01-introduction-to-statistical-computing.html#how-do-i-submit-the-assignment",
    "title": "Assignment 01",
    "section": "How do I submit the assignment?",
    "text": "How do I submit the assignment?\nCreate a PDF of your responses and submit the PDF via email to both the instructor and TA. Also cc any group members. Before you submit the assignment check that:\n\nAll group members’ names are on the assignment.\nAll tables are numbered and have a caption.\nAll figures are numbered and have a caption.\nAll figures are re-sized to not take up more page space than is necessary to read them.\nNo R syntax is included unless the question specifically asked for the syntax to be included. If there is R syntax included, be sure that it is typeset in a monospaced font (e.g., Courier, Inconsolata).\nDo not submit the script file you used unless the directions specifically ask you to submit it."
  },
  {
    "objectID": "assignments/assignment-02-simple-regression-description.html",
    "href": "assignments/assignment-02-simple-regression-description.html",
    "title": "Assignment 02",
    "section": "",
    "text": "Should more money be spent on public schools or should that money be spent elsewhere? Both sides of this ongoing public debate have been argued passionately, using a multitude of anecdotal evidence. Although we will not settle this debate, we will examine data akin to the types of data that policy makers use to make funding decisions.\nThe goal of this assignment is to give you experience fitting and interpreting simple regression models. In this assignment, you will use the data from the file state-education.csv to examine the relationship between teacher salaries and student total SAT scores at the state level."
  },
  {
    "objectID": "assignments/assignment-02-simple-regression-description.html#instructions",
    "href": "assignments/assignment-02-simple-regression-description.html#instructions",
    "title": "Assignment 02",
    "section": "Instructions",
    "text": "Instructions\nSubmit a PDF document of your responses to the following questions. Please adhere to the following guidelines for further formatting your assignment:\n\nAll plots should be resized so that they do not take up more room than necessary.\nAll figures and tables should have a name (e.g., Figure 1) and an appropriate caption.\n\nThis assignment is worth 10 points."
  },
  {
    "objectID": "assignments/assignment-02-simple-regression-description.html#preparation",
    "href": "assignments/assignment-02-simple-regression-description.html#preparation",
    "title": "Assignment 02",
    "section": "Preparation",
    "text": "Preparation\nBefore carrying out any analyses, create a predictor called salary_thousand that indicates the average 2020–21 state salary in thousands of dollars (e.g., salary = 52143; salary_thousand = 52.143). This variable (not salary) should be used in all analyses in the assignment."
  },
  {
    "objectID": "assignments/assignment-02-simple-regression-description.html#questions",
    "href": "assignments/assignment-02-simple-regression-description.html#questions",
    "title": "Assignment 02",
    "section": "Questions",
    "text": "Questions\n\nCreate a plot of the distribution of total SAT scores. Make sure your plot has a caption.\nExamine the structure and formatting of Table 1 at https://zief0002.github.io/musings/creating-tables-to-present-statistical-results.html. Mimic the format and structure of this table to create a table to present the numerical summary information for the distributions of SAT total scores and salaries. Provide the same measures for these variables as is given in Table 1 in the article. Re-create the formatting of Table 1 as closely as you can. Finally, make sure the table you create also has an appropriate caption.\nCreate a scatterplot of the distribution of SAT total scores (Y) conditioned on teacher salaries (X). Make sure your plot has a caption.\nDescribe the relationship between teacher salaries and SAT total scores. Be sure to comment on the structural form, direction and strength of the relationship. Also comment on any potential observations that deviate from following this relationship (unusual observations or clusters of observations).\nRegress total SAT scores on teacher salaries. Write the fitted equation using Equation Editor (or some other program that correctly types mathematical expressions).\nInterpret the value of the intercept from the regression equation using the context of the data.\nInterpret the value of the slope from the regression equation using the context of the data.\nCompute, report, and interpret the value for \\(R^2\\) based on values from the ANOVA decomposition. Show your work for full credit.\nCompute and report the predicted mean total SAT score for Minnesota students based on the average teacher salary in the state using the fitted regression equation. Show your work for full credit.\nCompute and report the residual for Minnesota. Show your work for full credit.\nExplain what the sign and magnitude of the residual value you computed in Question 10 tells you about how Minnesota’s mean SAT score compares to the mean SAT score for states having the same average teacher salary as Minnesota."
  },
  {
    "objectID": "assignments/assignment-03-correlation-and-standardized-regression.html",
    "href": "assignments/assignment-03-correlation-and-standardized-regression.html",
    "title": "Assignment 03",
    "section": "",
    "text": "Should more money be spent on public schools or should that money be spent elsewhere? Both sides of this ongoing public debate have been argued passionately, using a multitude of anecdotal evidence. Although we will not settle this debate, we will examine data akin to the types of data that policy makers use to make funding decisions. Specifically, we will examine whether teacher salaries are related to SAT scores at the state level.\nThis goal of this assignment is to give you more experience fitting and interpreting regression models. In this assignment, you will use the data from the file state-education.csv to examine the relationship between teacher salaries and student total SAT scores at the state level."
  },
  {
    "objectID": "assignments/assignment-03-correlation-and-standardized-regression.html#instructions",
    "href": "assignments/assignment-03-correlation-and-standardized-regression.html#instructions",
    "title": "Assignment 03",
    "section": "Instructions",
    "text": "Instructions\nSubmit a PDF document of your responses to the following questions. Please adhere to the following guidelines for further formatting your assignment:\n\nAll plots should be resized so that they do not take up more room than necessary.\nAll figures and tables should have a name (e.g., Figure 1) and an appropriate caption.\n\nThis assignment is worth 10 points."
  },
  {
    "objectID": "assignments/assignment-03-correlation-and-standardized-regression.html#part-i-correlation",
    "href": "assignments/assignment-03-correlation-and-standardized-regression.html#part-i-correlation",
    "title": "Assignment 03",
    "section": "Part I: Correlation",
    "text": "Part I: Correlation\nBefore carrying out any analyses, create a predictor called salary_thousand that indicates the average state salary in thousands of dollars (e.g., salary = 52143; salary_thousand = 52.143). This variable (not salary) should be used in all analyses in Part I.\n\nCompute and report the Pearson correlation coefficient between SAT scores and teacher salaries.\nIs the Pearson correlation coefficient an appropriate summary measure of the relationship? Explain. (Hint: Pay attention to the structural form in the data!)"
  },
  {
    "objectID": "assignments/assignment-04-simple-regression-inference.html",
    "href": "assignments/assignment-04-simple-regression-inference.html",
    "title": "Assignment 04",
    "section": "",
    "text": "Should more money be spent on public schools or should that money be spent elsewhere? Both sides of this ongoing public debate have been argued passionately, using a multitude of anecdotal evidence. Although we will not settle this debate, we will examine data akin to the types of data that policy makers use to make funding decisions. Specifically, we will examine whether teacher salaries are related to SAT scores at the state level.\nThis goal of this assignment is to give you more experience fitting and interpreting regression models. In this assignment, you will use the data from the file state-education.csv to examine the relationship between teacher salaries and student total SAT scores at the state level.\n\n[CSV]\n[Data Codebook]\n\n\n\n\n\n\n\n\n\n\n\nPart I: Unstandardized Regression\nBefore carrying out any analyses, create a predictor called salary_thousand that indicates the average state salary in thousands of dollars (e.g., salary = 52143; salary_thousand = 52.143). This variable (not salary) should be used in all analyses for Part I. Fit a regression model using teacher salaries to predict SAT scores.\n\nUsing symbols, write the null hypothesis that is tested by the F-statistic in this analysis.\nWrite no more than three sentences (to be included in a publication) that summarizes the results of the omnibus analysis. A summary of the results includes a written description of what is being tested by the F-test and the statistical results. At a minimum report the F-statistic, df, and p-value. A summary should also indicate what the statistical results suggest about the compatibility of the empirical data to the null hypothesis and what this means about the potential relationship between states’ average teacher salaries and average SAT scores.\nUsing symbols, write the null hypothesis that is tested by the t-statistic for the slope.\nBased on the results of the t-test for the slope, are the empirical data consistent with the null hypothesis that the sample slope is entirely due to sampling error? Explain.\nCompute and interpret the 95% compatibility interval for the slope.\nCreate a plot that displays the regression line from the unstandardized regression analysis. This plot should also include a confidence envelope (uncertainty) for the regression line. Be sure to give your plot an appropriate caption.\n\n\n\nPart II: Centering a Predictor\nCenter the salary_thousand predictor by subtracting the mean teacher salary from each value. Call this new variable center_salary_thousand This variable should be used in all analyses in Part II. Regress the SAT scores on the centered salaries.\n\nThe results of the F-test for this analysis are identical to the results of the F-test for the analysis in Part I. Explain why this is expected by referring to and comparing the hypothesis being tested in both sets of analyses.\nThe results of the t-test for the intercept in this analysis are different than the results of the t-test for the intercept in the analysis in Part I. Explain why this is expected by referring to and comparing the hypothesis being tested (and what that means) in both sets of analyses.\n\n\n\nPart III: Standardized Regression\nConvert the uncentered teacher salaries (salary_thousand) into z-scores by subtracting the mean salary and dividing by the standard deviation. Call this new variable z_salary. Also convert the SAT scores into z-scores and call that variable z_sat. Regress the SAT z-scores on the salary z-scores.\n\nThe p-value of the t-test for the intercept in this analysis is one. Explain why this is expected by referring to the hypothesis being tested in this analysis. (Hint: Think about what the intercept is and how that relates to what is being tested.)\nThe test of the slope (regardless of analysis) suggests that teacher salaries seem to be related to SAT scores. Unfortunately this relationship is negative, indicating that higher teacher salaries are associated with lower SAT scores. A public-policy wonk wants to use this data to support the de-funding of public schools. Write a couple sentences that explain to this person why your analysis does not support this causal conclusion based on the study design. (Hint: Recall that inferring causation requires particular study designs.)"
  },
  {
    "objectID": "assignments/assignment-05-introduction-to-multiple-regression.html",
    "href": "assignments/assignment-05-introduction-to-multiple-regression.html",
    "title": "Assignment 05",
    "section": "",
    "text": "Human overpopulation is a growing concern and has been associated with depletion of Earth’s natural resources (water is a big one that ) and degredation of the environment. This, in turn, has social and economic consequences such as global tension over resources such as water and food, higher cost of living and higher unemployment rates.\nFertility rate is a measure directly linked to population and has been useful to explore hypotheses about factors related to combating overpopulation. For this assignment, you will use the file fertility.csv to fit three models in order to explore the effects of contraceptive use on fertility rates."
  },
  {
    "objectID": "assignments/assignment-05-introduction-to-multiple-regression.html#instructions",
    "href": "assignments/assignment-05-introduction-to-multiple-regression.html#instructions",
    "title": "Assignment 05",
    "section": "Instructions",
    "text": "Instructions\nThe models you will fit are:\n\nModel 1: Contraception use in a country is thought to be negatively associated with fertility rates.\nModel 2: Women in countries with higher infant mortality rates will, on average, have more children (higher fertility rates). Controlling for the infant mortality rate is important to gain a more accurate view of the effect of contraception use on fertility rate.\nModel 3: Contraception use is really a function of educating women, so after controlling for education there is no/little effect of contraception use on fertility rate. In this model we will also still control for infant mortality rates.\n\nSubmit your responses to each of the questions below in a PDF document. All graphics should be resized so that they do not take up more room than necessary and also should have an appropriate caption. This assignment is worth 16 points. (Each question is worth 1 point unless otherwise noted.)"
  },
  {
    "objectID": "assignments/assignment-05-introduction-to-multiple-regression.html#preparation",
    "href": "assignments/assignment-05-introduction-to-multiple-regression.html#preparation",
    "title": "Assignment 05",
    "section": "Preparation",
    "text": "Preparation\nBegin by standardizing the fertility rate variable (outcome) and each of the three predictors used in the models listed above. Use the standardized variables to fit the regression models corresponding to each of the three models. You will use the output from the fitted models to answer the questions in the assignment."
  },
  {
    "objectID": "assignments/assignment-05-introduction-to-multiple-regression.html#part-i",
    "href": "assignments/assignment-05-introduction-to-multiple-regression.html#part-i",
    "title": "Assignment 05",
    "section": "Part I",
    "text": "Part I\n\nExamine the structure and formatting of Table 4 at https://zief0002.github.io/musings/creating-tables-to-present-statistical-results.html. Mimic the format and structure of this table to create a table to present the pairwise correlations between the outcome and each of the predictors, and between each set of predictors. Make sure the table you create also has an appropriate caption. (2pts.)\nExamine the structure and formatting of Table 9 at https://zief0002.github.io/musings/creating-tables-to-present-statistical-results.html. Mimic the format and structure of this table to create a table to present the numerical information from the three models you fitted in this assignment. Make sure the table you create also has an appropriate caption. If the table is too wide, change the page orientation in your word processing program to ``Landscape’’, rather than changing the size of the font. (2pts.)\nCreate a coefficient plot that graphically presents the coefficient estimates and the uncertainty (as 95% confidence intervals) for the coefficients included in the three fitted models. Be sure that the figure is appropriately captioned. (2pts.)\nBased on results presented in the regression table and from the coefficient plot comment on the size and direction of the effect of contraceptive rate on fertility rate. Do this by referring to the standardized coefficient and the uncertainty in the parameter estimates. Note: Do not base your response on a p-value.\n\n\nUse the results from Model 3 to answer the remainder of the questions on this assignment."
  },
  {
    "objectID": "assignments/assignment-05-introduction-to-multiple-regression.html#part-ii",
    "href": "assignments/assignment-05-introduction-to-multiple-regression.html#part-ii",
    "title": "Assignment 05",
    "section": "Part II",
    "text": "Part II\n\nReport the regression equation from fitting Model 3. Use Equation Editor (or some other program that correctly types mathematical expressions) to typeset the equation correctly.\nUsing output from the ANOVA table, compute and report the value for the model \\(R^2\\). Show your work for full credit.\nInterpret the value of the model \\(R^2\\) using the context of the data.\nUsing symbols, write the omnibus null hypothesis that is tested by the model-level F-statistic in this analysis in two different manners: (1) using the coefficient parameters used in the regression model, and (2) using the variance accounted for parameter.\nBased on the results of the model-level F-test, does the model seem to explain variation in fertility rates? Explain. Note: Here you can use the p-value as evidence, but do not compare this to 0.05.\nInterpret the estimated coefficient value associated with the partial effect of contraception.\nBased on the 95% confidence interval for the partial effect of contraception on fertility rate, which parameter values are reasonably compatible with the empirical data? Explain what this implies about the magnitude of the partial effect.\nCreate a publication quality plot that displays the results from Model 3. For this plot, put the contraception predictor on the x-axis. Control out the effect of education level by setting this to the mean level of education. Display two separate lines to show the effect of infant mortality rate; a small and large rate based on the data. The two lines should be displayed using different linetypes or colors (or both) so that they can be easily differentiated in the plot. Be sure that the figure is appropriately captioned. (2pts.)"
  },
  {
    "objectID": "assignments/assignment-06-regression-assumptions.html",
    "href": "assignments/assignment-06-regression-assumptions.html",
    "title": "Assignment 06",
    "section": "",
    "text": "This goal of this assignment is to give you more experience evaluating the assumptions underlying regression models. Submit your responses to each of the questions below in a PDF document. All graphics should be resized so that they do not take up more room than necessary and also should have an appropriate caption. This assignment is worth 15 points. (Each question is worth 1 point unless otherwise noted.)"
  },
  {
    "objectID": "assignments/assignment-06-regression-assumptions.html#part-i-evaluating-assumptions-for-the-simple-regression-model",
    "href": "assignments/assignment-06-regression-assumptions.html#part-i-evaluating-assumptions-for-the-simple-regression-model",
    "title": "Assignment 06",
    "section": "Part I: Evaluating Assumptions for the Simple Regression Model",
    "text": "Part I: Evaluating Assumptions for the Simple Regression Model\nResearch. Teaching. Service. The trifecta upon which that almost every university instructor is evaluated, and, ultimately compensated. One way which academic administrators judge teaching quality is through teachers’ course evaluations. While we know evaluation scores are not perfectly measures of teaching quality, nonetheless, they do play a role in the tenure and promotion process. Unfortunately, many other non-teaching related factors are also associated with evaluation scores (e.g., professor’s ethnicity, professor’s sex).\nFor this part of the assignment, you will examine whether instructor attractiveness explains differences in course evaluation scores—and thus on earnings differences. To do so, you will use the data in the evaluations.csv file to fit a regression model that uses professors’ beauty ratings to predict the variation in course evaluation ratings.\n\n[CSV]\n[Data Codebook]\n\nFit the regression model to predict the variation in course evaluation ratings using professors’ beauty ratings. You will use the output from the fitted model to answer the questions in Part I.\n\n\nPreliminary Examination of Model Assumptions\n\nCreate and include the density plot for the outcome. Does the distribution foreshadow problems for the normality assumption? Explain.\nCreate and include the scatterplot of the outcome vs. the predictor. Include the loess smoother in the plots. Does this relationship foreshadow problems for the linearity assumption? Explain.\n\n\n\n\nExamination of the Standardized Residuals from the Simple Regression Model\n\nCreate and include the density plot of the marginal distribution of the standardized residuals from the fitted model. Add the confidence envelope for the normal distribution. Does this plot suggest problems about meeting the normality assumption? Explain.\nCreate and include the scatterplot of the standardized residuals versus the fitted values from the fitted model. In the plot identify observation with extreme residuals (\\(\\leq-3\\) or \\(\\geq3\\)) by indicating the row number of that observation in the plot.\nDoes this plot suggest problems about meeting the linearity assumption? Explain.\nDoes this plot suggest problems about meeting the homogeneity of variance assumption? Explain.\nIs the independence assumption tenable? Explain."
  },
  {
    "objectID": "assignments/assignment-06-regression-assumptions.html#part-ii-evaluating-assumptions-for-the-multiple-regression-model",
    "href": "assignments/assignment-06-regression-assumptions.html#part-ii-evaluating-assumptions-for-the-multiple-regression-model",
    "title": "Assignment 06",
    "section": "Part II: Evaluating Assumptions for the Multiple Regression Model",
    "text": "Part II: Evaluating Assumptions for the Multiple Regression Model\nHuman overpopulation is a growing concern and has been associated with depletion of Earth’s natural resources (water is a big one that ) and degredation of the environment. This, in turn, has social and economic consequences such as global tension over resources such as water and food, higher cost of living and higher unemployment rates. For this part of the assignment, you will use the file fertility.csv to fit a model in order to explore the effects of contraceptive use on fertility rates.\n\n[CSV]\n[Data Codebook]\n\nFit the regression model to predict the variation in fertility rates using contraception use, female education, and infant mortality rate (three predictors). You will use the output from the fitted model to answer the questions in Part II.\n\n\nPreliminary Examination of Model Assumptions\n\nCreate and include the density plot for the outcome. Does the distribution foreshadow problems for the normality assumption? Explain.\nCreate and include the scatterplot of the outcome vs. each predictor (three total). Include the loess smoother in each of the plots. Do any of these relationships foreshadow problems for the linearity assumption? Explain. (2pts.)\n\n\n\n\nExamination of the Standardized Residuals from the Multiple Regression Model\n\nCreate and include the density plot of the marginal distribution of the standardized residuals from the fitted model. Add the confidence envelope for the normal distribution. Does this plot suggest problems about meeting the normality assumption? Explain.\nCreate and include the scatterplot of the standardized residuals versus the fitted values from the fitted model. In the plot identify observation with extreme residuals (\\(\\leq-3\\) or \\(\\geq3\\)) by indicating the country associated with that observation in the plot.\nDoes this plot suggest problems about meeting the linearity assumption? Explain.\nDoes this plot suggest problems about meeting the homogeneity of variance assumption? Explain.\nIs the independence assumption tenable? Explain."
  },
  {
    "objectID": "assignments/assignment-07-dichotomous-categorical-predictors.html",
    "href": "assignments/assignment-07-dichotomous-categorical-predictors.html",
    "title": "Assignment 07",
    "section": "",
    "text": "This goal of this assignment is to give you experience fitting and interpreting regression models with categorical predictors. For this assignment, you will again examine whether there are differences in course evaluation scores—and thus on earnings differences—between native and non-native English speaking professors. To do so, you will use the data in the evaluations.csv file to fit a series of regression models to predict the variation in course evaluation ratings."
  },
  {
    "objectID": "assignments/assignment-07-dichotomous-categorical-predictors.html#instructions",
    "href": "assignments/assignment-07-dichotomous-categorical-predictors.html#instructions",
    "title": "Assignment 07",
    "section": "Instructions",
    "text": "Instructions\nSubmit your responses to each of the questions below in a PDF document. All graphics should be resized so that they do not take up more room than necessary and also should have an appropriate caption. This assignment is worth 15 points. (Each question is worth 1 point unless otherwise noted.)"
  },
  {
    "objectID": "assignments/assignment-07-dichotomous-categorical-predictors.html#unadjusted-group-differences-model-anova",
    "href": "assignments/assignment-07-dichotomous-categorical-predictors.html#unadjusted-group-differences-model-anova",
    "title": "Assignment 07",
    "section": "Unadjusted Group Differences Model: ANOVA",
    "text": "Unadjusted Group Differences Model: ANOVA\nFit a regression model using the dummy-coded native_english predictor to explain variation in course evaluation scores. Use the glance() and tidy() functions to examine the output.\n\nWrite the fitted regression equation.\nInterpret the intercept coefficient.\nInterpret the slope coefficient.\nIn terms of means (not betas), write the null hypothesis (using mathematical notation) associated with the t-test of the slope? Be specific.\nBased on results of the t-test for the slope, what do you conclude about differences in evaluation scores between native and non-native English speaking professors?\nUse the fitted regression equation to estimate (a) the mean course rating for native and (b) the mean course rating for non-native English speakers. Show your work."
  },
  {
    "objectID": "assignments/assignment-07-dichotomous-categorical-predictors.html#adjusted-group-differences-model-ancova",
    "href": "assignments/assignment-07-dichotomous-categorical-predictors.html#adjusted-group-differences-model-ancova",
    "title": "Assignment 07",
    "section": "Adjusted Group Differences Model: ANCOVA",
    "text": "Adjusted Group Differences Model: ANCOVA\nNow, suppose you want to examine differences in course evaluation scores between native and non-native English speakers, but this time you want to control for differences in professors’ beauty ratings and the number of courses for which the professor has evaluations. Fit this model and use the glance() and tidy() functions to examine the output.\n\nWrite the fitted regression equation.\nInterpret the fitted regression coefficient for native_english.\nCompare the size and direction of the difference in course evaluation scores between native and non-native English speakers in the adjusted model to those from the unadjusted model. How do they compare? Also compare the uncertainty in the estimates.\nWrite the fitted regression equation for native English speakers. (Note: This equation should only include the effects of beauty and the number of courses for which the professor has evaluations.)\nWrite the fitted regression equation for non-native English speakers. (Note: This equation should only include the effects of beauty and the number of courses for which the professor has evaluations.)\nCompute the adjusted mean course rating for native and non-native English speakers (based on professors having an average beauty and an average number of courses). Show your work."
  },
  {
    "objectID": "assignments/assignment-07-dichotomous-categorical-predictors.html#model-assumptions",
    "href": "assignments/assignment-07-dichotomous-categorical-predictors.html#model-assumptions",
    "title": "Assignment 07",
    "section": "Model Assumptions",
    "text": "Model Assumptions\n\nCreate the density plot of the marginal distribution of the standardized residuals from the ANCOVA model. Add the confidence envelope for the normal distribution. Explain whether or not this plot suggests problems about meeting the normality assumption.\nCreate the scatterplot of the standardized residuals versus the fitted values from the ANCOVA model. In the plot identify observation with extreme residuals (\\(\\leq-3\\) or \\(\\geq3\\)) by indicating the row number of that observation in the plot.\nExplain whether or not this plot suggests problems about meeting the linearity and homogeneity of variance assumptions."
  },
  {
    "objectID": "assignments/assignment-08-polychotomous-categorical-predictors.html",
    "href": "assignments/assignment-08-polychotomous-categorical-predictors.html",
    "title": "Assignment 08",
    "section": "",
    "text": "The goal of this assignment is to give you more experience fitting and interpreting regression models with categorical predictors. For this assignment, you will be fitting several regression models to examine whether there are differences in the engagement level of reviewers on IMDb for Scooby-Doo episodes/movies based on which members of Mystery Inc caught the villain. To do so, you will use the data in the file scoobydoo.csv."
  },
  {
    "objectID": "assignments/assignment-08-polychotomous-categorical-predictors.html#instructions",
    "href": "assignments/assignment-08-polychotomous-categorical-predictors.html#instructions",
    "title": "Assignment 08",
    "section": "Instructions",
    "text": "Instructions\nSubmit your responses to each of the questions below in a PDF document. All graphics should be resized so that they do not take up more room than necessary and also should have an appropriate caption. This assignment is worth 15 points. (Each question is worth 1 point unless otherwise noted.)"
  },
  {
    "objectID": "assignments/assignment-08-polychotomous-categorical-predictors.html#preparation-create-dummy-variables",
    "href": "assignments/assignment-08-polychotomous-categorical-predictors.html#preparation-create-dummy-variables",
    "title": "Assignment 08",
    "section": "Preparation: Create Dummy Variables",
    "text": "Preparation: Create Dummy Variables\nCreate four dummy variables for the analysis; one for each condition of the caught_by attribute. Also create another dummy variable to represent media format."
  },
  {
    "objectID": "assignments/assignment-08-polychotomous-categorical-predictors.html#description",
    "href": "assignments/assignment-08-polychotomous-categorical-predictors.html#description",
    "title": "Assignment 08",
    "section": "Description",
    "text": "Description\n\nCreate a table of pairwise correlations between engagement, each of the four dummy variables you created to represent caught_by, IMDb rating, number of catchphrases uttered, and the media format dummy variable.\nInterpret (i) the correlation between the Shaggy/Scooby dummy variable and engagement, and (ii) the correlation between the Shaggy/Scooby dummy variable and the media format dummy variable."
  },
  {
    "objectID": "assignments/assignment-08-polychotomous-categorical-predictors.html#unadjusted-group-differences-model-anova",
    "href": "assignments/assignment-08-polychotomous-categorical-predictors.html#unadjusted-group-differences-model-anova",
    "title": "Assignment 08",
    "section": "Unadjusted Group Differences Model: ANOVA",
    "text": "Unadjusted Group Differences Model: ANOVA\nFit the regression model that uses the dummy predictors for which Mystery Inc. members caught the villain to predict variation in IMDb engagement. In this model, use Shaggy/Scooby as the reference group.\n\nWrite the fitted regression equation.\nWhich conditions of caught_by, if any, differ from Shaggy/Scooby in the average engagement on IMDb produced (more than we expect because of sampling variation)? Explain.\nReport and interpret the \\(R^2\\) value for this model.\nWhich comparisons of the caught_by condition reflected in the omnibus null hypothesis are not represented in this fitted model?"
  },
  {
    "objectID": "assignments/assignment-08-polychotomous-categorical-predictors.html#adjusted-group-differences-model-ancova",
    "href": "assignments/assignment-08-polychotomous-categorical-predictors.html#adjusted-group-differences-model-ancova",
    "title": "Assignment 08",
    "section": "Adjusted Group Differences Model: ANCOVA",
    "text": "Adjusted Group Differences Model: ANCOVA\nAgain, fit the regression model that uses the dummy predictors for which Mystery Inc. members caught the villain to predict variation in average IMDb engagement, but this time control for differences in (1) IMDb rating, (2) number of catchphrases uttered, and (3) media format. Again, use Shaggy/Scooby as the reference group.\n\nWrite the fitted regression equation.\nWhich conditions of caught_by, if any, differ from Shaggy/Scooby in the average IMDb engagment (more than we expect because of sampling variation) after controlling for differences in these other predictors? Explain\nReport and interpret the \\(R^2\\) value for this model."
  },
  {
    "objectID": "assignments/assignment-08-polychotomous-categorical-predictors.html#assumptions",
    "href": "assignments/assignment-08-polychotomous-categorical-predictors.html#assumptions",
    "title": "Assignment 08",
    "section": "Assumptions",
    "text": "Assumptions\n\nCreate the density plot of the marginal distribution of the standardized residuals from the ANCOVA model. Add the confidence envelope for the normal distribution. Explain whether or not this plot suggests problems about meeting the normality assumption.\nCreate the scatterplot of the standardized residuals versus the fitted values from the ANCOVA model. In the plot identify observation with extreme residuals (\\(\\leq-3\\) or \\(\\geq3\\)) by indicating the row number of that observation in the plot.\nExplain whether or not this plot suggests problems about meeting the linearity and homogeneity of variance assumptions."
  },
  {
    "objectID": "assignments/assignment-08-polychotomous-categorical-predictors.html#pairwise-differences",
    "href": "assignments/assignment-08-polychotomous-categorical-predictors.html#pairwise-differences",
    "title": "Assignment 08",
    "section": "Pairwise Differences",
    "text": "Pairwise Differences\nUse the results from comparing the pairwise differences between the conditions of caught_by in the ANCOVA model to answer the questions in this section.\n\nCreate a table (suitable for publication) that presents each of the possible pairwise contrasts (null hypotheses) of interest, the unadjusted p-values from the controlled model, and the Benjamini–Hochberg adjusted p-values for the controlled differences. (Note: To obtain all of these, you may need to fit additional models.)\nUse the Benjamini–Hochberg adjusted p-values from the controlled model to help answer the research question: Are there differences in the engagement level of reviewers on IMDb for Scooby-Doo episodes/movies based on which members of Mystery Inc caught the villain? In answering this question, also indicate how IMDb engagement differs for the conditions of caught_by.\nCreate a heatmap of the Benjamini–Hochberg adjusted p-values for the controlled model that you reported in the table in Question 13. You can see an example of a heatmap for correlations here. We want to create a heatmap that shows the p-value for each pairwise comparison instead of the correlation between variables. So for example, the heatmap we want to create should be a 4x4 grid (the rows and columns would represent conditions of caught_by) and the intersecting cells would include the p-value for the comparison (rather than the correlation coefficient). Color will be used to indicate the magnitude of the p-values. You may want to include different levels of color depending on the size of the p-value (\\(&lt;.05\\), \\(&lt;.01\\), etc.) Feel free to use any software tool you want to create this heatmap."
  },
  {
    "objectID": "assignments/assignment-09-interactions.html",
    "href": "assignments/assignment-09-interactions.html",
    "title": "Assignment 09",
    "section": "",
    "text": "The goal of this assignment is to give you experience fitting and interpreting regression models with interaction effects. For this assignment, you will be fitting several regression models to examine whether there are differences in the engagement level of reviewers on IMDb for Scooby-Doo episodes/movies based on which members of Mystery Inc caught the villain. To do so, you will use the data in the file scoobydoo.csv."
  },
  {
    "objectID": "assignments/assignment-09-interactions.html#instructions",
    "href": "assignments/assignment-09-interactions.html#instructions",
    "title": "Assignment 09",
    "section": "Instructions",
    "text": "Instructions\nSubmit your responses to each of the questions below in a PDF document. All graphics should be resized so that they do not take up more room than necessary and also should have an appropriate caption. This assignment is worth 12 points. (Each question is worth 1 point unless otherwise noted.)"
  },
  {
    "objectID": "assignments/assignment-09-interactions.html#preparation-fitting-models",
    "href": "assignments/assignment-09-interactions.html#preparation-fitting-models",
    "title": "Assignment 09",
    "section": "Preparation: Fitting Models",
    "text": "Preparation: Fitting Models\nCreate the following dummy variables for the analysis;\n\nA set of three dummy variables to represent the the caught_by attribute: Shaggy/Scooby, Fred/Daphne/Velma, and Other/Combo.\nA dummy variable to represent media format.\n\nThen fit the following regression models. For all models, use IMDb engagement as the response variable.\n\nModel A: Main effects of caught_by (use the other/combo group as your reference group), IMDb rating, number of catchphrases uttered, and media format.\nModel B: The same main effects included in Model A; and an interaction effect between media format and IMDb rating.\nModel C: The same main effects included in Model A; and an interaction effect between number of catchphrases uttered and IMDb rating.\n\nYou will use the output from these fitted models to answer the questions in the assignment."
  },
  {
    "objectID": "assignments/assignment-09-interactions.html#description",
    "href": "assignments/assignment-09-interactions.html#description",
    "title": "Assignment 09",
    "section": "Description",
    "text": "Description\n\nCreate and report a table of pairwise correlations between engagement, each of the three dummy variables you created to represent caught_by, IMDb rating, number of catchphrases uttered, and the media format dummy variable.\nBased on the correlation table, can you infer whether or not there will be a sample interaction between between IMDb rating and number of catchphrases uttered? Explain."
  },
  {
    "objectID": "assignments/assignment-09-interactions.html#table-of-regression-results",
    "href": "assignments/assignment-09-interactions.html#table-of-regression-results",
    "title": "Assignment 09",
    "section": "Table of Regression Results",
    "text": "Table of Regression Results\n\nExamine the structure and formatting of the table in the “Presenting Results from Many Fitted Regression Models” section here. Mimic the format and structure of this table to create a table to present the numerical information from the three models you fitted in this assignment. Make sure the table you create also has an appropriate caption. If the table is too wide, change the page orientation in your word processing program to ``Landscape’’, rather than changing the size of the font. (2pts.)"
  },
  {
    "objectID": "assignments/assignment-09-interactions.html#model-b",
    "href": "assignments/assignment-09-interactions.html#model-b",
    "title": "Assignment 09",
    "section": "Model B",
    "text": "Model B\n\nWrite the fitted regression equation for Model B.\nBased on the inferential results from fitting this model, is there an interaction effect between media format and IMDb rating? Explain.\nInterpret the coefficient associated with the interaction effect between media format and IMDb rating.\nRe-write the fitted regression equation, by writing two fitted equations: one for TV episodes and one for movies. Be sure each equation is identified with the appropriate subgroup.\nCreate and include a plot of the fitted model showing the pertinent results from Model B. Be sure to appropriately differentiate between lines that you include in the plot (e.g., do not differentiate by color unless you plan to print in color). If you partial out any variables, be sure to note this in the caption."
  },
  {
    "objectID": "assignments/assignment-09-interactions.html#model-c",
    "href": "assignments/assignment-09-interactions.html#model-c",
    "title": "Assignment 09",
    "section": "Model C",
    "text": "Model C\n\nWrite the fitted regression equation for Model C.\nBased on the inferential results from fitting this model, is there an interaction effect between number of catchphrases uttered and IMDb rating? Explain.\nCreate and include a plot of the fitted model showing the pertinent results from Model C. Be sure to appropriately differentiate between lines that you include in the plot (e.g., do not differentiate by color unless you plan to print in color). If you partial out any variables, be sure to note this in the caption.\nUse the plot you created in Question 11 to help interpret the interaction effect between number of catchphrases uttered and IMDb rating."
  },
  {
    "objectID": "data.html#how-to-download-a-csv-file-to-your-computer",
    "href": "data.html#how-to-download-a-csv-file-to-your-computer",
    "title": "Data",
    "section": "How to Download a CSV File to Your Computer",
    "text": "How to Download a CSV File to Your Computer\nTo download a data (CSV) file, click on the data set. Then click the RAW button. Now you should be able to right-click the dataset and save it. If you are using Safari on a Mac, make sure that the downloaded data does not have an extra .txt appended to it when it downloads. If it does, just delete the extra .txt suffix."
  },
  {
    "objectID": "worksheets/worksheet-01-using-R.html",
    "href": "worksheets/worksheet-01-using-R.html",
    "title": "Using RStudio Worksheet",
    "section": "",
    "text": "Complete the problems on this worksheet with 2–3 other students. You may want to refer to the Getting Started with R and Data Structures in R chapters in the Computational Toolkit for Educational Scientists."
  },
  {
    "objectID": "worksheets/worksheet-01-using-R.html#directions",
    "href": "worksheets/worksheet-01-using-R.html#directions",
    "title": "Using RStudio Worksheet",
    "section": "",
    "text": "Complete the problems on this worksheet with 2–3 other students. You may want to refer to the Getting Started with R and Data Structures in R chapters in the Computational Toolkit for Educational Scientists."
  },
  {
    "objectID": "worksheets/worksheet-01-using-R.html#problem-set-i",
    "href": "worksheets/worksheet-01-using-R.html#problem-set-i",
    "title": "Using RStudio Worksheet",
    "section": "Problem Set I",
    "text": "Problem Set I\nUse R to compute the answer to the following problems. You can (for now) work in the console or use a script file. This problem set will focus on skills and ideas you learn in the section Getting Started with R.\n\nCompute: \\(2 + 3 \\times 5\\). Does R follow order of operations or not?\nCompute: \\((3 \\times5^2 \\div 15)-(5-2^2)\\)\nCompute: \\(\\sqrt{~\\left\\vert3-4\\right\\vert~}\\)\nCreate a sequence of the numbers 1 and 5. Store this in an object called one_five.\nCreate a sequence of all integers between 10 and 1. This sequence should begin at 10 and end at 1. (e.g., \\(10,9,8,7,\\ldots,1\\)). Store this in an object called my_num.\nUse the length() function to count the number of elements in the one_five object. Also use it to count the number of elements in my_num.\nCompute \\(\\mathtt{one\\_five}+\\mathtt{my\\_num}\\). Explain how R is using the two objects to obtain the result it did.\nInstall the following three packages from CRAN: dplyr, ggplot2 and remotes.\nLoad the remotes package.\nInstall the educate package from GitHub."
  },
  {
    "objectID": "worksheets/worksheet-01-using-R.html#problem-set-ii",
    "href": "worksheets/worksheet-01-using-R.html#problem-set-ii",
    "title": "Using RStudio Worksheet",
    "section": "Problem Set II",
    "text": "Problem Set II\nUse R to compute the answer to the following problems. For these problems, please work in a script file. This problem set will focus on skills and ideas you learn in Data Structures in R.\n\nCreate a new script file. Save it on your computer. When you do this give it a name other than Untitled.\nUse the c() function to create a vector that includes the number of pets each person in your group has. Name this vector pets. (If you have 2 or fewer people in your group, combine your results with another group so that you have at least 3 values.)\nCreate a data frame that includes two columns. The first column will be the first names of everyone in your group (or combined groups). Call this column first_name. The second column will be the number of credits each person in your breakout room is taking this semester. Call this column credits. Assign this data frame a name so that you can compute on it later. (Feel free to make it a three column data frame by adding the pets column.)"
  },
  {
    "objectID": "worksheets/worksheet-01-using-R.html#problem-set-iii",
    "href": "worksheets/worksheet-01-using-R.html#problem-set-iii",
    "title": "Using RStudio Worksheet",
    "section": "Problem Set III",
    "text": "Problem Set III\nUse R to compute the answer to the following problems. For these problems, please continue to work in the script file. This problem set will focus on skills and ideas you learn in Data Structures in R.\n\nImport the comic-characters.csv data into R. Assign this to an object called comics. If you use the Import button to do this, make sure you copy the syntax into your script file.\nExamine the codebook for the comic-characters.csv data.\nRun the syntax View(comics). This should display the imported data in a tab in RStudio. Use the search bar in the tab to search for “Hobgoblin”. How many Hobgoblin characters are there?\nUse the search bar in the tab to search for “Skrull”. How many Skrull characters are female? (Hint: Click the arrows in the sex heading to sort by sex.) How many Skrull characters have brown eyes? What years were Skrull characters part of the Marvel storyline?\nRun the syntax mean(appearances). You should get an error. Try to figure out why you are getting this error. (Hint: Look at the objects in your environment.) Can you figure out the syntax to find the average number of appearances for a comic character? (Hint: Because there are missing values in this column, you will need to add the argument na.rm=TRUE in the mean() function.)"
  },
  {
    "objectID": "worksheets/worksheet-02-learning-dplyr.html",
    "href": "worksheets/worksheet-02-learning-dplyr.html",
    "title": "Learning dplyr Worksheet",
    "section": "",
    "text": "Work with one or more other students to complete each of the tasks in this document. As you work through these tasks, you may want to refer to the following:\n\nData Wrangling with dplyr\n\nAs part of this, include the syntax you use to complete each tasks in a script file. As you write your script file, adhere to good coding practices:\n\nInclude comments\nInclude spaces\nInclude a line break after every pipe operator you use.\n\nYou can check in with Andy or the TA at the completion of each task to check your work."
  },
  {
    "objectID": "worksheets/worksheet-02-learning-dplyr.html#directions",
    "href": "worksheets/worksheet-02-learning-dplyr.html#directions",
    "title": "Learning dplyr Worksheet",
    "section": "",
    "text": "Work with one or more other students to complete each of the tasks in this document. As you work through these tasks, you may want to refer to the following:\n\nData Wrangling with dplyr\n\nAs part of this, include the syntax you use to complete each tasks in a script file. As you write your script file, adhere to good coding practices:\n\nInclude comments\nInclude spaces\nInclude a line break after every pipe operator you use.\n\nYou can check in with Andy or the TA at the completion of each task to check your work."
  },
  {
    "objectID": "worksheets/worksheet-02-learning-dplyr.html#task-1-import-data",
    "href": "worksheets/worksheet-02-learning-dplyr.html#task-1-import-data",
    "title": "Learning dplyr Worksheet",
    "section": "Task 1: Import Data",
    "text": "Task 1: Import Data\nImport the comic-characters.csv data into an object named comics. Also, examine the data codebook so you are familiar with the different variables."
  },
  {
    "objectID": "worksheets/worksheet-02-learning-dplyr.html#task-2-arranging",
    "href": "worksheets/worksheet-02-learning-dplyr.html#task-2-arranging",
    "title": "Learning dplyr Worksheet",
    "section": "Task 2: Arranging",
    "text": "Task 2: Arranging\nUse the arrange() function to determine whether Marvel and DC introduced LGBTQ characters around the same time, or whether one company was more progressive?"
  },
  {
    "objectID": "worksheets/worksheet-02-learning-dplyr.html#task-3-more-arranging",
    "href": "worksheets/worksheet-02-learning-dplyr.html#task-3-more-arranging",
    "title": "Learning dplyr Worksheet",
    "section": "Task 3: More Arranging",
    "text": "Task 3: More Arranging\nUse the arrange() function to determine the first appearance of an LGBTQ female character in DC comics."
  },
  {
    "objectID": "worksheets/worksheet-02-learning-dplyr.html#task-4-filtering-rows",
    "href": "worksheets/worksheet-02-learning-dplyr.html#task-4-filtering-rows",
    "title": "Learning dplyr Worksheet",
    "section": "Task 4: Filtering Rows",
    "text": "Task 4: Filtering Rows\nUse the filter() function and nrow() to determine the percentage of female comic characters."
  },
  {
    "objectID": "worksheets/worksheet-02-learning-dplyr.html#task-5-more-filtering",
    "href": "worksheets/worksheet-02-learning-dplyr.html#task-5-more-filtering",
    "title": "Learning dplyr Worksheet",
    "section": "Task 5: More Filtering",
    "text": "Task 5: More Filtering\nUse the filter() function and nrow() to determine the percentage of LGBTQ comic characters introduced in 1970 or later. Based on the result is there evidence that the Pride Movement may have had an impact on LGBTQ representation?"
  },
  {
    "objectID": "worksheets/worksheet-02-learning-dplyr.html#task-6-selecting-columns",
    "href": "worksheets/worksheet-02-learning-dplyr.html#task-6-selecting-columns",
    "title": "Learning dplyr Worksheet",
    "section": "Task 6: Selecting Columns",
    "text": "Task 6: Selecting Columns\nThe Selecting a Subset of Columns section of Data Wrangling with dplyr included syntax for selecting a subset of data and exporting it to a directory on your computer. Modify the syntax to export a CSV of the LGBTQ character data to the desktop on your computer.\nAlso try modifying the syntax to export the CSV file to another folder/directory on your computer."
  },
  {
    "objectID": "worksheets/worksheet-02-learning-dplyr.html#task-7-mutating-columns",
    "href": "worksheets/worksheet-02-learning-dplyr.html#task-7-mutating-columns",
    "title": "Learning dplyr Worksheet",
    "section": "Task 7: Mutating Columns",
    "text": "Task 7: Mutating Columns\nCreate a new column called first_appearance_pride that indicates how many years before or after the Pride Movement (1970) the character’s first appearance happened. For example a character that first appeared in 1980 would have a value of 10, and a character that first appeared in 1968 would have a value of -2. Assign the data that includes this column into an object called comics2."
  },
  {
    "objectID": "worksheets/worksheet-02-learning-dplyr.html#task-8-grouping-and-summarizing",
    "href": "worksheets/worksheet-02-learning-dplyr.html#task-8-grouping-and-summarizing",
    "title": "Learning dplyr Worksheet",
    "section": "Task 8: Grouping and Summarizing",
    "text": "Task 8: Grouping and Summarizing\nModify the syntax in the Computations on Groups section of Data Wrangling with dplyr to determine the average number of appearances for both DC and Marvel LGBTQ and non-LGBTQ characters (as well as the standard deviation and sample size) before and after the Pride Movement."
  },
  {
    "objectID": "worksheets/worksheet-03-ggplot2.html",
    "href": "worksheets/worksheet-03-ggplot2.html",
    "title": "Learning ggplot2 Worksheet",
    "section": "",
    "text": "Work with one or more other students to complete each of the tasks in this document. As you work through these tasks, you may want to refer to the following:\n\nVisualizing Data with ggplot2\n\nAs part of this, include the syntax you use to complete each tasks in a script file. As you write your script file, adhere to good coding practices:\n\nInclude comments\nInclude spaces\nInclude a line break after every pipe operator you use.\n\nYou can check in with Andy or the TA at the completion of each task to check your work."
  },
  {
    "objectID": "worksheets/worksheet-03-ggplot2.html#directions",
    "href": "worksheets/worksheet-03-ggplot2.html#directions",
    "title": "Learning ggplot2 Worksheet",
    "section": "",
    "text": "Work with one or more other students to complete each of the tasks in this document. As you work through these tasks, you may want to refer to the following:\n\nVisualizing Data with ggplot2\n\nAs part of this, include the syntax you use to complete each tasks in a script file. As you write your script file, adhere to good coding practices:\n\nInclude comments\nInclude spaces\nInclude a line break after every pipe operator you use.\n\nYou can check in with Andy or the TA at the completion of each task to check your work."
  },
  {
    "objectID": "worksheets/worksheet-03-ggplot2.html#task-1-import-data",
    "href": "worksheets/worksheet-03-ggplot2.html#task-1-import-data",
    "title": "Learning ggplot2 Worksheet",
    "section": "Task 1: Import Data",
    "text": "Task 1: Import Data\nImport the riverview.csv data into an object named city. Also, examine the data codebook so you are familiar with the different attributes."
  },
  {
    "objectID": "worksheets/worksheet-03-ggplot2.html#task-2-sketch",
    "href": "worksheets/worksheet-03-ggplot2.html#task-2-sketch",
    "title": "Learning ggplot2 Worksheet",
    "section": "Task 2: Sketch",
    "text": "Task 2: Sketch\nMake a rough sketch of the plot you think this syntax will produce. Try not to run the syntax until you create a sketch. After you have created your rough sketch, run the syntax to check your work.\n\nggplot(data = city, aes(x = education, y = income)) +\n  geom_point() +\n  xlab(\"Education level (in years)\") +\n  ylab(\"Annual income (in U.S. dollars)\") +\n  theme_bw()"
  },
  {
    "objectID": "worksheets/worksheet-03-ggplot2.html#task-3-mimic-a-plot",
    "href": "worksheets/worksheet-03-ggplot2.html#task-3-mimic-a-plot",
    "title": "Learning ggplot2 Worksheet",
    "section": "Task 3: Mimic a Plot",
    "text": "Task 3: Mimic a Plot\nWrite syntax to create the following plot, which was created using the city (riverview) data. Write this syntax in a script file. Be sure you are using good coding practices and putting each layer on a different line. (Hint: If you want to be exact, use Tin Eye Lab’s Color Extraction tool to extract the hex codes for the color.)"
  },
  {
    "objectID": "worksheets/worksheet-03-ggplot2.html#task-4-export-a-plot-and-import-it-into-a-document",
    "href": "worksheets/worksheet-03-ggplot2.html#task-4-export-a-plot-and-import-it-into-a-document",
    "title": "Learning ggplot2 Worksheet",
    "section": "Task 4: Export a Plot and Import it into a Document",
    "text": "Task 4: Export a Plot and Import it into a Document\nExport the plot you created in Task 3 as a PNG file. Be sure the aspect ratio of the plot is reasonable (the plot is not scrunched or stretched). Import the exported PNG file into a word-processed document. Re-size the plot so that it is not taking up excess space in the document, and also still readable. Add a figure number and caption to your plot."
  },
  {
    "objectID": "worksheets/worksheet-03-ggplot2.html#task-5-mimic-a-plot",
    "href": "worksheets/worksheet-03-ggplot2.html#task-5-mimic-a-plot",
    "title": "Learning ggplot2 Worksheet",
    "section": "Task 5: Mimic a Plot",
    "text": "Task 5: Mimic a Plot\nWrite syntax to create the following plot, which was created using the city (riverview) data. Write this syntax in a script file. Be sure you are using good coding practices and putting each layer on a different line."
  },
  {
    "objectID": "worksheets/worksheet-03-ggplot2.html#task-6-mimic-a-plot",
    "href": "worksheets/worksheet-03-ggplot2.html#task-6-mimic-a-plot",
    "title": "Learning ggplot2 Worksheet",
    "section": "Task 6: Mimic a Plot",
    "text": "Task 6: Mimic a Plot\nWrite syntax to create the following plot, which was created using the gapminder.csv data. Write this syntax in a script file. Be sure you are using good coding practices and putting each layer on a different line.\n\n\n\n\n\nLife expectancy versus per-peron income for 193 countries."
  },
  {
    "objectID": "worksheets/worksheet-03-ggplot2.html#task-7-add-text-to-label-a-point",
    "href": "worksheets/worksheet-03-ggplot2.html#task-7-add-text-to-label-a-point",
    "title": "Learning ggplot2 Worksheet",
    "section": "Task 7: Add Text to Label a Point",
    "text": "Task 7: Add Text to Label a Point\nAdd text to the plot you created in Task 6 to label the country with the highest life expectancy."
  },
  {
    "objectID": "worksheets/worksheet-04-z-scores.html",
    "href": "worksheets/worksheet-04-z-scores.html",
    "title": "z-Scores",
    "section": "",
    "text": "Complete the problems on this worksheet with your break out group. You will have no more than 20 minutes."
  },
  {
    "objectID": "worksheets/worksheet-04-z-scores.html#directions",
    "href": "worksheets/worksheet-04-z-scores.html#directions",
    "title": "z-Scores",
    "section": "",
    "text": "Complete the problems on this worksheet with your break out group. You will have no more than 20 minutes."
  },
  {
    "objectID": "worksheets/worksheet-04-z-scores.html#problem-set-i",
    "href": "worksheets/worksheet-04-z-scores.html#problem-set-i",
    "title": "z-Scores",
    "section": "Problem Set I",
    "text": "Problem Set I\nConsider the distribution of the following six observations: \\(X = \\{1, 2, 2,3,4,12\\}\\).\n\nPlot these values. (Don’t bother using R, just create a dot plot by hand.)\nDescribe the shape of the distribution.\nCompute the mean of the distribution. (You can enter these observation into R using the c() function.)\nCompute the standard deviation of the distribution."
  },
  {
    "objectID": "worksheets/worksheet-05-interactions.html",
    "href": "worksheets/worksheet-05-interactions.html",
    "title": "Interactions",
    "section": "",
    "text": "Use the fertility.csv data to fit an interaction model that uses the effects of female education level and the dummy-coded GNI predictor (high_gni) to explain variation in infant mortality rates. Remember, to fit an interaction model you need to create the interaction term and then include the constituent main effects AND the interaction term in the lm()."
  },
  {
    "objectID": "worksheets/worksheet-05-interactions.html#directions",
    "href": "worksheets/worksheet-05-interactions.html#directions",
    "title": "Interactions",
    "section": "",
    "text": "Use the fertility.csv data to fit an interaction model that uses the effects of female education level and the dummy-coded GNI predictor (high_gni) to explain variation in infant mortality rates. Remember, to fit an interaction model you need to create the interaction term and then include the constituent main effects AND the interaction term in the lm()."
  },
  {
    "objectID": "worksheets/worksheet-05-interactions.html#questions",
    "href": "worksheets/worksheet-05-interactions.html#questions",
    "title": "Interactions",
    "section": "Questions",
    "text": "Questions\n\nFit the interaction model and write out the fitted equation.\nGive a general interpretation about what it means to have an interaction effect between female education level and GNI on infant mortality rates. (No need to look at any output for this.)\nBased on the coefficient-level output, is there an interaction effect between female education level and GNI on infant mortality rates?\nWrite out the fitted equation for low GNI countries.\nWrite out the fitted equation for high GNI countries.\nPlot the two fitted equations.\nIs this an ordinal or disordinal interaction? Explain.\nUse the plot and the coefficient estimates to add to the general interpretation of the interaction effect you gave previously in Question 3. You should be able to give a more nuanced interpretation, including quantifying some findings.\nRemember that every interaction can be interpreted in two manners. You gave one of those interpretations in Question 3 (and 8). What is another way we can interpret the interaction effect between female education level and GNI on infant mortality rates."
  },
  {
    "objectID": "worksheets/worksheet-05-multiple-regression.html",
    "href": "worksheets/worksheet-05-multiple-regression.html",
    "title": "Displaying Effects from a Multiple Regression Model",
    "section": "",
    "text": "Complete the problems on this worksheet with your small group. Import the mn-schools.csv data and fit a model to explain variation in graduation rates using the predictors of median SAT score and tuition."
  },
  {
    "objectID": "worksheets/worksheet-05-multiple-regression.html#directions",
    "href": "worksheets/worksheet-05-multiple-regression.html#directions",
    "title": "Displaying Effects from a Multiple Regression Model",
    "section": "",
    "text": "Complete the problems on this worksheet with your small group. Import the mn-schools.csv data and fit a model to explain variation in graduation rates using the predictors of median SAT score and tuition."
  },
  {
    "objectID": "worksheets/worksheet-05-multiple-regression.html#questions",
    "href": "worksheets/worksheet-05-multiple-regression.html#questions",
    "title": "Displaying Effects from a Multiple Regression Model",
    "section": "Questions",
    "text": "Questions\n\nWrite the fitted equation.\nInterpret both effects in the model.\nCreate a plot to show the effects. Put SAT score on the x-axis and display lines corresponding to an “inexpensive” and an “expensive” school. (Use the data to choose those values.)\n\nMake sure the lines are displayed in different colors and linetypes;\nPlay around with labelling the lines using text in the plot; Google geom_text() for help.\n\nExplain how we can use the plot to interpret the effect of SAT.\nExplain how we can use the plot to interpret the effect of tuition.\nCreate the same plot, but this time separate the “inexpensive” and “expensive” schools into separate panels (i.e., create a diptych plot.)"
  },
  {
    "objectID": "worksheets/worksheet-06-multiple-comparisons.html",
    "href": "worksheets/worksheet-06-multiple-comparisons.html",
    "title": "Hospitalizations and Astrological Sign",
    "section": "",
    "text": "Discuss the following study with your group. Do you believe the findings? Why or why not?\n\nMethods\nA study was conducted of 5,337,472 residents of Ontario aged between 18 and 100 years in 2000 using the Registered Person’s Database. These residents were classified according to their astrological sign. The researchers then searched through 223 of the most common diagnoses for hospitalization and identified 72 (32.3%) for which residents born under one astrological sign had a significantly higher probability of hospitalization compared to residents born under the other astrological signs combined. For example, Libras were 1.37 times more likely to be hospitalized for a fractured pelvis than people born under other astrological signs (\\(p=.0108\\)).\nThe p-values for the 72 significant associations ranged from 0.0003 to 0.0488. The two most frequently occurring diagnoses for which each astrological sign had a higher probability of hospitalization compared to the other astrological signs combined are described in Table 1."
  },
  {
    "objectID": "worksheets/worksheet-06-multiple-comparisons.html#directions",
    "href": "worksheets/worksheet-06-multiple-comparisons.html#directions",
    "title": "Hospitalizations and Astrological Sign",
    "section": "",
    "text": "Discuss the following study with your group. Do you believe the findings? Why or why not?\n\nMethods\nA study was conducted of 5,337,472 residents of Ontario aged between 18 and 100 years in 2000 using the Registered Person’s Database. These residents were classified according to their astrological sign. The researchers then searched through 223 of the most common diagnoses for hospitalization and identified 72 (32.3%) for which residents born under one astrological sign had a significantly higher probability of hospitalization compared to residents born under the other astrological signs combined. For example, Libras were 1.37 times more likely to be hospitalized for a fractured pelvis than people born under other astrological signs (\\(p=.0108\\)).\nThe p-values for the 72 significant associations ranged from 0.0003 to 0.0488. The two most frequently occurring diagnoses for which each astrological sign had a higher probability of hospitalization compared to the other astrological signs combined are described in Table 1."
  }
]